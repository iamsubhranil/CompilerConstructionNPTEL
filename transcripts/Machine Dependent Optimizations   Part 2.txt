 So, what we saw is about the data hazard, we will see how data hazards need to be handled. As I mentioned earlier whenever you have a true dependency or what is called a read after write dependency, the subsequent instruction has to wait until the previous instruction writes a result back in the register. Let us look at a situation in which my ith instruction produces a value, but the i plus oneth and i plus second instruction do not need this, but let us say the i plus fourth or the i plus fifth instruction need that value. Then what happens in that situation? So, let us not worry about the immediate second or third instructions, but let us say that dependency is a little further away, may be the fourth or fifth instruction. In this case there is no problem, right. There is a dependency, but there may not be a stall because by that time that instruction goes into the so called instruction fetch phase, this instruction would have completed its execution, correct. So, the problem happens only if the dependent instructions are within one or two instructions away. If you are assuming two stall cycles, then it is one or two instructions away. If assuming one stall cycle, then it is only one instruction. There are some hardware solutions for data hazards and that can actually reduce the data hazards stalls to one cycle in most of the cases, okay. But we are not going to focus on the hardware solution, we are going to focus more on the software solution and let us see what can be done in software to take care of this. Instruction scheduling is a technique which can be used to take care of this problem. As I mentioned earlier, only if the dependent instructions are close to each other, that is one or two instructions within each other, then the stalls matter. Otherwise the dependencies would have been any way been satisfied and you do not incur a stall. So, the question is, is it possible for me to reorder my instructions so that dependent instructions are a little further apart. That is the question, okay. Let us make the question little bit more specific. If I assume, right, the dependent instructions cannot happen within one cycle. That means that between i and i plus one cycle alone I need to worry about. Or more specifically for certain types of instructions like load instruction, right. I say if the load instruction loads some value, then the next instruction on that. But let us say the i plus second onwards can be dependent on that, right. Supposing if that is what we want to look at as a problem, then can compiler help us? That is the question, right. So, instruction scheduling is a technique. We are going to see more about this in the next lecture, okay, in the probably on probably tomorrow afternoon or on Friday, Thursday afternoon or Friday. We will see more about that, right. Essentially that tries to place instructions so that dependent instructions are kept a little further apart from each other. But when you do this you have to ensure that all the dependences are satisfied, right. The true dependences are satisfied and so on, okay. Now if you look at this technique is essentially what we call as a compile time technique because the compiler reorders these instructions, right. And then make sure that the dependent instructions are kept further away from each other. There is also a run time solution to this problem where the hardware does this reordering of instructions in terms of not necessarily moving them out, but in terms of the order in which they are being executed, issued and executed. We will not again go into the details of this because we are again focusing on the compiler related topics here, okay. So as I mentioned earlier we are going to talk about a specific form of a delay called load delay slot. In that what we essentially mean is that if the i-th instruction is a load instruction then the i plus 1-th instruction cannot be dependent on that. That is the only problem that we are trying to solve. In all other cases we will say the hardware solution will take care of things, right. Only in the case of this load delay, right, the software or the compiler has to help, right. Let us look at that particular problem, okay. So here is what I mean by this. Here is an example. So you have a load instruction and that load instruction produces a value in R3 register which has to be consumed by the subsequent add instruction, right. Similarly and then there is one more add instruction which is also dependent on R3. Then another load instruction and again that load instruction produces a value in R13 which is being consumed by this instruction, right. And by what we have described earlier if there is a load instruction and then there is an immediate dependent instruction there will be one stall cycle between them, correct. So again you can see that because of this dependency and if these instructions happen to be successive instructions there will be one stall cycle. Similarly here also. So if I present this code from my compiler to this machine then that machine would incur two stall cycles. That means that if I am only considering these five instructions then the time it would take to execute these five instructions looking at from a throughput point of view is that it will produce the results of these pipeline instructions, right. Five instructions in seven cycles. That is really what would happen because of these two stall cycles, right. So that is why it is important to remove these stall cycles if possible from this, right. Let us see what happens. If I am allowed to reorder these instructions but when I reorder the instructions I have to make sure all the dependences that are given in the instructions are preserved. That is an important condition if I say that. Then how can I do that? For example if I look at this instruction, right, also uses R3. So I cannot move this between these two instructions, right. What about this instruction? This instruction uses R11 register and none of these instructions actually write into R11. That means that this instruction is not dependent on any of these instructions, right. So the question is can I move this instruction, right, in between these two instructions, right. And if I do that what will I gain? Okay so let us see that. So the first load instruction is there. Then the add instruction is there because of the dependency I want to put some instructions in between these two instructions. Look at see, right. If I move this load instruction in that location then what happens is that this is an independent instruction which is between this load instruction and the dependent add instruction. Now this instruction can be fetched, let us say if this instruction is fetched in time t, this can be fetched in time t plus 1 and this can be fetched in time t plus 2 and that will not really cause a stall because that one stall cycle that I am talking about I have avoided by putting this instruction in between. Now what about the stall between this instruction and this instruction? I have also taken care of that because immediately after this instruction there is a independent instruction which is independent of this, right. So if I leave the rest of these instructions in the same order, right, you can see that I have taken care of the dependency between the load instruction and the corresponding add instruction as well as this load instruction and the respective add instruction, right. They have been now separated by more than one instruction and as long as they are separated by more than one instruction these stalls will not occur. So in this particular case, sorry, in this particular case I have zero stall cycles that means that in a pipeline execution I can actually finish executing all these five instructions in five cycles. So I have again, now I have improved the throughput to one instruction per cycle. So this is essentially what we call as instruction scheduling. This is done by the compiler. We are going to spend time on Thursday and Friday trying to talk about instruction scheduling, how to do instruction scheduling, right. And this is for what we call as a simple pipeline processor which can issue and execute one instruction every cycle. We will also talk about this in the case where there is instruction level parallelism, right. Again remember that when we talk about these dependencies, data dependencies and data hazards we talked about doing this, right, as a compiler technique. I will very briefly talk about a runtime technique also to do this but only very briefly because again from a compiler perspective that is not very relevant for our discussion. Okay, now let us talk about control hazards which is the third type of hazard, right. So what happens? Let us look at the ith instruction. If the ith instruction is a branch instruction, then let us assume that it is fetched in the instruction fetch phase. Decoded during the decode phase, the operands are also fetched. During the execute phase, we figure out whether the conditions are satisfied or not and we also find out where the target location has to be. So let us see at the end of the execute cycle, we know whether the branch condition is true or not. If the branch condition is satisfied, then we have to go to the target location. The target location is also computed during the execute phase, right. Now if this instruction is fetched in the time t, what happens in time t plus 1? The i plus 1th instruction is normally what is being fetched but if the condition is true, I should not be fetching this instruction, right. So during the decode phase, as soon as I figure out that the ith instruction is a branch instruction, I should stop stalling this instruction, right. I should really stop fetching this or doing anything with the i plus 1th instruction. But only in time t plus 2, correct, I will really know whether the branch condition is satisfied or not. If it is satisfied, then the stalling is fine. If it is not satisfied, I can start fetching the instruction in the next cycle, right. So let us see what happens. Again we stall because only at this point in time we know what we need to do. Then at the next cycle, right, that is if this is t, t plus 1, t plus 2, in cycle t plus 3, we can figure out whether to fetch from i plus 1 or to fetch from the target location. So again what you see here is that you will see two stall cycles. Again this is for a specific pipeline with five stages assuming that condition is being evaluated here. If the condition is evaluated here, then the stalls will be three cycles. If the condition is evaluated only at a later stage, stalls will be even more number of cycles, correct. So the number of stalls depends on where exactly the condition and target addresses are calculated because you need both of them to move forward, correct. If it takes four cycles before you compute that, then there will be four stall cycles, correct. Now again compilers can do little bit of help here. Let us see what happens there. Again as I mentioned earlier, in order for us to resolve about conditional branches, we need to know whether the condition is satisfied and we also need to compute the target address. In our example, we assumed both of these things to be happening in the EX stage, but if you can build an aggressive hardware where both of these could be done in the ID stage itself, then you can reduce the delays to one stall cycle, right. Again depends on how aggressive you want to build your hardware. Today's hardware can actually do that. In fact, it can do much more than that, okay. Let us look at the case when the stall is reduced to one cycle, but minimally it at least requires one stall cycle because after the IF stage only you can decode and execute the instruction. So, minimally you need one stall cycle, okay. So, one possible way to handle this problem is what they call as static branch prediction technique. In a static branch prediction technique, you have this policy of static not taken policy. That means that even when you hit a branch, you pretend that the branch is not going to be taken, right. And if you pretend the branch is not going to be taken, you allow the i plus 1 instruction to be fixed in cycle t plus 1, right. And then one cycle later you will figure out whether the condition is satisfied or not. If the condition is not satisfied, then your prediction is correct. You can move forward without any delay, but if your condition is, if the condition is satisfied and the branch has to be taken, then your prediction is wrong, you incur one stall cycle. But anyway you would have incurred one stall cycle. So, by taking a static not taken policy for branches which are not taken, you can essentially avoid the stall cycle, right. So, this idea is a very simple idea, but quite useful. So, what I want to do is that if in my instruction stream, I will always assume that the branch is not going to be taken, right. And then I will build my hardware such that the i th instruction and the i plus 1 th instruction are always executed independent of whether the branch is taken or not taken, correct. So, I am now saying that see whenever you see a branch instruction, the idea is that the condition is satisfied after the i th instruction, you go to the target. But now we are building a hardware in which even if the condition is satisfied, the i plus 1 th instruction will also be executed before you go to the target location, right. So, what do you put in the i plus 1 th instruction has to be, you have to be careful about it. That has to be the instruction which you anyway have to execute irrespective of whether the branch is taken or not taken. For example, if I have an instruction above the branch which is independent, right, I can possibly put it below the branch also so that i and i plus 1 are executed, right. That is really what we are going to talk about this rest of it is actually from a hardware perspective, not. So, what we are going to talk about is what we call as delayed branching. In delayed branching what we have is that you are designing a hardware so that irrespective of what happens to the branch instruction, the instruction following the branch is also executed. Question? Sure. Yeah. Question? Do that. Question? Sorry. Question? Yeah, right. In this example is what you are talking about. Yeah, I was talking about this example. So, here let us say I have the first instruction and later I have the branch instruction. So, you said that in a branch instruction if you go to the right side. Just hold on to your question for a few minutes, we will come to it because I want you to first understand what delayed branching is and exactly what I am going to do I will show you, right. So, hold on to your question for a second. So, let us skip this, let us go here, right. My animation is probably little bit messed up, but let us see. Supposing let us say I have the following sequence of instructions, right. There is an add instruction, there is a branch instruction, right. This branch instruction uses the value of R1 and R1 is not being written by this instruction, right. Now, my hardware is that if the branch instruction is executed, the instruction following the branch is also executed before you either go to the instruction below that or go to this location out, right. So, this is how my hardware behaves, right. Hardware has been changed such that not only the branch instruction, but the instruction following the branch is also being executed irrespective of whether the condition is satisfied or not. That means that you react somewhat slowly to the branch, right. That is why it is called delayed branching. You are not going to jump immediately after the branch. You are going to jump one instruction after means one instruction. You are going to execute one more instruction before you take the branch. That is why it is called delayed branching. So, in delayed branching, if the ith instruction is a branch, the i plus oneth instruction is always executed, correct. One simple way by which I can fill the i plus oneth instruction slot is to put a no op instruction. A no op instruction is a no operation dummy instruction. If I put that, nothing will happen, correct. It will execute that instruction and then after that, it either branches to the target location or continue its execution from that. That is a very simple thing to do, but what have I achieved by that? Really nothing because no op instruction is a wasteful instruction. Instead of having a stall, I executed a wasteful instruction which is equivalent to having a stall, correct. But instead of putting a no op, let us say in this example, if I can pull this instruction down and put it over here because there is no dependency that I am violating, right, this instruction produces a value in R3. This instruction is not dependent on that. If I pull this instruction below and if I execute that, then I can have delayed branching. This branch can execute the instruction following that can also execute and after that depending on the condition, either I can follow that location or I can go to the target, correct. That is fine. So, my hardware essentially allows me to, I mean hardware essentially says that I am going to execute the branch instruction and the following instruction. So, if you want to put any useful instruction in that, go ahead and put. That is essentially what the hardware tells the compiler, okay. So, unfortunately the delay slot is hiding the branch instruction. So, sorry about that, okay. The instruction following the branch will also be executed. The slot has to be one step down, okay. Now, you see the action happening, right. So, technically speaking, this should have been, let me just go back and show it to you correctly. So, let us again look at this. So, I have the add instruction, correct, and I have the branch instruction, okay, and then I have the delay slot, correct. What is the hardware ensuring? The hardware is ensuring that after the ith instruction, the i plus oneth instruction will always be executed, correct, right. This is what the hardware is saying irrespective of whether the branch condition is satisfied or not, that instruction will always execute, correct. Okay. So, what I am going to do is that this delay slot which is that I need to put some useful instructions there. One way of doing is that take this instruction which is not dependent or this branch instruction is not dependent on that and move it in this delay slot, correct. Then what happens is that instead of having this sequence, you will have branch instruction R1 and then add R3 instruction, right. Now, if this is executed in the ith cycle and this is executed in the i plus oneth cycle, right, because of the delayed branching, you can see that the add instruction will be fetched in the next cycle and it will be executed, right. And after one cycle, the branch condition would have been evaluated and you know whether you have to go to i plus 2 or to the target location out, right. So, after that instruction, it can jump, right. And what I have done is that this one delay slot that we had where a useful instruction can be filled, we have moved an instruction and moved it down, correct. Now, does that answer your question? This is the kind of delay slot that you want to fill in. Now, these are two independent things, right. Sometimes you can move a branch instruction between a load instruction and a dependent instruction to take care of both of them. But if you have a branch instruction and if delayed branching is supported, may not even have to be a load instruction, it can be any other instruction also which can be moved, okay. That is really what we are talking, okay, all right. So, this animation was a little bit messed up. That is why you do not see the things correctly. You see a sequence of two add instructions, but it should have been that this add instruction, there is a branch instruction and then subsequently an add instruction, okay. So, this way the stall cycles can be avoided. Now when you do this filling of delay slot, you can actually fill instructions from above the branch or sometimes you can also take instructions from below the branch, okay. But you have to take instructions that do not affect the branching condition. Again, it is essentially saying that we have to satisfy the dependencies. So, where do you get these instructions from? As I mentioned earlier, it could be from the target branch or from the fall through, okay, or from above. Any of these things is possible. I do not know whether I have an example for this, but if you want, I can always say this thing, okay. Go through this animation and the case that we discussed was one where you have from before the branch, right. This is because those instructions are anyway executed before the branch instruction is executed. So, if you move them down to the delay slot, it is always used, okay. But you have to make sure that you are not violating any dependencies. Now if this is not possible and you do not have any independent instructions which is above the branch, which could be moved into the delay slot, then you can think in terms of taking an instruction from the target location. Remember, we were supposed to jump out whenever the condition is true, right. So, I can take the first instruction or the second instruction from that and I can also put it in that provided, right, certain conditions are satisfied. I have to make sure that executing that instruction does not cause any violation of dependencies, okay. And that is useful only if the branch is taken more often than not. If the branch is, you know, very few times it is taken, then taking an instruction from the target location and filling it up is of no use because by executing that instruction, I am actually doing a dummy action. When the branch is more often not taken, then I can take an instruction from the fall through branch and I can put it in the delay slot and I can do that provided again it does not violate any dependencies, okay. And this is useful whenever it is more often not taken than taken. So, these are the ways by which we can do and a compiler can actually help you to do this thing. That is why we study this here, okay. Now, let us move on to the next topic which is about instruction level parallelism where you are trying to execute more than one instruction every cycle, okay. So, here the idea is that you have an instruction sequence, but from that instruction sequence you try to identify what are called independent instructions. Instructions which are not dependent on each other and you try to issue and execute them together in a single cycle, right. So, why do we want to do this? We want to do this because we want to improve the throughput. With ideal pipelining, you get a throughput of one instruction per cycle, right. But if you want to get more than one instruction per cycle, even an ideal pipeline cannot give that. So, an ideal superscalar processor or an ideal VLIW processor which can fetch decode and execute let us say four operations per cycle can give you an IPC or throughput of four instructions per cycle. So, that is the reason why we want to do that. But you can only execute independent instructions in every cycle because dependent instructions have to be delayed by appropriately so that the dependences are satisfied, okay. Now, how do we identify these independent instructions which can be issued and executed together? That is the question, right. In superscalar processors, this is done by the hardware. Hardware is presented a sequence of instructions. It fetches four instructions together every cycle. It decodes this understands whether these instructions are independent or not. And if they are independent, whichever instructions which are independent, it tries to issue and execute them in parallel. Whereas, in VLIW processor, VLIW stands for very long instruction word. In VLIW processor, the compiler analyzes the code, identifies these independent instructions and puts them in parallel, okay. And the hardware essentially fetches these instructions together and executes them. We are going to spend more time on understanding this phase of how the compiler can do that. But very briefly, we will also see what should be done here, okay. So, essentially this is pictorially what happens in a superscalar processor. You have a sequence of instructions, right. So, two or four of them are fetched in each cycle in the instruction fetch phase and they are put into some kind of a queue where they are being decoded. So, from here to here, things happen in program order. That means that in whatever order the instructions are, they are actually decoded in the same order. That is when you will understand what the programmer has intended. If there is a data dependency from this instruction to this instruction, if you decode them only in that order, you will understand that the dependency is from that instruction to this instruction. Otherwise, you do not know which way is the dependency, right. So, if you look at here, pictorially the green instruction happens first, then the blue instruction, then the red instruction, gray and then the purple instruction, correct. So, that is the order in which it has to go. So, decode is done in the program order. And after you have decoded, you would understand the dependencies between the instruction, the true dependencies between the instruction. For example, in this case, the gray instruction is dependent both on the green and red instructions, right. And the blue instruction produces some value which is being consumed by the purple instruction, right. But the gray instruction is not dependent on the red or green. The red is not dependent on the green, right. So, given this series of instructions which are decoded and after decode, the hardware now understands this dependence relations between them, right. The hardware tells that as soon as you are ready, you can execute the red and green instruction, right. And similarly, you can execute the blue instruction also or you can execute all of these three instructions in parallel, right, whenever you want. Let us say the hardware decides to execute the green instruction and blue instruction in the next cycle, right. And let us say they produce a value. Then in the next cycle, because the blue instruction has finished executing, the purple instruction can execute. But what about this gray instruction? It still has to wait. Why? The red instruction has not finished its execution. So, this will wait until this red instruction finishes the execution. When the red instruction finishes execution, some kind of a signal is given and the hardware figures out that the gray instruction is now ready for execution. So, in this what happens is that the instructions are decoded in program order and their dependencies and they are stored in what is called the instruction window. And in the instruction window, we understand the dependencies between these instructions, correct. And whenever instructions have all their data operands available, they can be issued to the execution unit, right, and they can be executed, right. And after they finish execution, right, to the dependent instructions, they send the data value or they send some kind of a signal, so that the gray instruction can figure out that the green has finished execution or the red has finished execution or when all of them has finished execution, the gray itself can go for execution, right. And then after that, the values of each of these instructions are written in the destination location again in program order. This is again important. Remember, they are decoded in program order. They are issued possibly out of program order, right, that is out of order. But again, the result values are written back in program order. This is again required for certain hardware reasons. I will not go into the details of that, okay. Yeah. Sir, is this done by hardware or the entire software? All of this is done by the hardware, okay. In the superscalar processor, you give the series of instructions just like the way we saw on the left-hand side of the static instruction scheduling example, correct. And the hardware will figure out that the first load has a dependent instruction, maybe this gray. So, it will hold on to that and it will possibly figure out that the blue instruction or the red instruction is another load instruction which it will try to independently issue and execute. So, the hardware will figure out all of these mechanisms. Sir, what is the difference between hardware and hardware? I am going to see in the next slide, okay. I will tell you how that is going to happen. This is just at a high level pictorially what is likely to happen, okay. This is not necessarily say anything about how it happens in the real hardware, okay. So, more question or we are okay. So, let us look at how the hardware would look like, okay. So, I have skipped a few things and then now I am introducing what is called the instruction cache. Remember, we talked about doing instruction fetch and instruction fetch if it has to happen in a single cycle that instruction obviously has to be in the cache. So, let us assume that there is an instruction cache and we will fetch the data from the fetch the instruction from the instruction cache. Every cycle I will fetch multiple such instructions, right. Four consecutive instructions will be fetched and they will be put in the instruction buffer. From the instruction buffer, okay, this part of it let us skip. From the instruction buffer I move them to what is called a decode, rename and a dispatch phase, okay, where the four successive instructions are being decoded, right and the dependencies between them are understood, okay. It also does something called the register renaming which for the time being we will skip. We will not go into the details of that, right. And then after the decode and the dependency information is understood, right, at the same time I put the order in which these instructions are decoded into my reorder buffer. This is again finally I am going to use this because I want to write the results in the destination location in the program order. I am just trying to remember this, right. After the instructions are decoded I am going to push them into appropriate instruction queues, right, in which they can be from which they can be taken and executed. This can be in the form of a queue or this can be in the form of a buffer. It depends on the architecture. So, the instructions go and wait there for all the operands to become available. As soon as all the operands of an instruction becomes available they can go from the instruction queue to the execution unit. And as you can see here there are multiple execution units. There could be an integer add unit, there could be an integer multiply unit, there could be a floating point add, floating point multiply or there could be multiple integer add units, there could be a separate load store unit, there could be a separate, right, floating point unit, there could be a separate branch processing unit and so on and so forth. So, there could be multiple functional units. So, in every cycle whatever instructions whose operands are available who have become data ready they can now be moved to the appropriate functional units and can be executed. That is what we mean by they get executed in parallel. After they get executed in parallel the result values are returned to this reorder buffer which stores the order of these instructions, right. And then from there it actually, my animation, yes, my animation is little bit off. So, it goes to the reorder buffer and then from there it goes to the register file, okay. So, essentially what happens is that instructions are fetched in program order, decoded in program order, put into this instruction queue and in the instruction queue they need not necessarily how to be executed in program order. They can be executed depending on the availability of the data operands. And once the execution is complete the result is written back into the reorder buffer where it stays there, right. And from the reorder buffer we write the result values back into the register file in program order, okay. That should partly answer some questions that you asked me and also your questions, right. Any more questions? Unfortunately we are not going to see more details on this. So, this is for the load instruction because load instructions has to access the data cache part of it and instruction fetch, if the instruction is not there has to also fetch it from the memory. So, that is a memory interface part, right. So, this is how a superscalar processor kind of executes instructions, right. And all of this as I mentioned earlier is done by the hardware, okay. Software essentially presents a sequence of instructions. If it wants it can do some instruction reordering and give the instruction to the hardware, but once it has done whatever it can do the rest of it is all done by the hardware, okay. Any questions? Okay. Now, let us look at another kind of an instruction level parallelism processor called VLIW processor, right. In the superscalar processor as I mentioned earlier independent instructions which can be executed in parallel they are identified by hardware in this stage, okay, when they are being put into the instruction queue, right. And therefore, this hardware has to be a complex hardware. It has to understand what are the instructions, what are the dependencies between the instructions, which instructions have got their operands available or for which instructions the previous dependent instructions their predecessors have already completed execution. I have to take those values, I have to take those things and then find out which instructions are ready I can send them to the execution unit for execution, correct. So, this piece of hardware which is going to do all of this is going to be complex, right. Whereas, in the case of a VLIW processor the compiler is going to detect all the independent instructions and is going to put them together in one large instruction board. That the processor can fetch, do the decode and simply execute. The processor can execute all of these instructions together because the compiler guarantees that they are independent, okay. That means that my hardware does not have to do any check, right. The hardware simply can say that if these are the four operations that can be that are together in an instruction, I can execute all of them in parallel, right. I do not really have to worry about checking their dependencies are in it, right. So, that is why in a VLIW processor the compiler does all the work and the hardware is simpler. Whereas, in a superscalar processor the hardware does all the work and the compiler really does a little bit of reordering and nothing more. So, there the hardware is much more expensive, right. So, typically this is what we call as a smart compiler the one which does a lot of hard work and a dumb hardware, okay. So, this is how the VLIW processor looks like. It has an instruction memory from which the instructions are fetched every cycle. That instruction itself will have multiple operations and each of these operations because the compiler has compiled it this way we know are independent and therefore, can be sent to the appropriate function unit and those function units can execute them taking their operand from the respective register files and after finishing execution they will write the result back in the respective register file, right. If they need to do load store operations then they will access the data memory, right. So, this is a simple pipeline hardware is simple, right. No detecting of dependencies and other things. It can simply go through this cycle, right and it can execute parallel instructions every cycle. This is what that happens in the case of a VLIW processor. So, the question is if you are writing the compiler for this VLIW processor what all do we need to do during the instruction scheduling stage? That is the problem that we will talk about, right. That is the reason why we are talking about VLIW processor. So, again we have the super scalar processor where the detecting of independence between instructions is done by the hardware. We have a VLIW processor where this is done by the compiler, okay. I think this is probably where we are going to stop. This is an example of a VLIW processor which has a 256 bit instruction and can hold up to 7 wide operations, okay. So, at this point in time we probably have to switch and then stop here discussing about machine, machine architecture to a large extent, okay. We will come back and talk about memory hierarchy mostly on Friday and data parallelism also on Friday, okay. Any questions before we switch the slides? So far things are good, right. Suppose to give you a quick overview of architecture, right that is required for us to write the compiler, right, okay. So, again so as I mentioned earlier we started off with machine architecture. We covered most of what we wanted to see immediately, okay. Now we are going to move on to code generation, right. So, that is basically what we are going to discuss for the rest of today, okay. So, again we will have some introduction why code generation, what is code generation and you know how do we need to do that and so on. Then we will talk about a simple code generator which is just based on usage counts, okay, sorry not usage counts, simple code generation, right. Then we will go to more sophisticated code generation mechanisms and specifically talk about the construction of a directed acyclic graph for basic blocks and then do code generation for that. And we will see that even in this case if you want to generate what is called the optimal code that problem is an NP-hard problem, right. And therefore, we will resolve to some simpler mechanisms like using certain heuristics for generating code. But fortunately if you are talking about code generation for expressions which are like trees, which is a specific case of a DAG, then in those cases you can generate optimal code either by using the Seti-Wilman algorithm or by doing what is called dynamic programming. So, we will talk about those approaches as well and then subsequently we will talk about how do you do instruction selection using tree pattern matching, instruction selection and code generation using tree pattern matching, right. And finally, we will also talk about little bit of machine dependent optimization called peephole optimization. So, this is how we are going to cover, these are the things that we are going to cover in this module and we will kind of get to a shape by which we can do code generation, right. So, you might have seen that these are the different phases of the compiler, right. And obviously things on lexical analysis, syntax analysis and semantic analysis would have been covered. You would have also seen intermediate code generation, correct. And you would have also seen machine independent optimization in the last two days, correct. So, all of this has been covered so far, right. So, what we are going to assume is that you are given an optimized intermediate code. What kinds of intermediate representations did you people see? Three address code, anything else? AST, then. So, we are going to assume that you are going to be given one of those things and we are going to assume that all the machine independent optimization have already been performed, right. Now our objective is to take this optimized intermediate code and generate an efficient target code, right. And we are, as I mentioned earlier, we are only going to consider about generating assembly code. From the assembly code going to the machine code is a part of the assembler which we will not discuss, okay. So, again as far as the machine dependent optimizations are concerned, these are the different phases. You do code generation and as a part of the code generation you do instruction selection. Then you do a set of machine dependent optimizations which is what I call as the peephole optimization. And once you have finished all of these things you have a so called an efficient code, right. On that you may possibly do the register allocation. Then after that you do instruction scheduling in, it is not after that you actually do that before doing register allocation. But then again it is possible to think of doing it in any order. So, these are two other components that we are going to see. And the essential challenges here are that generating code, right, is a non-trivial task, okay, given the intermediate code. Intermediate code is in what form? Three address code form, right. But the three address code, how complex or simple is it? Relatively simple, right. From the high level you have reduced it down to, right, simple things. And if you look at it in some sense it is closer to the assembly code, correct. It is somewhat closer to the assembly code. But yet going from that intermediate code to the assembly code is not that trivial. There are several ways by which you can do this and each will have its own cost. And you may end up incurring unnecessary cost if you are not being efficient about it. The problem of generating the so called optimal code is an NP-hard problem. In fact several of these problems that you see here, right, fall in this NP class. And therefore getting the optimal solution for them will take a large amount of compile time, that is computation time, right. And therefore we may end up going for certain heuristic solutions for these things. Again I will talk, when we talk about code generation we will talk about which problem is NP-hard and why, what kind of solutions we go for that. Similarly when we talk about register allocation we will talk about why register allocation problem is like that and so on and so on, right. So optimal code generation, register allocation, instruction scheduling, all of these problems are NP-hard. So if you want to get the optimal solution you have to spend a lot of, the complexity is high so you have to spend a lot of time for that. So we will not obviously get that. We will try to see efficient solutions for all of them. Okay, just to put things in perspective as I mentioned earlier we are going to take an optimized intermediate code and generate an efficient target assembly code. So this consists of two parts which is basically the target code generator and then certain code optimizations. And these code optimizations are specific for the target machine that you are talking about. Okay, so what could be the forms in which intermediate code can be? It can be a three address code, it can be some kind of a syntax tree, okay or it can be a linear representation, right. When you talk about other kinds of compilation environments you also talk about what is called byte code, right. Many of you who are familiar with Java, Java generates, when you compile a Java code it generates a byte code and the byte code is what is being executed by the Java virtual machine, correct. Typically the byte code is interpreted by the Java virtual machine, right. But then in your Java virtual machine you can write a compiler which is what they call as a just-in-time compiler that can take the byte code and generate executable code for that, correct. So in that case if you are considering of let us say a JIT compiler, that JIT compiler would have the byte code sequence as its intermediate representation, right. Or like any of the other intermediate representation that you are familiar with in the case of a normal, right, offline compilation kind of thing. The output that you generate is basically the target assembly code and depending on whether your processor is a RISC processor you generate a RISC code. If your processor is a CISC processor you generate the appropriate RISC code. If you are generating code for let us say if you are generating byte code taking the Java as your input language, okay and you are generating byte code, Java byte code then you are essentially generating that byte code as your output code. So again this notion of what is input, what is output depends on what is the context in which you are working on. If you are talking about a JIT compiler your input is a byte code and your output is an object code. If you are talking about a Java compiler your input might be a three address code or a syntax tree and your output might be a byte code, correct. So all of this can be called as code generation in some sense. Of course we are going to focus more on taking a three address code and generating an assembly code and we are going to give all our examples for a CISC machine, okay But the techniques are general enough that you can extend it to RISC machines as well, right. But examples are unfortunately going to be for a CISC machine, all right. Okay. So again we have already answered this question what do we need to know about the target machine for code generation, okay. The simple answer is whatever you wanted to know we have already covered in the last lecture, right. So what are the instructions in the target machine, right. What are the addressing modes that are required, right. I mean that are supported in the architecture. What is the cost of each instruction, right. There are many ways of executing the same instruction or implementing the same instruction. They may have different cost. I need to know the cost of this so that I will choose the one which is with the lower cost that is the idea. And of course I also need to know how the runtime storage management and other things are done, okay. So has runtime environment been covered as a part of this workshop so far? Will be covered later, right. Next week it will be covered, okay. That is fine. The stack frames and other related things. I am not very sure whether that is going to be covered but okay. Unfortunately you will have to have that gap. I am not planning to cover. We can, I can see whether I can give you a quick reading on that, okay. Sure, sure, sure, sure. Typically runtime environment how the stack frames are maintained and they need to know the parameters, parameter passing, local variables, that kind of a thing, right. Okay. I should have checked this before but unfortunately I forgot to, right. So this part is something that I am sure that Shekhar will take care of coordinating and getting at least some part of it covered otherwise you can read it, right. Again remember when we talked about local variables, local variables are allocated in the stack, correct. So you need to know how those variables are addressed. They are always addressed with reference to either a stack pointer or a frame pointer, more like a frame pointer rather than a stack pointer because that stack frame, right, both the stack pointer and the frame pointer will be pointing to that. You typically use the frame pointer to address the local variables, right. Similarly for the parameters that you are passing to your function, they will be with reference to the stack, a frame pointer, right. And when you want to do return that is with reference to the stack pointer. So managing the stack frame is one aspect that you need to know in order for you to do the code generation and that part of it is typically covered in runtime storage management, okay. Another thing is about call convention. So certain processors have this notion of caller saved register, callee saved registers and so on, right. In a caller saved register before you make the call, the caller will save some of these register contents in the stack before making the call. In the callee saved registers, the callee will save those things in the stack frame before they modify those registers and then before they return from the callee to the caller, they will actually restore those values. So these are responsibilities that are, you know, given to these things. So as a compiler or as a compiler writer, you need to know that so that when you generate code before you make a function call, all the caller saved registers have to be saved in the stack frame. And in order for you to save them in the stack frame, the stack frame size has to be designed appropriately, decided appropriately. So those are the things that the compiler writer has to take care of and for that purpose, they need to know about the runtime storage management, okay. For now, we will assume that you have that knowledge, but wherever you require, I will kind of tell you what it is and maybe we can figure out how to implement that later on, okay. So why is efficient code generation kind of complex, right? The answer is that there are many possible ways of implementing the same instruction or the same three address code and you have to choose the one which is the most efficient one, right? And the different sequences obviously will have different cost. So the one which is with the lower cost is what needs to be set. And then particularly when you talk about using registers, right, there are, let's say, several program variables. You want to figure out which of these variables need to be stored in the register and which of them can be accessed from the memory directly. This is only in Sysc architectures because in Sysc architectures, you can afford to have some operands in memory. And when you can afford to have some operands in memory, essentially the decision of what is in memory, what is in register or what should be loaded into register has to be, that decision has to be made and again that decision influences the cost of your code, right? And therefore, again there are many choices and you have to choose the right one in order for you to generate the efficient code. Again my, okay, sorry about this animations coming out of order, but let's, let's try to work this out, okay? So look at this simple instruction A is equal to B plus C. In this case, what I am going to do is that I am going to load B into a register, C into another register. I can, I am sorry, I am not loading B into a register, I am loading the address of B, load address of B into a register, load address of C into another register and I am moving the contents of the location pointed by R naught register into R2. Then I am adding the contents of the memory location pointed by R1 with R2, right? So this has A, sorry, yeah, let's look at it. So this is basically moving the value of B into R2. This is the pointer to A. So I am adding the contents of A now with the contents of B and I am then putting it back into that location, right? So that is essentially what is happening. If you look at it, basically I have five instructions and then in couple of instructions I have this move address which is possibly using the absolute address mode and that has an additional cost, right? So I add up those costs. Another way of generating code for this, again things are coming slightly out of order. Let me just get all of this and then work on it, right? So this is the second way of generating instruction for this in which I don't move any of these addresses into register or even the values into register. I simply move the value of B using a move instruction into this R0 register. Similarly, move the value, sorry, add the value of C which is in memory with the value of B which is in R0 register, put the result in R0 register and subsequently move that value into A register. This takes only three instructions, right? But then some of these instructions are heavy duty instructions, right? As opposed to some of those instructions. So again the cost of this need to be estimated. Here I have used a simple cost mechanism, one for each instruction plus one for any additional words that I am using. So this has a different cost. Again for each architecture this may be different and you have to use the appropriate cost. If I have the facility to just move, so here is what happens. I first copy the contents of B into memory location A and then I add C to memory location A and put the value back in A. So provided my architecture supports and add instruction with two memory operators. If that is possible then I can do this. Whatever is the cost for that I have to pay for that. So this is another way of generating this instruction, right? So here is the third way where you basically, so here you are basically moving, you are assuming that the values of B and C are available in these two registers. I am first moving the value of B into again register A. I am adding register A with register C and I am now writing the value into this, right? Again just to show that there are different ways by which you can achieve the same instruction and each may have a different cost. So essentially in a code generation phase, yes? I have assumed some way by which each instruction has a cost and then every instruction which use additional bytes. For example, here I have to store the absolute address of B following that instruction as an additional cost, right? So the machine architecture will tell you exactly what the cost is, right? And as a compiler writer one needs to understand that cost and then incorporate that cost. That is really what it is. So in this particular example, see the address of B has to be following this instruction. So that has to be fetched and you know used. Therefore, I have added an additional cost for that. Similarly, the address of C. Similarly, the address of A. Here I am assuming that the value of B is already available in a register, right? If that is available, then I can actually move that into another register. I can do the add with the memory location or with another register and then write the value back. Here I am assuming that all of this is register operands and therefore, they incur only one cost. I mean they only incur a cost for the instruction and only in this instruction I have an additional memory address and therefore, I am assuming a cost of one, right? So when you generate code for a target architecture, right, you say that each instruction has this kind of a cost and if the operand is like this, then there is an additional cost for that. So that information is being specified to you or you can understand that from the instruction set manual and based on that you will try to generate the code for that. That is really what happens, right? Okay, if it is a Sysc machine, this is what you will do. If it is a Rysc machine, what can we do? You have to move the memory locations to registers, right, and then perform the operation and then store them back, right? Okay, so here is what you have, right? Load B into R naught load C into R1, add R1 and R2, put the result in R naught and store it back into A, right? So if you assume that each instruction is one word, there are four instructions, there is nothing additional in here, so you will have a cost of, right? Here is again I am assuming that this B and C can be specified simply, I mean simply meaning like using some kind of displacement addressing mode with a small offset. That is what I mean, that is why I said that these could be four bytes. Whereas if these addresses are not there, I mean that can typically happen if these are local variables are available in the stack frame. If this is not available in the stack frame, but if these are variables which are global variables, then their addresses first need to be moved into some locations from there it can be do. For example, in order for me to move A, or sorry move B, first I need to store the most significant word of the address of B, right? That is the upper 16 bits into one register. Then I have to add the least significant 16 bits, okay, with that in another. See, remember my immediate constant can only be 16 bits long in the MIPS architecture. So in that MIPS architecture, the immediate constant can be at most 16 bits, whereas the addresses are supposed to be 32 bits, right? So I have to do it in two steps. First get the most significant bit, then add the least significant bit to it, get the address in R naught, then use that address, load it, put it into R. Similarly for the C, right? And then I perform the addition, I again get the address of A in register R phi and then I store the value. In fact, this is what I have to do if A, B and C happen to be global variables, right? Or their addresses are truly 32 bits and they are not available in any registers, right? If I know A and B are offset by four locations, right? I could have used that in some efficient way, right? You understand that, right? For example, if A and B happen to be part of a structure fields, correct? A happens first and then B happens next. The compiler allocates saying that the first one is going to take four bytes, the next one is four bytes off from that. Then once I have loaded B's address, A's address is only four bytes offset from that. I do not need any of these things. I could have simply used 4 minus 4 or 0. I could have done that, correct? So similarly some intelligent decisions could have been done. So depending on where the variables are and how they are relative to each other, when the compiler generates code, it has to generate the complete code so that it can load that information. In this case, if these are local variables, they are available in the frame pointer, right? And offset within the frame pointer is a short offset. So that can be a part of the immediate concept, right? So I can specify it within the same instruction, right? For local variables, local variables are always stored in the stack frame and they are always referenced with regard to the frame pointer. So there will be a small offset from the frame pointer so that can be specified. So that is where these instructions are simpler and has a lower cost. Whereas these instructions, they require more instructions and their cost is correct and therefore the total cost is, okay? So depending on whether you are generating code for a Sysc machine or for a Rysc machine, what types of instructions are supported, what types of addressing modes are supported, right? You have to generate code appropriately and you have to generate a code which is supposedly more efficient, right? And there are several possibilities. The idea here is to just show that there are several possibilities, okay? All right. So let us just first start off with a simple code generation. Yes, question. Right. So for every global variable, right, there is something like, okay, so for every global variable it is going to associate a location, right, saying that it is going to be in that particular location starting from the data segment. So the data segment starts off from some place. So everything is offset from that, right? So all the global variables are going to keep allocating from that, right? It is a little bit, so if you think of your entire C program as a single C file, right, that picture is kind of easier to look at than thinking of it as multiple C files. Even multiple C files can be done. So if I have a single C file and let us say I have three global variables A, B and C, the compiler can understand that there are only three global variables, all of them have to be in the data segment. The first variable is an integer variable, so it will be at an offset 0. The second variable let us say is a double precision variable, so it will be from the first variable which is 4 bytes, the next variable will be at an offset of 4. The third variable let us say is another integer because the previous one is a double and then the one before that is an integer, so it is 4 plus 8, 12 bytes offset. It can actually work out all of these details. So it can say that global variable A will be at offset 0, global variable B will be at offset 4, global variable C will be at offset 12. It can fix that thing, right, and it can use that. Now when you have multiple such files which you are linking together, then something has to be above, something has to be below, but then again they are relative, right, and the compiler can work those things out. Compiler and then subsequently the linkers and loaders, not just the compiler, it is also the linkers and loaders because each file is compiled separately, you remember that, right. So as far as that single C file is concerned, there are only these global variables, right. Again, unfortunately I may not be able to provide a complete answer because there are several other things also, but it gives you an idea, right. If it is a stack frame, it is simple. Local variable is simple, right. Good. Shall we move forward? Okay, so let us look at a very simple code generation scheme, right. So this is going to essentially go and then do the following thing. It will read one, right, intermediate statement and it will generate the code for that, right. That is essentially what it is going to do, but then when it generates a code depending on what the architecture allows, it has to decide, you know, what stays in the register and what stays in the memory and so on and so forth, right. So it is going to essentially maintain two data structures, one called register and another called address descriptor, right. The register descriptor essentially keeps track of which register holds what variable names, right. For example, let us say if you have moved contents of A into register R1, then you have to remember that R1 contains variable A. So that subsequently if there is any instruction which is making use of variable A, instead of again loading it into one more register, you might as well use this R1 because you save cost, correct. Once you have already moved it into a register R1, it will be good to remember that, right. So that whenever A is being used, instead you can use R1 to do that, right. The other data structure which is the address descriptor essentially kind of holds the other kind of the map. For every variable name, it says where all it is being held, right. For example, variable A, you say whether it is in memory or you say whether it is in memory and register, you say that, okay. Because it is possible that you could have it in both places, right. And similarly, when a variable is being copied into another variable, right, the register descriptor can have for the same register multiple variable names. Say for example, A is copied into B and then I have loaded A, sorry A is copied into B, then I have loaded B into R1 register, right. If I have the sequence of statement A equal to B and then move B to R1, then R1 essentially holds B, but R1 also holds A. To remembering this would be helpful because if I am going to do something with A, I might as well use R1. However, I have to be careful not to overwrite something, right. Because if B is going to write onto itself some value or let us say the register which is holding B is going to be modified, then at that point in time I have to go and say that A no longer resides in register, A is only in the memory, correct. Otherwise, I will be using the wrong location. So, this kind of a mapping is what needs to be maintained by these two descriptors. That means the current up to date value for every variable, where is it available or in what all locations is it available. Similarly, each register what are all the variables that it is currently holding. You need to kind of remember this in order for you to do this code generation. I will show you a simple scheme and also a simple example. We will not really bother too much about it because nobody is going to write a code generator using this method. I mean it is going to produce a very very inefficient code if you use this method. So, there is no need to learn this completely and then generate code using that because you are not ever whenever ever going to use this thing. But just to have a quick idea of what it is, right. So, in this code generation scheme what we want to do is that we want to use the register location whenever the operand is available both in the memory and register because using register location saves cost, right. That is simply what it is, okay. And for every destination operand you try to allocate a register whenever it is possible and hold that value in that register as long as possible. But if no free registers are available you can also use memory. That means that you are going to incur more cost. That is the only difference, right. If they are if the see particularly in the case of a Sysc machine which allows your operand to be in memory or register you could have used either one of them, right. Again it depends whether that particular Sysc architecture allows your destination operand to be in memory. If it allows it to be in memory then you can use it. If it does not then necessarily you have to allocate a register for that. So, you have to find a register and so on. So, essentially this algorithm is going to go in the following way but it is very I mean it is going to generate a very inefficient code. Let us see how it works, right. For each three address code which is of this form x is equal to y operation z, right. It first finds out, okay. It first gets a register for the destination location, right. So, we will find out how get register works but the idea is that get register is going to identify a location L which could either be a register or a memory location, right for the destination operand, right. And then next it determines where y is available. So, it finds out the location y prime where y is available, okay. It could y prime could either be a register or a memory, okay. The location where it is available. Then what it is going to do is that it is going to copy this y into this location L, right. Remember we are going to talk about generating address for CISC machines and in this CISC machines typically it is a two address, two operand instruction address format, okay. And one of the operands is also the destination location. That is why we are taking the first operand and then putting it in the destination location, right. So, remember earlier itself we figured out that L should be the location where the destination is going to be, right. So, we have said that x is going to be available in L and in that location I have now moved the value of y. Now I find out where z would be, right. Find out the location where z is, okay. And then generate code operation z prime, L. If z prime is a register, then it is a register, register. If z prime is a memory, then it is memory register, right. Assuming this machine supports one operand to be in the memory and the other to be in the register, right. This kind of works. So, that is the reason why we have done this, okay. Actually, I am sorry, I will take that back a little bit because I do not know whether L is a register, right. We only know that L is the location where the destination has to be. This could even be a memory, memory operation. All that I know is that I have moved the value of the first source operand into the destination location. Then I have performed the operation of performed the operation with regard to the second source operand and the first source operand which is already available in my destination location and the result is also available in my destination location, right. After I have done that, now I have to say that x is now available in L, right. x is available in L. I cannot say anything about y prime. I am sorry, I cannot say anything about y except that y is available in y prime which is where it was available earlier. So, I do not have to necessarily do any update on that. He said also I am not moving anywhere, right. You understand that prior to this instruction, y was available in the location y prime and z was available in the location z prime. Neither of them have been disturbed, right. They have not been modified. So, both y and y prime continue to stay wherever they were before. Only thing is that x is now available in L. If L happens to be a register, then we know that it is a register. If L happens to be a memory location, then it is a memory location. So, the address registers, address descriptor and the register descriptors have to be appropriately updated, ok. x was not already available in L. x was not already available in L. We have to use this getreg function to find out a location for L and if that happens to be a register, it is something different. Otherwise, it will be memory location x, ok. Let us see what the getreg function is doing, right. So, getreg function is essentially supposed to find a location L for x for computing this operation. So, it is actually going to do something very interesting. It first finds out whether y is in a register R, ok. If y is already in a register, y is the first source operand, ok. And that register is not holding any other location. That means that only y and R has only y, nothing else, right. And of course, y is not needed anymore, y is not live anymore, right. That is that value which is available in the R register is not needed anymore. Then I can use that itself. See, it is possible that you have computations where certain value is in a location. You use it once or you use it a few times and after that, you do not again need it for a long time. In which case, I can actually overwrite onto that location, right. That is what it says. So, if y is available in a register R and R does not hold anything other than y, then I can and of course, y is not needed anymore and I can go and modify that itself. That register itself I can reuse. That is essentially what it is saying. So, allocate that register to x and say that that is the location where your destination is going to be. And if you do that, you remember that your y is already in R, ok, in your destination location. Else, if there is an empty register R prime, you can allocate that, right. If no register is available, ok, and if the operand requires the destination to be in a register, then you have to spill one of those existing variables which is already holding all your registers, throw that out and give that register to this location, right. So, that is essentially what you are going to do. So, spill a register into memory and give that, ok. If none of this is true, then this particular instruction can have a memory operand. So, you say that x is in memory, right. So, again let me go through this one more time. So, you either find out if the first source operand is a register and that register is no longer required for y, then you can use that itself as the location for x. If that is not the case, if there is a free register which is available, then you can say that free register will hold that. If that is also not the case, but if your operand can be in memory, sorry, if your operand has to be in a register, then spill one of the existing variables which is in one of those registers and then give that register to x. If none of this is the case, then you simply use a memory location for x, right. Questions? The third one, ok. So, let us say that I have four registers and they are storing y, z, w and v, right. But now this particular operation, right, the target machine specification says that the destination has to be a register operand. It cannot be a memory operand, correct. Then I cannot say that x can be in memory, x has to be in a register, right, as far as this operation is concerned. Let us say that this is some multiply operation or something like that, just as an example. So, multiply the destination has to be a register, it cannot be a memory operand, right. Then what I need to do is that, right, this x which is a temporary, I will say that it has to be in one of the registers. I have only four registers and these four registers are being taken by y, z, w and v. I do not have a place for x. What do you do? You ask one of these people, y, z or w, right, to get out and then give that register. That is what is called as spill. So, let us say that we are going to say that, ok, w is the victim. So, I will spill the value of w into the memory location and whichever register w was holding, I will assign that to x, right. That is really what happens. There is no use of y subsequently. After this instruction, it is going to hold the value of x. Absolutely. It is not holding the address, it is holding the value of y, value of y, ok. All the four registers might be, will be used later also. So, let us say that here I have spilled w and give that register to. No, in this instruction there are going to be only two operands, correct. Remember, this simple code, this, remember, remember this is a simple code generation. We do code generation for one intermediate statement at a point in time. It has only two operands. I have to have y definitely in the register or I have to, means I have to have x in the register. So, eventually I am going to move y into that register and do something, correct. But that is ok. Other than that, right, if there is a variable w which is holding a register, which is not needed for this instruction, right, my immediate point is to get this instruction done and in order for me to do that, I will spill w. It may so be the case that the next instruction is w, correct, and I want to reload it into the register. I will incur the cost. That is why this is called a simple code generator and that is why it generates inefficient code. It does not look beyond how it takes a decision of what to spill, right. We have not really said it could spill any one of those things arbitrarily and that might be required immediately after that, right. So, you will incur a cost for that, no doubt, but you can generate code. Functionally, correct code, it will execute, but it may be inefficient, right. Yes, yes, if I have only two registers, what will I do? That is an architecture which may not be a useful architecture, right. So, you will have enough registers to work with, but all of them may have some values. Then you have to take a decision of which one to spill. It may so happen that the one that you have spilled may have to be reloaded again. There is a cost that you pay for. Right. That is really what happened. Any more questions? Yeah. Okay. So, this has to go a little bit more into the details of how this intermediate code is, what these temporaries are. The assumption here is that this is a new temporary which is being generated. That is why you try to find a location. If it is already in a register, you can definitely use that. Okay. You just modify this slightly so that that is taken into account. This is assuming that this is a newly generated value, right. Which register to spill is it? Okay. We are going to talk about separate, as far as this simple code generation scheme is concerned, nobody is using it or nobody will be using it. So, the answer is very simple. Use anything that you want, okay, randomly. But, yeah, it is going to be inefficient. You could do something smarter. Like you could look ahead and then see which variable you are not going to use for a longer time, etcetera, etcetera, and you can make your decision smarter. But then this mechanism itself, this way of generating code itself is so inefficient that you do not want to fix some part of it. So, let us not worry about it, right. Let it do something very, very inefficient. It is okay. It only adds to our motivation that we have to build a more efficient code generator, right. Good. But I think the points are very well taken and your answers and as well as questions are indeed in the right direction. That tells you how to think about writing a good code generator. That is really what is important, right. If I ask you, okay, can you make it better? You would obviously make it better. But then for what purpose, I do not know, right, because this whole process itself is very inefficient. We are going to see something significantly better than this. Please stay with me for some time. Just wanted to get the concepts through. That is why I am just saying this, okay. So, here is an example that we are going to see, right. So, we have these variables t, u, v and w which are being generated and they are basically these things, right, these expressions. So, what I do is that I take each statement and try to generate code for that statement. Taking into account whatever is the current registered descriptor and address descriptor at that point in time, right. So, let us say that at this point in time r naught has t, right. So, let us say initially nothing was there in my data structures. So, now what I do is that I move first a to r naught and I subtract b from r naught and therefore, r naught will contain the value of a minus b which is my t. So, in this instruction, when I am generating code for this instruction, for this destination location t, when I ask to get register, I assume that it gave me r naught, right. Therefore, my first code is move b to r naught and then subtract, I move a to r naught and then subtract b from r naught and then store it into, I mean that essentially puts the value a minus b in r naught, ok. So, after this instruction is over, I know that r naught has t, right and that is of course, the same thing t is in r naught, right. Now, when I go to the next instruction u, I am again asking the question give me a location for u, right and this time let us say it gave r 1, right. Now, I generate the code move a to r 1, subtract c from r 1, the result is in r 1. Now, we can say that r naught has t and r 1 has u, the same thing in the address descriptor, put the other way around, right, t in r 1 and this one. Now, let us say v is equal to u, this is a copy operation, right. Now, in this case, I do not necessarily need to generate any code for this, right. I find out what is u, u is in r 1, correct, it is a copy operation and for the copy operation, I can simply say if this is already in a location, this will also be in the same location, right. So, after this we say that r naught has t and r 1 has both u and v, same thing over here. Now, let us see here, here I am going to do t plus v, right and I am going to write it into w, right. Again, when I try to say get register for w, it gave me r 2, right and I find out where t is in r naught. So, I move r naught to r 2 and then of course, find out where v is, v is in r 1. So, I add r 1 to r 2, right. So, at the end of this r naught has t, r 1 has both u, v and r 2 has w, same information mapping in the reverse direction. So, I keep updating my register descriptor and address descriptor and I keep working on these things, every statement one after another and I keep updating it. I keep generating the code for this, right. You can see that in this particular case, right, I first move a to r naught, then again I move a to r 1, right. I could have avoided this by giving a register for this and keeping a in that register, but I do not do those intelligent decisions in this simple code generator. All I do is that for this instruction, what is that I need to do? For that instruction, what is that I need to do? And based on that, I generate code and therefore, I may end up moving the same operand into different registers at different points in time, right. See, if I have moved a into r naught, b into r 1 and subtracted a minus b as r naught minus r 1 or something like that, my a would have been saved and I could have used it later, but I did not do that, right. So, that is why this kind of generates inefficient code, okay. Essentially, the point is that we are not looking ahead, we are just concerned about that particular statement and generating code for that. So, if you want to generate more efficient code, obviously, you have to have a global picture. Well, let us not call it as a global picture, let us first let us call it as a big picture, because the word global is going to be used in compiler in a very specific way, right. So, we need to have a slightly bigger picture of the whole thing, right. Obviously, the bigger picture cannot be too big, then we will be there may be too many details and we may not be able to see. So, we are going to focus on a small region of code for which we can generate and this region of code is what we are going to call as the basic block, right. So, in control flow analysis, right you would have seen data flow, sorry in data flow analysis you would have talked about basic blocks and other things. So, what is a basic block? Sequence of instructions. One follows the other and if one instruction is executed, then all of them are executed, correct. Let us say that is what is called a basic block, single entrance, single exit, correct. So, essentially if I ask the question, can you come up with a code generation scheme which looks at a basic block and generates efficient code for that, right. And then of course, you have different basic blocks which are combined by means of control flow, you can generate code for that and so on and so forth. So, the code generation problem we are essentially going to look at at a basic block level to start with, right. Anything which is beyond basic block is what is called the global in the compiler terminology. So, that is why we did not want to say we want to have a global picture. We want to have well a big picture, okay, which is at the level of a basic block to start with, okay. Then of course, we will look at how this basic block are connected by means of these control flow graphs and so on and so forth, right. So, that is the next step, right. And when you talk about basic blocks, right, the way to kind of capture the information in the basic block and what variables are being repeatedly used, etcetera, is can be captured by means of a DAG which is a directed acyclic graph. So, we will talk about constructing a DAG and then doing code generation for the DAG. That is our next step.