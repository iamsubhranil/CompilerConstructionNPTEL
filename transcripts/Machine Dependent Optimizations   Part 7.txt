 Software Pipelining. Software Pipelining is essentially instruction scheduling for loops. So, we will talk about all of those topics in the current module, right. So, the current module is titled Instruction Scheduling. And what you are going to see here as I mentioned earlier is that we will introduce the need for instruction scheduling. Then we will talk about how basic block scheduling is done. And subsequently we will extend this to global instruction scheduling beyond basic blocks. And there are three different global instruction scheduling that we talk about namely trace scheduling, super block scheduling and then hyper block scheduling. Then subsequently we will talk about software pipelining which is all about instruction scheduling for loops. And towards the end we will very briefly talk about the interaction between register allocation and instruction scheduling that is something it is also used. So, what is instruction scheduling? Instruction scheduling is essentially given an instruction sequence you reorder them in some other way such that it minimizes certain things. But when you do the reordering you have to make sure that the dependences which are there in the original program are always preserved, right. And what you try to minimize? You try to minimize the execution time of the program that is of concern to you, right. So, instruction scheduling can be done to minimize the number of stalls that would be incurred by the pipeline execution unit. Remember we talked about data hazards and control hazards. And then when you have these hazards, right, normally stalls are incurred and to avoid these stalls you can again reorder instructions to avoid them. Some processors may exploit, not some, today almost all processors exploit instruction level parallelism. That means that they are capable of executing multiple independent instructions in a single cycle. And when you have processors of that kind then instruction scheduling can expose these independent instructions to these processors so that they can be executed together. And when you have instruction level parallelism and when you kind of expose this parallelism using instruction scheduling you have to make sure that the schedule that you generate, right, the schedule will have multiple parallel instructions because they are able to, your architecture is able to exploit them. But these multiple parallel instructions should also obey resource constraints. By that what we mean? If you say that there are three multiply instructions, right, in your instruction schedule then there must be at least three multiply function units for you to execute them. If you have fewer than that then obviously this schedule does not satisfy that resource constraint. So when you talk about instruction scheduling you always talk about satisfying dependence constraints which is data dependency and you always talk about satisfying resource constraints, right. These are the two things that you talk about, okay. So instruction scheduling can improve the performance of a program by actually moving these independent instructions around, okay, independent instructions in parallel or adjacent positions. Parallel because if you have a VLIW kind of an architecture you express them in parallel. If you have a superscalar architecture even, I mean each instruction is going to hold only one operation. So successive independent operations are put next to each other so that the hardware when it decodes can actually look at these instructions and then say that yes they can be executed in parallel, right. And of course in simple pipeline the processor you also use these things to stall data and control hazards, okay. Whereas for VLIW, EPIC and superscalar processor they are meant for exposing the parallelism, instruction level parallelism. We will give examples of these things as we go by, right. Essentially the clue is important point here is that we need to make sure that these independent instructions, right, are exposed as parallelism. That is really the key point, okay. In this example we saw in the in yesterday's class you have a sequence of instructions, right and here is there is a data dependency. The load instruction writes a value into R3 transistor which is being used by the add instruction. Typically load instructions have a stall of one cycle that means that the load value is only available during the mem phase of the pipeline and the mem phase is always after the execute phase. Therefore if the next instruction wants to execute this it has to at least wait till the mem phase of the next instruction and that is where the stall is kind of introduced. So both these instructions essentially generate one-one stall each, right and therefore you have two stalls following the two loads. But if you do instruction scheduling and then replace the loads around or move the loads around then you can avoid all of these stalls as you can see in this example. Here what you have done is that between the load and the dependent add instruction you have another load instruction and that itself ensures that this load and the dependent add instruction are at least separated by one instruction, right. Therefore this schedule which you see on the right hand side essentially satisfies all the stalls and removes them. It satisfies all the dependencies and removes all the stalls. Again you can see that the data dependencies that you see in the original source program are always observed in the instruction in the scheduled program in the reordered program, right. The essential dependencies that we talk about is from this R 3 to R 3 that dependency is being preserved, right. Similarly this R 13 to R 13 that dependency is being preserved. There is a dependency from this R 3 to this R 3 that is also being preserved, correct. That is why we were unable to move either of these add instructions. Both of these add instructions are dependent, right. So, we could not have moved that thing. We cannot even move this instruction ahead here because this add is dependent on the load. So, you cannot move this earlier than the load. When you talk about instruction scheduling essentially to hide stalls it is still a serial schedule, serial but reordered schedule. Right there you do not talk about resource constraints because every instruction means in every instruction contains only one operation. So, there is no notion of a resource constraint that you talk about. But of course, if you have non-pipelined units then there are certain other things that you need to do but we will not get into that right now. Okay. So, if you have a superscalar architecture, right, which exploits instruction level parallelism then it may have multiple function units and during the instruction fetch and decode phase it can fetch and decode multiple instructions. These multiple instructions are put into this issue queue or some kind of a buffer where they wait for the dependencies to be satisfied and after the dependencies are satisfied, right, whenever they are ready, data ready, they can go to the respective function units and can start execution, right. So, if an add instruction and a load instruction are independent of each other we can actually put them next to each other so that they could be fetched, decoded and then issued also in parallel, right. That is what you do in the superscalar processor. Instruction scheduling helps in superscalar process but it is not mandatory, okay. Without even instruction scheduling also the superscalar processor can identify the parallelism and can export. Maybe it can identify only lesser amount of parallelism and exploit that because independent instructions are far apart from each other. Typically what happens in a superscalar processor there is something called a reorder buffer, right. The reorder buffer is the extent to which it can actually see the instruction, reorder buffer or the issue queue is the extent to which it can really see, you know, independent instructions and expose parallelism, right. Whereas the compiler can see a much larger window of instructions and can possibly move that ahead and then allow this to exploit parallelism. So, remember that in the case of superscalar processor instruction scheduling is preferable and it helps to expose the parallelism but it is not mandatory, right, because the architecture is smart enough to identify whatever it can identify the parallelism. Whereas in the case of VLIW processor instruction scheduling is mandatory. If you do not identify independent instructions and put them together the processor cannot exploit. Whatever you specify as a parallel operation, right, in a single VLIW word that is the only thing that it can execute, right. So, in that architecture the compiler's role particularly, I mean, the compiler role in terms of doing instruction scheduling is very important, okay. So, the VLIW processor again we saw this in the last class consists of a simple instruction memory from where instructions are fetched but the fetched instruction could be a long word meaning it could be 256 bit, 128 bits or 256 bits or 512 bits and it may contain multiple operations but these multiple operations that are there in a single word they are independent of each other and they can be executed in parallel. So, the hardware in a VLIW processor essentially decodes that instruction and then moves those instruction to the respective functional unit and those functional units can actually start executing them because when an instruction is issued it is assumed that it will have its operands available. So, you do not need to necessarily check whether the instruction is data ready to be executed. The operands would be anyway available. So, it can be issued and then in the next cycle it can be executed, right. And after they finish executing they write the results either in the register file or in the memory and of course, this instruction execution itself can be pipelined, VLIW instruction execution itself can be pipelined. So, what we will see next is that we will see one example program. So, this is again an architecture that we talked about which has 256 bit instruction word, 7 operations can be done in parallel, it has one branch, two integer, two memory operation and two floating point operation in every cycle and so on, right. So, here is how the instruction schedule looks for a VLIW processor, okay. Let us see if we can understand. Unfortunately, I did not put the initial part of this example here, but it does not matter. We will try to understand what this code is trying to do, right. So, you can see that in this particular example I have taken memory operation, floating point operation and integer or branch operation. So, I have taken an example of three operations in every VLIW word, okay, right. And then you can see that in each one of this, it would have been nice if I have taken, okay. Let me try to do the following. Let me first try to, just give me a minute here. Let us see what this code is because this is when you will appreciate what this is doing. So, let me try to do this. Okay. So, let us say this is my original program, right. You are able to see that, correct. So, can somebody look at it and then tell what it is trying to do? Let us say that R 1 contains the address of an array A, right, contains the address of A of 0, let us say then it loads that value into F 0 register, it loads that value into the F 0 register, it adds it with some scalar value, okay. F 2 is the other scalar with which it is adding, putting the result in F 4 and then storing the result back in the same memory location. So, this is a code which says A of i is equal to A of i plus s, correct. And then it is incrementing the pointer by 8 assuming that these are double precision floating point numbers, correct. And then what it does is it subtracts the loop count by 1 and when the loop count becomes 0, it comes out of the loop otherwise, right. Now assume that I have unrolled this loop, right, 6 times, right. That is why you see 6 of those loads, right. So, you have 5 loads here that corresponds to 5 times unrolling, isn't it, right. So, you have 5 loads, 5 floating point adds and 5 stores, correct. So, those are the 3 instructions which are what are here. This of course is incrementing the pointer, right. Now how am I loading A of 0? It is 0 of R 1. A of 1 is offset 8, right, from A of 0. A of 2 is offset 16 and so on, right. So, each one of this load is doing A of i, A of i plus 1 and so on. So, yesterday when you are unrolling the code, were you seeing something like this in the, yes or no? How many of you saw this? Okay, how many of you did not see this? Did you? Yeah, yeah, don't feel bad to say that you did not see it. Maybe the compiler did not do it for you, right. It is not your problem. It is a compiler's problem. So, the question is did you use the minus o 2 option? If you use the minus o 2 option, it would not try to add 8 to this and then try to do this, add 8 to this and then try to do this because when you add 8 and then try to do this, there is a dependency chain that you are creating, correct. Whereas when you do like this, all these instructions are independent of each other. Any of these instructions could be executed in any order, correct. If I have done this instruction, add 8 to R 1, then 0 of R 1, then another add 8, 0 of R 1, another add 8, 0 of R 1, then it would have become sequential. But just by using these offsets appropriately, right, there is no dependency between these load instructions. But there is a dependency from this load instruction to this add instruction because this loads the value into F naught, that F naught is being used here. And then you compute some F 4 and that F 4 is being stored here. So, this dependency that you see here is a original dependency which was there in the code and you want to retain the dependency, right. That is what we mean by saying that dependence constraints have to be satisfied, right. We are not violating that. And look at this, this load and this add are separated by at least one cycle. That means that this one stall that we talked about has also been taken into account, correct, right. And then in cycles 3, 4 and 5, you have a load instruction and an add instruction executing together because these are independent of each other. They can be executed in parallel, right. And if you look at this particular cycle, right, you have a store instruction, an additional add instruction and a subtract instruction. So, one integer operation, one floating point operation and one memory operation. All of this can be executed in parallel. If you have unrolled this loop more number of times, for example, 8 times or something like that, more of these instructions could have been in parallel. Now, let us look at the instructions which are integer instructions, right, and then see what is happening here. See this subtract instructions when we unroll, right, 5 iterations of the loops are executed in this one unrolled version. Therefore, the count has to be decremented by it could have been decremented by 1, 5 times or it would have been decremented by 5 one time, right. That is right. So, this is 5, right. Similarly, this add instruction which was incrementing the R 1 pointer by 8 for each iteration can now be incremented by 40 because we are incrementing for every 5 iterations and these offsets are appropriately taken into account, okay. Now, anything else that you see strange in this code or you are happy with it? So, this branch instruction is dependent on this R 2. There is no problem. The dependency is being preserved. If you think of a branch delay slot, this is the branch delay slot in which a useful instruction is being put. That means the branch has been moved a little earlier, okay. That is fine and every one of these instructions satisfies the resource constraint because exactly there is one memory operation, one floating point operation and one integer operation every cycle, right. What else do you see as something different can be discussed about? The first two, first three stores are all right, 0, 8, 16. Then you should have seen 24 and 32, correct, but you are not seeing that. Why? Right. You see here what is happening to your R 1. R 1 in between has been incremented to incremented by 40, correct. That means that after this instruction R 1 is already R 1 plus 40. That means that the pointer has been moved to A 5. So, if you want A 4 and A 3, they are minus 8 and minus 16 from the A 5. So, that is why these have the offset values minus 16 and minus 8. In other words, other way of looking at it is that this add instruction should not have been moved before the store, but if you want to move it, you have to adjust the offset values. Otherwise, you will be violating the dependence, correct, because if you have moved this by 40, incremented this by 40 and kept 24 and 32 here, then you will be writing some A 8 and A 9 instead of A 3 and A 4, correct. That is wrong. So, when the scheduler does this, it can possibly do this also, right. It should possibly do this also, not it can. It should do this, right. So, this schedule which is the schedule obtained for this loop over here, right, which was unrolled five times and scheduled for a VLIW machine with three parallel operations. Of course, I should have considered eight parallel operations, but then I could not have been able to fit it in the slide, could not have been able to fit it in the slide. I have to unroll it maybe eight times or 16 times to get the parallelism. What else do you observe here? Very simple observation is that in many places we have dashes and what does this dash means? It is no instruction for that or no op for that. That means that these are wasted opportunities for parallelism. My code did not have enough parallelism. That is why I was not able to exploit it. If I have unrolled it maybe eight times, right, I would have had fewer of these tools compared to the number of instructions that I have. Okay, what else do you observe since we have talked about the other point? Is there anything else that you observe with regard to let us say register allocation? Such an exercise that you will probably do today, right. So, how many registers, how many floating point registers was the original code using? F naught, F 2, F 4, correct, three registers, but it is actually three into two, six registers because each floating point register is actually here, two registers because it is a double value, but again even if you take it as three, it is only three. What about here? Yeah, something like that, right. And if you have unrolled it eight times instead of five times, it would have increased more. If I have unrolled it 16 times, it would have increased more. What would have happened? I would require even more number of registers and when I do not have that registers, I would have to spill, right. So, you cannot be stretching this unroll, unroll, unroll and then you can put more instruction. Somewhere it is going to come back and then tell you as you unroll, your register pressure is going to increase and if the register pressure increases, you will introduce spill code which is waste. So, there is some kind of, right, conflicting requirements happening. You can unroll and then expose more parallelism, but as you unroll, one thing that happens is that the number of instructions increase. That is also true, right. Your code earlier had only six instructions. Now you have this many instructions, right, ten instructions or something like that. So, your code size increases. The code size increases, what happens? We do not know. The code will not fit in the instruction cache, ok. Whoever who suggests raise their hand, they could get the appreciation. That is good. So, it will not fit into the instruction cache and therefore more instruction cache misses can happen, right. That is why you do not want to unroll the loop infinite number of times, right. If you unroll it eight or sixteen, you will increase the thing by a factor of nearly eight or sixteen. It increases the code size, so potentially there could be instruction cache misses that could happen. You will increase the register pressure, potentially there could be spills. So, do this, but do it carefully is the advice, ok. Alright, so we have really not seen how instruction scheduling works, but we saw an example of an instruction schedule and how that is helpful in terms of exploiting instruction level parallelism, ok. Now let us go to the nitty-gritty details of how to do instruction scheduling. That is if you are given this loop or if you are given the unrolled version of this loop, but still a sequential code, how do I generate this code? How do we do this magic, right? That is the question. Because if I take the unrolled version of the loop, what would I have seen? I would have seen three of these instructions, another three of these instructions, another three of those instructions, another three of those instructions like that and then maybe the control transfer instructions in the end. But from that, how do I get this, this parallel version, right. I need to understand the dependencies between the instructions. I need to understand when this add instruction can be scheduled. I need to understand when this store instruction can be scheduled and so on and so forth, right. So, we need to talk about all of these things. Question? No? Okay, right. So, we will we will get to that details now, okay. So, as I mentioned earlier we do instruction scheduling to reduce stalls which is basically control hazard and data hazards, okay, to exploit parallelism in super scalar processors and VLIW processors, okay. And the instruction scheduling that you do must necessarily obey data dependence and resource constraint, okay. When you talk about instruction scheduling within a basic block, it is basic block scheduling. When you talk about instruction scheduling beyond basic block, it is global scheduling, So let us get into basic block instruction scheduling. First we will talk about instruction scheduling on a processor which is a simple pipeline processor. That means that we are not interested in exposing parallelism, right. Later we will also talk about parallelism, okay. So, basic block scheduling or instruction scheduling is essentially a reordering of instructions within the basic block, right. And what we try to do is that we want to minimize the execution time of this instruction schedule that essentially reducing the schedule length or schedule length is the total time it takes to execute this instruction, okay. Again there are different instruction scheduling methods. Again instruction scheduling problem is an NP complete problem, okay. If you want to do optimal instruction scheduling that is an NP complete problem, right. And therefore, what you can say is that instead of getting the optimal schedule which is going to take a very large amount of time, one could actually go for some heuristic methods which will give you close to that good solution but at a much smaller computation time. So, this typically talk about heuristic scheduling methods and they differ in terms of whether they do operation based scheduling or cycle based scheduling. We will talk about examples of these two things or we will talk about these two algorithms in detail and also examples of these. You can also do an optimal instruction scheduling, right. You can also do optimal instruction scheduling using integer linear programming approach or using other approaches. But the time to compute the schedule could take very, very long time because these are NP complete problems, okay. Or you could have used other kinds of evolutionary algorithms like genetic algorithm, right or simulated annealing kind of algorithms and so on, okay. Now, before we do instruction scheduling as I mentioned earlier instruction scheduling has to obey all the dependence constraints. So, how do we identify this dependence constraint and how do we ensure that they are satisfied? Again to do this thing what we do is that we represent the basic block in terms of a data dependence graph, okay. And in this data dependence graph each instruction is a node, right. There is a directed arc from instruction i to instruction j if j is dependent on i, right. So, that is what it says there is an edge directed edge, okay from u to v, right. If there exists a dependence from instruction u to v. So, here I talk about true dependence, okay. So, let me just say how many of you know about true dependency, anti dependency and output dependencies? One, only one. How many of you know raw dependency, war dependency and war dependency? They are the same, yes very good, right. So, those who know this same thing, okay. Now, let us see what it is, okay. Let us first start off with this example and then come to the dependence graph. So, can you just tell me where there is a raw dependency in this? i 1 to i 2, correct, right. Where else? i 2, i 4, okay. i 1 to itself, i 1, i 3, okay. I am sorry i 1, i 3, correct. i 3, i 4 should be there, right. All of these dependencies are what are called raw dependency or true dependencies, right. Raw is read after write. These reads must happen after those writes have happened, right. And this dependency is a dependency which must necessarily be observed in the program, correct. Because you expect this add instruction to take the value produced by this load instruction. You expect this load instruction to take the value produced by this add instruction and so on, correct. So, these are what we are going to call as raw dependency or true dependency. So, what is a war dependency? Write after read. Can you give me an example here? Is there one? No, right. There is none here, okay. Let us just try to change this to R3, right. Then now what happens? In this case, this write of R3 should happen only after this read of R3 has happened, correct. So, this is what is called a war dependency. The dependency between them let me, right. So, the dependency that you see from this instruction to this one, okay. This is what you are going to call as the war dependency, right after read dependency. This is also called the anti-dependency. Dependency, anti-dependency, correct. Why does this anti-dependency happen? Now that you people have seen register allocation, right. Let us say that this is the code generated by your register allocator, right. At that time you thought you did something very smart, right. Exactly. There was a variable which was live up to here and that was using the register R3. That liveness ended here. A new liveness started, new variable started here for which you gave the same register which was okay because the live ranges were not conflicting. You did the smart thing by giving the same register, right. But now what have you done? You have created an anti-dependence between these two instructions, isn't it? So, the anti-dependence happens because you are reusing the same register, okay. Let me give you one more example, right. Let us say that this instruction instead of using R5, let us say was using R1. Then what happens? Then we introduce what is called the war dependency. So, between this instruction and this instruction there is a wow hazard or sorry wow dependency, W A W right after right. What do we say that? This right must happen only, sorry this right must happen only after this right has happened or this right must happen before this right happens. So, this is essentially called output dependency. Output meaning destination, correct. So, in this example what we have seen is that we have seen true dependency, anti-end, output dependency. Among all these dependencies why do we only call this as the true dependencies? Because these two dependencies are false dependencies. Why is that they are false dependencies? They happen essentially because you are reusing the same register, correct. If instead of using R3 here, if you have used some R13 there would not have been any dependence between these two instructions, correct. Similarly, instead of using R1 if you have used R21 there would not have been any dependence between I1 and I4, correct. So, the dependence has happened because you are reusing the registers, not the true dependence which is dictated by the program. Whereas, this dependence that you talk about between this R3 and R3 that a true dependent the program wants is if you rename this register to R13 this also has to be R13. If you rename this to R24 this also has to be R24 because here you want to take the value produced by add and use it in this add instruction, right. So, this is a true dependence no matter how you do register allocation this dependence will always exist. This dependence anti-dependence was introduced by your register allocator or introduced by something that you have done during code generation phase. It is not what is there in the original program. Maybe you reuse the same temporary variable, right. In your program you always write temp is equal to something and after some point in time, okay let me reuse the temp instead of declaring one more variable, right. That is exactly what has happened here, right. That is why you have this anti-dependency and that is why you have this output dependency. That is why anti and output dependencies are together called false dependencies whereas, this one is called true dependence, okay. Alright, now I need to find a mechanism for clearing this mess. So, we all understand all these dependencies, right. So, if somebody asks you now what is an output dependence or what is an anti-dependency you should be able to. Okay, now let us get back to our data dependence graph, right. So, in this data dependence graph you are going to say that each node represents an instruction and there is an edge from one node to the other if there is a true dependence. Now, you understand why we talk about true dependence, okay. Of course, we need to worry about anti and output dependencies as long as they use the same register names but then that is something that we can talk about it later. Remember I said that typically instruction scheduling is done first in the compiler and then register allocation. If instruction scheduling is done first then it will be using only temporary variables. There the chances of seeing anti and output dependencies are lower. So, you do not necessarily see them, right. Therefore, it is okay to only talk about true dependencies in those cases, okay. Now here is the sequence of instructions. We already analyzed all the true dependencies and those true dependencies are depicted by means of an edge. Now it is a directed edge, remember, okay. In addition to this what we are going to say is that we will also allocate or assign weights to each one of the nodes that will essentially tell how long it takes for that instruction to execute, right. When I say that this takes one cycle to execute that after one cycle this node will produce the value which can be consumed by this. When I say two cycles here after two cycles only this can produce the value. So, if this is scheduled at time t then this node can only be scheduled at time t plus 2 or later because this is not going to produce a result value until t plus 2, right. Similarly, if this is scheduled at time t 2 then this can also be scheduled only after t 2 plus 2, correct. That is really what we mean. So, each node has a weight assigned to it or associated with it which tells you how much time it takes to execute this node. Sometimes we will put the weights on the node, sometimes we will put the weights on the edges. The kind of mean more or less the same thing. When you put the weights on the node it means that all the outgoing edges have the same latency. When you put the weights on the edges then you can individually specify this has one latency, this has two latency or this has five latency and so on and so forth, right. You can do it either way but you only need to do one of them not both, okay. Either associate the weights with the nodes or associate the weights with the edges, one of the two things that you do, right. And what do these weights represent? Number of cycles or amount of time it takes to execute that node, okay. So, when you talk about this schedule for pipeline to processors where you are only talking about avoiding stalls, the schedule problem is essentially defined as below. Basically, you have an instruction in the basic instruction sequence in the basic block which is I 1, I 2, I 3, I m and you want to reorder them such that the number of stalls is minimized. That is really what you are trying to do and this reordering is going to be in such a way that these m instructions are going to be permuted in some form, right. Their positions are being changed in some form but then in the change the form you essentially make sure that the dependences are again satisfied, right. So, the instruction reordering in this serial schedule is essentially a permutation function f on 1 to m such that f of j identifies the new position of instruction j, right. But then whatever is the new position of instruction j, if j is dependent on some k, sorry in this case, yeah, okay. If k is dependent on j, then f of j must necessarily happen before f of k, right. So, that is essentially saying that all instructions on which an instruction is dependent must necessarily happen before, right. Here we are not worried so much about the delay cycles and other things because it is a serial schedule and we are trying to put them and the delay cycles essentially tell you that if you do not have enough stall cycles between them, they will be introduced as stall. And your objective is to minimize the number of stalls, okay. That is really what it is, okay. Now, let us define a few terms before we go into the details of instruction scheduling, right. We define the start time of a node which is the time in which it starts executing, okay. Obviously, the start time of a node j has to be greater than or equal to 0, right. And two instructions can have, okay. In this particular case, I am saying the two instructions cannot have the same start time. That means that I am putting one instruction in every cycle. So, this is still the serial schedule that we are talking about, right. And if there is an edge from j to k, that means that k is dependent on j, then the start time of k has to be greater than the completion time of j. And what is the completion time of j? Which is basically start time of j plus execution time of j, right. So, that is essentially what it is, okay. Now, the schedule length is essentially the completion time of the last, not necessarily the last node in the schedule. The node that finishes last or finishes execution last. That means that for all the nodes, you find out what is the completion time, whichever one which ends last max, okay. That is the one that we are going to take as the completion time. So, essentially if you are given an instruction sequence and you construct a schedule that satisfies these properties, correct, which is basically the resource constraint, right, then the cost of that schedule is essentially the make span of that schedule, which essentially tells you how long it takes to complete execution of all the instructions in the schedule. And the idea is to minimize the schedule length, correct. That is really what we want to do. Again, we will see examples. That is how we understand things, right. So, again I have taken the same program, okay, and I want to construct a serial schedule for this, okay. So, here is one schedule where I do I1, I2, I3, I4, which is the original schedule, right. Now, what happens? Okay, I may have made some mistake, but let us see. So, this takes five cycles, okay. So, this must not, this must have been one instead of two, okay. So, let us do the correction, okay. So, for example, let us say that this is not two, but one, okay. Then this schedule, okay, should have checked this. Sorry about that, okay. So, let us start off with this example again. Remember that I2 is an add instruction and I3 is a load instruction. So, the load instruction is the one which takes two cycles. Add instruction takes only one cycle. By mistake, I put two here. So, ignore that for the time being, okay. So, if I have one here and if I do this schedule, because I3 takes two cycles, I3 will only complete at the end of the cycle and I4 can only start here, right. Whereas, if I do this schedule where I first schedule I3 and then come back and schedule I2, right, by the time I2 finishes, I3 would have also finished, because I3 takes two cycles and I2 takes only one cycle. Therefore, I can now start I4 in the next cycle and this schedule would have resulted me only four cycles, whereas this one would have resulted five cycles. So, the instruction reordering problem essentially says that the instruction order should be I1, I3, I2 and I4, correct. Okay. So, with that understanding, let us just move forward. Okay. Again, this is the same idea, but let us just try to explain this also, okay. I have taken another data dependence graph and in this example, instead of attaching the weights to the nodes, I have attached the weights to the arcs. As I mentioned earlier, this representation is useful only if the different arcs from the same node have different weights. Otherwise, I could have attached it to the nodes itself. So, for example, if this takes one cycle and this takes two cycles, then this representation is useful. But if both of them take only one cycle because both of them take only two cycles, then attaching weights would have been the same thing, right. So, essentially we have a data dependence graph and the arcs represent dependence, right, from node I to node J, okay. And the edge weight or arc weight essentially tells you how much time it takes for the node to produce the result. Now the goal of instruction scheduling is to construct a schedule such that the length of the schedule is minimized. And the length of the schedule is typically defined by what is called the critical path, right, the path which takes the maximum amount of time, right. So, in this example, right, 1, 3, 4, 6 would be the critical path if d 1 3 plus d 3 4 plus 6 has a value which is greater than d 1 3 plus d 3 5 plus d 5 6 or d 1 2 plus d 2 6 or d 1 2 plus d 2 4 plus d 4, right. So, you call this a critical path only if that path takes more amount of time. What do we mean by this? By this we say that if node 1 is scheduled, for example, let us take some examples, right, supposing let us say this is 1, this is 5 and this is 3, okay. And let us just take all of the others as 1 just to make things easier, right. Now see what happens, right. If this node is scheduled, if node 1 is scheduled, right, at time step 0, node 3 can be scheduled at time step 1 and node 4 can be scheduled at time step, yes, it depends on this also, correct, but the earliest it can be scheduled is 6, right, because this result is going to come no earlier than time step 6, okay. Let us assume that the other nodes get completed before that. And then what about this? What about node 6? The earliest it is going to get scheduled is time step 9, correct. So, this path takes 9 units of time, whereas this path takes only 2 units of time, this path takes 5 units of time, this path takes 3 units of time. So, this is the critical path, right. And what does the critical path tell you? Critical path tells you that these nodes are important nodes, do not delay scheduling them, correct. If you delay any of these nodes by one cycle, what happens to your critical path length? It increases by 1, because this node is going to finish later. Whereas if you delay this node or this node, it may not matter that much, correct, because you have some buffer amount of time, right. This is how you schedule your work, right. So, that is essentially what is being done in this. So, when we talk about instruction scheduling, we will always talk about this critical path either directly or indirectly and try to make sure that the nodes on the critical path are scheduled as soon as possible without delaying them, right. Okay, so the heuristic series scheduler is going to do the following things. You construct the data dependence graph and for all the nodes in the dependence graph, okay, you want to schedule them. You can have some kind of a rank function. We will discuss what the rank function is little later, okay, but right, but then you basically try to schedule the nodes based on this rank function. The rank function could be either the node is critical or non-critical. Critical nodes are given more priority than the non-critical nodes, right. Now, for each instruction j, what you do is that you make sure that how many instructions on which it is dependent on. That is basically what we call the number of predecessors, right. Until all of those nodes finish, you cannot schedule j, right. So, you do all of this preparatory work and then you start your instruction scheduling. First, you schedule all the nodes which has predecessor count equal to 0. That means that these are the what we call as the root nodes or the start nodes in your program. They do not have any dependencies. They can be scheduled in the initial cycle, right and as and when you schedule them, you make sure that their successors are decremented. The successor predecessor counts are decremented which is being done by this. Of course, you need to do this only for the at the appropriate time, but in this example we will not really worry about that too much. Let us assume that all of them have one cycle or so. So, it can be immediately decremented, okay. So, you schedule j and then you continue you kind of update all its successors, their predecessor counts and as and when there are more instructions in your ready queue, you start doing this. And when you pick a ready instruction, you always pick the ready instruction based on some priority which could be critical versus non-critical, right. That is really what you do. These functions for doing update of predecessor count is that basically you look at all successor and for every successor, you decrement its predecessor count by 1. That is really what it is and if the predecessor count of that node becomes 0, you can put it into the ready list, okay. This simple scheduling essentially has an order m squared complexity because at every step you need to kind of look at this all other things. So, it is an n squared complexity algorithm, right. And even constructing the DDG where you try to find out sorry where you try to find out whether a node has a dependence to any other node, you essentially have to consider all node to every other node in the graph. So, it has a complexity of order n squared in the worst case. An interesting result which was proved in 1987 is that the heuristic scheduling that you try to do is always within a factor of 2 from the optimal schedule. That means that you will never ever do worse than the 2 times the best schedule. But let us not worry about it. We want to do as good as the best schedule, okay. Now, let us talk about one kind of a rank function for nodes. We said that one possibility is critical node versus non-critical node. That is more like a binary decision, right. But we can do something better than that and that is typically decided by what is called the earliest start time and the latest start time, right. So, let us define these two functions, one called EST and the other called LST, latest start time, right. Let us see what it is. The earliest start time, okay, is essentially the earliest time in which a node can be executed. That means that for the start node, it is 0 because all the root nodes can be started to start with. But then for any other node, it is the start time, earliest start time of its predecessor plus that node weight. But then among all these nodes, you should take the one which is giving you the maximum because it can have multiple predecessors. Whichever predecessor which gets executed last, that completion plus its completion time, not necessarily scheduled last, but scheduled plus its completion time, whichever one which has the greater value, that is the one which dictates how soon this node can be started, right. So, that is the earliest start time. I will give you an example in the next slide. So, do not worry. You will understand that better. The latest start time is actually the reverse of that, right. If I am, if I want to schedule this node as late as possible, but still meet the overall critical path length, how late can I schedule? That is really what is the latest start time. And to compute the latest start time, what you do is that you first compute the critical path length. Critical path length is essentially the earliest start time of a node, okay, plus its execution time across all nodes. That is the completion time. And that completion time is same as the start time of the end node, which is our fictitious end node. But let us call this as the completion time. Then what you do is that for every node, you say that the latest start time is basically the minimum of the latest start time of its successor minus the weight of this node. That is, if that node has to start at time t, I have a weight of 2, then I must at least start t minus 2 cycles before. That is really what you do. And if you, if that node has, sorry, if you have multiple such successors, then among all of them, whichever one is the minimum, that is the latest start you can do. Because if you delay it further, potentially you might be affecting the critical path. That is really what it is. So, we will give you examples of this. Rank is essentially the difference between the latest start time and the earliest start time. If the rank is 0, that means it should be given very high priority because there is no slack between its earliest start time and its latest start time. Only critical nodes in the critical path will have slack 0, right. And when you have slack 0 for the nodes on the critical path, you essentially say that they have to be scheduled as soon as possible, right, and they have no slack. And that is why they have higher priority. So, let us, let us see the same example. I have taken the same graph, okay, and here correctly I have put the weight as 1, thankfully, right. And then I have introduced two fictitious nodes, start and end, right. There is an arc from the start node to every node, but here I have only shown it to the first node, right, but you can actually put it. Similarly, there is an arc from every node to the end node, but I have only shown it for this just for the sake of clarity. You will see how that doesn't really matter. Now, this node can start at time t 0, right. Its earliest start time is 0, okay, right. Because this is the first node, it can start at time 0. What about this one? What would be its earliest start time? If this starts at 0, this can start at 1, right. What about this one? That can also start at 1, right. What about this one? 3, because 1 plus 1 is 2, whereas 1 plus 2 is 3 and I have to take the maximum of these two things. So, let us see whether it calculates these things correctly, right. It does, right. And then I say that the earliest start time of end, which is same as the maximum of all of these things, well, this completes at 4. So, this is 4, right. Now, from here I am going to work backward to compute the latest start time. If this has to complete at 4, this must start at 3, otherwise it won't complete at 4. If this has to start at 3, what about this one? This has to start at least at 2, because this has only one latency. What about this one? It has to be 1, right. So, its latest start time is 1. What about this node? This latest start time is 1, this latest start time is 2. Let us start from here. This 2 minus 1, this says that the latest start time is 1, right. Whereas, if I take this 1 minus 1, what does it say about its latest start time? 0. Since for the latest start time I have to take the minimum of the two, I will take this as 0. So, let us see what happens. CPL is 4, LST is 0. So, these are the values that we have calculated, right. So, let us say for node 1, the earliest start time and the latest start time are both 0. For node 3, the earliest start time and the latest start time are both 1. For node 4, the earliest start time and the latest start time is 3. Which is the critical path here? This is the critical path, right, because that is the one which takes maximum amount of time. This is not the critical path, because on this path if I add the weights of all the nodes, it is only 3. Whereas, if I add the weights of all the nodes here, the weight is 4, correct. So, for the nodes which are on the critical path, the earliest start time and the latest start time would be same. And because they are the same, the rank value for them are 0. Whereas, for all the other nodes, the rank will be greater than 0. Now, at any point in time when you have multiple ready nodes, you can choose the nodes based on the rank. The ones which have the lowest rank can be chosen first. If you have multiple nodes and you can only schedule one of them, the ones with higher priority will get scheduled, the others will be delayed. That is really how it goes. So, that is as far as the serial schedule is concerned. We will now move on to more interesting things like the parallel schedule, multiple ALUs, VLIW architectures and so on, so here this problem essentially becomes an important problem because you now have M resources, a machine with M identical. Let us say to start off with identical resources, then we will talk about different types of resources and so on, right. If I have M identical resources and I want to schedule these things, right, then that particular problem for M greater than or equal to 3 is NP complete. This is again what is called the job stop scheduling problem, okay. I will try to explain some notions here before we move further. So let us try to write down something, right. This notion of what are called identical function units and clean pipeline, right. So, two things that we talked about, identical function units and clean pipeline, okay. First of all, let us just talk about pipeline function unit. We say a function unit is pipeline, okay. If every cycle you can put an operation into it and now we are talking about function units, not just the instruction execution pipeline. Remember that in the instruction execution pipeline, you have the instruction fetch phase, the decode phase and then the execute phase. The execute phase is essentially what corresponds to the function unit. If you have an add instruction, maybe there is a simple add function unit. If you have a multiply, then you have a integer multiply function unit. If you have, let us say a floating point add, you have a floating point function unit and so on. In the superscalar architecture, we saw these things as different functional units, right. So these functional units again depending on what they perform may take one or more cycles. If you want, I can quickly show you. So, each one of these function units depending on, okay. So, they may take multiple time steps to complete, but if it is fully pipelined, then you can actually start issuing an operation every cycle. Let me take an example. Let us say that I have an integer, okay. So, let us say that I have this integer multiply function unit, which takes four latencies, four cycles time, right, latency of four cycles. That is essentially a pipeline with four stages, correct. I do not even know what this is, okay. Let us just call them stage one, stage two, stage three and stage four. This is only for, let us say, floating point multiply, right. And if I say that this is pipeline, then what it means is that every cycle I can initiate a new operation in this, right. That means that at time t equal to one, that could be instruction i one, right, going through this i one, i one, i one, i one. So, this is time one, i one, i one, i one, i one. So, this is time two, three, four, right. In the next cycle, I can start the next operation and in the next cycle, I can start the next operation and so on. And they will go through the pipeline completing it one cycle through. Correct. But the instruction still takes, the floating point multiplication still takes four cycles. So, the latency is four cycles, but it is pipeline. And when we say it is pipeline, you can issue one operation every cycle. How is this important from a scheduling perspective? From a scheduling perspective, if I schedule, let us say, a floating point multiply operation in time t, I can schedule another floating point multiply operation on the same pipeline at time t plus one. If it is not pipeline, what does it mean? The one can only be issued at time t plus four, exactly. Correct. So, that is why this may not be possible and you can only do that. Okay. So, that is what we mean by pipeline versus non-pipeline functional unit. When I talk about identical, when I say identical, it is actually a simplification. In practice, what happens is that you have a separate function unit for integer operation, you have a separate function unit for floating point, you have a separate function unit for load store, you have a separate function unit for let us say integer divide or floating point divide and so on and so forth. They are all different. Okay. But to do the instruction scheduling, sometimes we will assume that let us say I have n pipelines and all of them are identical. But in practice, this does not happen. Okay. In practice, it is always each function unit is unique or different. Right. So, but for simplification, we will consider it to be that way. Oftentimes, we will either use this word called homogeneous, which means that all of them are same or heterogeneous. Okay. In practice, it is always heterogeneous, but to make the problem simple, we can look at it as homogeneous. Even when you look at it as homogeneous, when you have more than three functional units, right, the instruction scheduling problem, optimal instruction scheduling problem is NP complete. So, if it is heterogeneous, it is going to be equally or more hard. Right. That is the reason. Okay. So, as I mentioned earlier, different resources, for example, integer operation, memory operation, floating point operation, etcetera, they all go through different pipelines. And each one of these pipelines could either be fully pipelined or non-pipelined. If they are pipelined, then every successive cycle, you can actually initiate a new operation, even though the latency may be greater than one. Whereas, if it is non-pipelined, you can only initiate after that operation completes. That is really what it is. Okay. Alright. So, we will see some instruction scheduling methods, essentially what you are going to call as the list scheduling methods. They differ, there are many list scheduling methods that have been published in the literature. And they differ in terms of whether these are, so what happens is that there are several list scheduling methods. And these list scheduling methods depends, you know, varies in terms of the following aspects. Some list scheduling methods are operation based and some are cycle based. I will give you examples of these two things as we go by. Some are forward and some are backward or some are what we call as greedy and some are lazy. And then they use some extent of backtracking, which essentially means that after I have done scheduling of an instruction, I try to go forward and as I keep scheduling more instruction, suddenly I decide that one of the instruction that I have scheduled earlier is possibly a wrong choice. I try to undo that instruction and then try to schedule more instructions. So, that is backtracking. And if you do backtracking, like in any backtracking things, you actually spend more time, but you are likely to produce better schedules. Then they use different kinds of priorities and some of them update this priority only once at the beginning of scheduling. Some of them keep updating the priority as time goes by. So, they differ in that as well. They also differ in terms of how they consider these functional units, whether they consider these functional units as uniform, that means homogeneous or heterogeneous and whether they consider how many units are there, etcetera, depending on that they differ. So, here is the algorithm for doing what we call as cycle based scheduling. So, what you do is that you have the instructions from the basic block from which you construct the data dependence graph. The data dependence graph is represented by means of the edges and sorry vertices and edges. Now, what you do in a cycle based scheduling is that you start with time step 0 and you keep visiting every cycle. And as you keep visiting each cycle, you will see whether there are any ready operations to be scheduled. And among all the ready operations, you choose the ones which have higher priority and you start scheduling them in the decreasing order of priority. If you cannot schedule any more operations, you increase your time step to the next cycle. And before you increase our time step to the next cycle, you also make sure whether there are any new ready operations that have become available. Then in the next cycle again, you try to schedule the operations based on the priority and you keep doing this. So, that is essentially what this algorithm is. Let us go through the detail. To start with your schedule is initially empty and you start off at cycle t equal to 0. Now, you start saying that ready list is the list of all nodes which are the source node in the graph. That means that they do not have any predecessors. They are always ready. They can be executed. So, now prioritize this ready queue because you do not, there may be five nodes that are ready in the initial list. There may be five source nodes and you may have only slot for scheduling two of them or you can only do two integer instruction and one floating point instruction. So, you have to now see which ones can be scheduled. So, what you do is that you prioritize them. Then, right, while schedule is not complete, that means that while you have not scheduled all the nodes in the graph, keep doing the following, right. What you do is that you first take a node from V, from the ready list in the priority order, okay. In the prioritize order, you take a node and then you try to schedule the node in the current time step, okay. And when you schedule the node, you have to essentially ensure that there is no resource conflict. That means that if in the current schedule, let us say you have already scheduled two integer add operations and there are only two integer function unit, then the third odd operation cannot be scheduled in the current cycle no matter how high its priority is. Because the other two nodes must have had higher priorities. That is why they have got scheduled. This node has a higher priority than some of the other operations, but still it cannot be scheduled in the current cycle. So, you have to skip this operation and go to the next operation in the priority queue and then see whether any of them can be scheduled, right. So, that is really what you try to see. If there is no resource conflict, then you add this node to what is called the schedule, right. And then you mark the resources that are being used for this schedule. That is essentially what this add function is doing. You add the node to the schedule and then mark all the resources which are used by the schedule. And you keep doing this. If the node has a resource conflict, you skip that, go to the next node in the priority list until you exhaust all the nodes in the ready list. After you have exhausted all the nodes in the ready list, you know that no more nodes can be scheduled. Then you increment your time step. After you have incremented your time step, you try to find out whether any node v has now become ready. When would you know a node is ready? Know a node is ready if its predecessor has completed its execution. If all its predecessors have completed its execution. Therefore, whenever you schedule a node, you have to make sure that its successors will be intimated after a time which is equal to the execution time of this. So, if node v is scheduled at time step t, this is v and v has a successor u and let us say u has only v as the predecessor. If this is scheduled at time step t and this takes let us say two cycles to execute, then obviously this will become ready in t plus 2. If u has multiple predecessors, then it will be ready at t plus 2 or later depending on whichever ones that happen. So, you can say that one of its predecessors have become ready at t plus 2 and then you can decrement the predecessor count. So, that when the predecessor count becomes 0, you know that all the inputs are available and then at that point in time node u can be added to the ready list. So, you do that for all the successor nodes of v, not only the v, I mean for every v that is scheduled in the current cycle. So, that essentially increases your ready list, then again you prioritize your ready list and then keep doing this again and again until you schedule all the nodes. Now, let us see what this schedule is, how do you represent this schedule and how do you kind of keep track of your resources. So, let me try to do this example. So, let us consider the simple case that I have that I am trying to schedule for an architecture which has one integer unit, one floating point unit and one load store unit. So, this schedule is essentially a table, the number of columns is equal to the number of resources that you have and the number of rows is equal to the schedule length. So, if when I schedule let us say an operation v at time step t, then what do I do? If it is a load store operation, then I say that that particular operation is scheduled. If I am at time step t and I find one more load store operation, I go and look in this table, I find that there is already an entry that therefore, I cannot schedule any more operation, any more load store operation. So, similarly I see at time step t plus 1, what happens? Correct? May be a node u was scheduled in the floating point unit. If I have an integer operation, I can schedule this. If I have a load store operation, I can schedule this in time plus 1, but not a floating point up. So, you keep kind of updating this table and that tells you how your resources are being used. As far as the cycle based scheduler is concerned, it basically goes from t equal to 0 to some t equal to k in the increasing order, keep scheduling the nodes. Let us later on see what is called an operation based scheduler, which will actually do things in a slightly different way. That is the next slide. Any questions on this cycle based scheduler? No, right? Easy enough to understand? Okay. So, the operation based scheduler works in a different way. It does not have the notion of cycle time. Instead what it does is that you have the d d g and then you have the for all the nodes, there is a priority function just like the LST minus EST kind of a priority function. Correct? So, every node has an assigned priority. Now, what you do is that among all the nodes you find, this is what you try to do, right? You find, okay, so let us assume that for every node we have this notion of EST and LST, okay, to start with. Now, what I do is that ready node is the set of source nodes in the set of nodes. So, initially I have all the source nodes in the ready list and then I start off with an empty schedule, okay, and I continue to do this until I construct the schedule for all the nodes. Then what I do is that I do select the node with the highest priority from the ready queue, right? Okay. So, now given this operation, what I try to do is that I now try to schedule this not in the current time step, but from its earliest time step to its latest time step I try to schedule it or the maximum time I try to schedule this. In other words, what happens is that you remember the resource graph that we talked about, sorry, right, the resource graph that we talked about, the table. So, if you are scheduling for that, then what I do is that, so this table has all these rows and columns. So, I take an operation V, I find out when is the earliest that it can get scheduled, correct? So, let us say if it can get scheduled at time step 0, I try to schedule this in time step 0, correct? But let us say this operation V has an earliest start time of 0 and blah, blah, blah. I do not even worry about its latest start time for the time being, okay, right? Now what I try to do is that I take this operation, I try to see if I can schedule this in time step 0. Maybe this is a floating point operation and I see that there is already a floating point operation in time step 0. Then I go to time step 1 or I go to time step 2 or I go to time step 3 and then try to schedule this wherever it is possible to schedule this. Let us say finally I find a place to schedule, I schedule it. Then I go and take the next operation and then try to schedule. So, for each operation I try to find a time slot from its earliest time step to the maximum possible time step where it can be scheduled, okay? Now let us see how do we update the ready list here, right? So, we now start adding all the nodes V such that, okay, there is an edge from U to V, right? V is the node that we have just completed execution. So, let us look at it in the following way. U is already scheduled and the scheduled time of U plus the delay of U is greater than the cycle time. Well, this actually should not be cycle time, okay? Let us do one thing. I think there is something wrong here. We will probably stop here and then complete because this does not work this way. This essentially tries to take each node and try to schedule it between its earliest time and end wherever it is possible and then try to adjust things. So, I will come back and then correct this tomorrow and then maybe we can discuss it, okay? We will probably stop here because this is more or less what I wanted to cover for the day. After this we have an example, but that example is not very specific to the earliest, I mean to the operation base or other things. Then we will go to global scheduling. So, let us stop here.