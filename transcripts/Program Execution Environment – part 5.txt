 What did you try? Printing the L feather. What did you try? Did anyone try to do the assignment where you had to correlate the symbols with the data in this data section? Were you able to do that? But that is something which I would suggest you do that because what will happen is that is actually the crux of how to interpret elf structure and if you are able to do that you will have thorough understanding of how the information in elf is organized properly. So just to recap yesterday right what all we kind of did we kind of covered lot of things yesterday. So we started off with how function calls are implemented right. Do you still remember all the things stack, caller save, callee saved registers? What else we studied in function calls? Yeah. So essentially we kind of talked in detail about how function calls are implemented, how object files are organized, what is the structure of object file and then we covered some aspects of linking in terms of how linker merges the two things, why does it need to relocate some of the stuff because assembler may not have all the information during assembly time of addresses of various symbols and then things like addend and so on right. Now before we kind of start I will just give an example because I think that might be a simpler one to understand relocation. So let us say so we yesterday were trying to see there is something like minus 4 which kind of complicated the entire story of why things are that way in terms of so if you recollect so we had this relocation which had this R386 PC32 and if we did OBJ dump this the value here was some minus 4 which kind of complicated I was trying to explain it but I do not know how easier it was to convey but I will try to give a simpler example which will make things bit more clear. So let us say I have an array and let us say I have a pointer to it which holds address of 0th element of the array which is essentially same as saying it points to ARR right. Now let us say I do this and so before this program is going to run somehow PTR needs to hold address of ARR right. We saw this is achieved by doing the relocation. So we see that OBJ dump dash D dash dash section so reloc.hold holds something this is the content of the array right and this is the PTR sorry this is the PTR right and you see it had 0s so it does not actually hold anything. Now if we actually add this flag it says that this needs to be changed so that these 32 bits need to be changed with address of ARR. So what this relocation is telling linker is change 32 bits here with the value of ARR does that make sense and what linker will do is once it is done assigning address to ARR whatever is that address linker will simply substitute it here. Now let us say instead of ARR I do ARR of 1. So what I am saying is PTR actually points to ARR of 1 that means it is actually pointing to ARR plus 4 in terms of byte address right. So how do I represent this what should be the symbol here because there is no symbol called ARR of 4 right. So that is where the addend come into picture so now if I compile this program and if I look at the OBJ dump sorry it is actually exactly same but this value is actually 4 it is not 0. So what linker is going to do is first use this so in nutshell linker is using the original content which was there and adding that to the symbol does that make sense. So linker will not simply replace these bits linker will use these bits plus the symbol and whatever is the computed value that is what it is going to put. So in context of code what that minus 4 was representing is add this value first before you patch the address of the symbol. So relocation is essentially function of couple of things one is address which needs to be modified in this case it is address 14 in the data section right. The exact content which needs to be modified that is given by this offset the relocation symbol is saying what it needs to be patched with the content of this is saying what should be added to this address before actually patching the value and this R38632 etcetera is the relocation operation and it could be slightly more complicated than this. This one is fairly simple which is saying simply take 32 bits of the content and put it here but there could be much more complicated relocations. Does that make sense? So that is where we kind of stopped yesterday and in between we also saw things around strong symbols weak symbols how they are treated by linker and all the quirks which come out of that behavior. So today we will look into static libraries. The static libraries the primary reason we need to support that is remember why we were writing source code in multiple files and why we had linker can anyone recall that point why we had that? So we had couple of reasons for separate compilation and one of the other reasons was being able to distribute your code right. Now let us say you have created some utilities so let us say you have build a collection of programs which do various data structures implementation right. So you have linked list you have binary tree you have graphs you have whatever other data structures like heap and other things. So let us say you have built a collection of such programs and you want to share it with your friend right. say linked list dot c tree dot c graph dot c heap dot c and so on and you will put compile all of them to their dot o files right. Now to be able to use that I need to have access to dot o right. So let us say I am the client user of your data structure library I need to get somehow the linked list dot o which you have created. Similarly I need to get tree dot o right. Now one way you can distribute your code to the users is you can give all of the dot o's to them saying that this is linked list dot o this is tree dot o this is x y dot o and so on right. What are the problems with that? What is happening is you have created a discrete collection of various things and user might have many more such things because user will need imagine if for printf you got one dot o for scanf you got one dot o for something else you got one dot o and so on right. So it is a mess to manage for you also to distribute something that way is a problem it does not form a logical collection of something right. Does that make sense? So that is where static libraries come into picture. So essentially static libraries is a collection of various object files. So instead of giving multiple dot o's separately you can put all of them together and share with the user. So user just needs to have data structure lib dot a or something like that and everything will be put inside that. So on linux typically these files are stored in a format called archive. And essentially what linker will do is linker just like it accepts the object file as the input. So what linker was doing so far is it was getting sample dot o x y z dot o and so on and link them together linker can also take a dot a file as the input and do whatever it was doing earlier with a dot o. So it is as if linker was given multiple dot o's together on command line. So how do you create a static library? So there is a utility called ar just like you had gcc there is a utility called ar to which you can give multiple object files and it will produce a dot a file. And this is the dot a file which contains all of the dot o's which you have given. And this is the command in which you can actually do it. So if you do ar rs whatever is your name of the library now the typical convention on linux is to have lib as the prefix then whatever name you want to give like lib data structure or lib graph or lib math or whatever you want to call it it is typically prefixed with lib. So lib whatever is your name dot a and then list of objects which you want to put that put into that archive. So once you give this it will create a library. Now one of the interesting aspect is just like you have separate compilation. So you do not want to recreate bigger archive again and again if you modify something. So let us say you had created an archive and you found some bug in say tree dot o you do not want to recreate it. So what archive utility allows it allows you to do incremental updates where you can replace only one of the objects. And how do you use the static library? Let us say you have created something. So let us say I had foo dot c and bar dot c I put them in the archive say lib my dot a and I have some client code which is going to call foo and bar. While compiling that I will do gcc whatever my program dot c. So this command remains same. Now you have to somehow tell the compiler on where are the other static libraries which are used by this program. So that is typically done by this dash capital L. Dot is indicating it is in the current directory. It could be anywhere but in this case what it is saying is all the static libraries used by this code are in the current directory and the directory which and the library against which I am trying to link is specified using small l and just the name. We do not specify dash lib my. That is why that convention exists because linker what compiler will do is it will prefix lib and suffix dot a automatically when you give this command and then it will produce a dot out and this program should actually work. So can you try creating this library I will keep this command as it is. So create some sample library where you put some function. No it does not need to be in same location. You can create it anywhere and then specify the path using dash capital L. Make it just a dumb tool. You can do anything with it. Yes, yes, yes. But you it is not make which is doing it. It is your make file which needs to specify. So whatever is your lib my.a as the target in make you need to say it depends on these many objects and then it will do whatever you want it to do. Make is not I mean make the way make works it is it simply has list of dependencies and that. So if you say my lib my.a depends on foo.o bar.o xyz.o as soon as any of the prerequisite changes it will do whatever command you have mentioned. And it did not get the use case. Now simply try one minor modification in the final command which you are using which is specify the dash L before myproc.c. So the only difference between this command and prior command is the dash L my was specified after myproc.c. Try to specify it before and see what happens. Sorry undefined reference. So I am still giving everything same to the linker but linker is saying now foo or bar is undefined. So this results in error. Any idea why? So essentially the reason this happens is because of an optimization of how linker works. It is not really optimization which linker is doing it is optimization which linker does for faster processing of the program. Now imagine what happens is if you give libc.a it has 1500 object files. We can see that. So let us say I do. So I have this. I could list how many objects it has using a command called art and if I simply pipe that to wc dash L I get 1503. So it is saying that libc has 1500 objects. Now if linker has to read all the 1500 objects and libc is fairly common. Libc is used by almost every program. So if linker has to process all 1500 objects then you can imagine it is going to slow things down because linker will say this object has these things so let me put that in memory and do whatever I need to do. So what linker does is linker only extracts the required objects. So linker does not want to pull in all the things which are not needed. Linker wants to take only things which are used. For example if the client code is only using link list and not trees why do you want to pull in the code for trees? It is unnecessary going to waste memory when the program is loaded at run time. What linkers algorithm is for resolving references is something like this. So whatever is the command line which you have given to the linker, linker will scan the .o file. What linker has to do if you think about it conceptually linker gets multiple files as the input. So what linker will do is start scanning them from left to right in the command line order and it keeps the information of whatever is undefined in that. So for example when I give main.c it will say foo is undefined, bar is undefined. Then when it encounters the next file it checks whether this file defines foo or bar. And then if so then it says that this file is useful for linking. Let me take this. Does that make sense? So linker will only use the object files which are trying to help define some things which were previously undefined. So that forms an ordering between the things. So in the example which we tried earlier what we did we told linker that first compile my .c and then take lib my .a. So linker said when it is compiling main.c it encountered that foo and bar are external. So I need someone to specify their definition and lib my .a specified their definition. So lib my .a was actually used and linked. In the earlier in the other case what happened is first linker scanned and lib my .a. Linker says lib my .a does not have anything unresolved because it had foo and bar defined and foo and bar are not the main functions which I care about. Then it went to my proc.c and my proc.c says foo and bar are external. But linker didn't remember the fact that prior objects defined it because if it has to remember then it has to read all the things which were specified. And then when my proc.c was compiled linker assumed that someone else after this will provide me the definition but nothing else was there on the command line after that and then it failed with unresolved reference. Does that make sense? So it's a lazy searching which is being done by linker. So it will search only on demand. So whenever it takes an object file it records all the undefined things and assumes that subsequent things will define it. So order of the arguments on the command line do matter for linker. In most cases it does not matter like in the typical Linux commands doing ls-l versus ls-l-a does not matter the order in which you specify the flags but for linker it does matter. Now what if there are circular dependencies? So you have say linked list.c and you have tree.c and tree.c calls something in linked list.c and linked list.c calls something in tree.c. How will you solve that problem? Okay but what if it's there? So the good software designing practices you should avoid putting circular dependencies between two libraries. So that's a design principle but what happens if it is there? See what is happening is linker is forcing you to specify a particular order. So if lip.c you put first then whatever is undefined in lip.c will get resolved by a library which is specified after that. But if subsequent library has some dependency on the prior thing it will not get resolved. So essentially the trick is linker allows you to specify same library multiple times. So it's not unnecessary that one library has to specify only once. So you can say gcc myproc.c –la –lb –la. So first when it encounters –la it will try to resolve whatever was undefined in myproc.c. from that then whatever was undefined in lib.b will get resolved by the subsequent –la which was specified. But having said that it's still a bad practice. So don't do such things. It's just that there is a workaround available if you have that for some specific reason where you cannot avoid circular dependence. Now some of the things which help you analyze archive files. So if you do aart of whatever library you have it lists all the objects which were present in that. So let's see what all is there in lip.c. So if I do this see I can see there is something called as vprintf.so and blah blah blah. So scanf. So you can now know that scanf code was written in a separate source file than printf source code. So aart command is listing all the objects which were part of the sample of the archive. You can list down all the symbols which were present in an archive using command called nm. Remember I told you there is a command called nm yesterday. We didn't use it but you can use it on this. So this is defining all the symbols. So for example you can say there will be a symbol for printf here or malloc for example. So all the symbols in the archive you can list using nm command. Now for linker to work whenever an archive is given linker doesn't want to read the entire ELF file. You remember yesterday to be able to read the ELF file you had to read the ELF header then you had to read get the section header table then you had to get to the symbol table. So it takes time to be able to read an ELF file. So every archive has something called as an index which you can display using nm-s. So index is simply saying which symbol is defined in which object file. So it says that init-first.o defines libc init-first. So now whenever linker is using archive it doesn't have to read all the ELFs it simply reads the archive index to figure out whatever is unresolved is it getting satisfied by init-first.o or not. But if so then it will read init-first.o and make it part of the linking process. Does that make sense? So these are some of the things which are present in the archive. I am not going to go into details of how archive itself stores all the ELFs as in what is the binary format of the archive. Yes. So the question is is libc a static library? So libc comes in both form static and dynamic. What I am showing here is the static version of it. But in most common cases what whenever you are compiling your programs etc by default the shared version of the libc is used. And we will get into details of what is a shared library those who don't know what it is. But you can specify something called as static. So there is a command line option called dash static to the GCC to link against libc statically. Any questions so far on things which we have seen? We will get into details of how dynamic libraries work. So what are the disadvantages of static libraries? Now one of the things is you have to remember is after you are done linking a static library the executable which is produced the code from the library was actually part of your application. So to see that let us say so I have this mylib.a. So it defines some symbols. Let me actually create new one. Why is this so slow? Is someone running some cron jobs on these machines user 9, user 2, user 1? Do you know are these users here? It has slowed down the machine a lot anyways. So here I have created a program with simply. So here I have code which was essentially doing so unfortunately the machine is has become really slow. So there is some client code which calls function foo and then there is some sample.c which I have created a static library. Now if you look at the code which is present in a.out you will actually see the foo is also part of the same executable. That means what linker did when it created the after linking the created the executable all the things which were present in the static libraries which were needed were also pulled in as part of the executable. So the executable does not have any dependency post creation on the library. That is why it is called a static library because the dependencies results statically. Now one of the problems is for common libraries like libc every code will have I mean if you link statically against a library like libc every executable which you will produce will have code for printf copied into it. So it will occupy lot of space on the disk and everyone will have a copy of printf code which may or may not be the best alternative. Furthermore the biggest problem is let us say there is some bug fix which is done in a static library then every user needs to re link their application with the updated library. For example let us say you had distributed your data structure library to your friend and he started using it entire college started using it and then you realize that there is some bug in your tree code. So you have to redistribute your library. Now simply placing that library is not sufficient. Every user needs to rebuild their application to take the updated fix which you have done. Now imagine every time some bug is fixed in libc and every program in the universe needs to be recompiled then that is going to be a messy situation. That is why static libraries have a problem with respect to distribution and updates which the library author may want to give to the clients. And how to overcome both of these fundamental things so shared libraries will help us solve this. We will see them a bit later today. But is it clear? So we saw how static libraries are created. We saw how linker is kind of bit lazy in not opening up all the object files but taking only things which are defined and undefined and using that information to figure out which all object files to actually pull in from the archives. And that requires you to specify command line and specific order. So if you look at what is happening in the linker as such. So in nutshell linker is trying to do something like this. So linker is reading all the inputs which were given to it. It will keep recording all the metadata in terms of what are the sections defined in this, what all symbols exist, what all relocations exist and so on. Then it will do symbol resolution which we saw yesterday where it has to categorize every symbol as strong or weak and then replace all the weak symbols with a strong symbol. And it will duplicate, error on duplicate strong symbols when they are present. And then it will lay out all the sections where it will take all the text sections from various objects file, put them together to form a segment. It will take all the data sections from various objects file, put them as data segment. And then it will assign addresses to all of the segments which it has created. Then it will do the relocation and it will write the final executable. So this is what linker does in nutshell. And all of this is happening statically as part of your compilation. Any questions so far on linker or libraries which we have seen? Yes. Yes, so typically no one will link against Lipsy statically unless there are very specific reasons. There are specific cases where you might want to link because you might be doing something very custom with that and you don't want the client side Lipsy affect your implementation. See what happens is if you don't link against Lipsy statically and shared library is used then you are relying on shared library which is present at the client side. And if that version had some issues let's say hypothetically then it will affect your library also. Any other questions? So far whatever we have seen everything was happening before program was being executed. We never tried to run the program. So now let's see the other side of the things where program actually starts running and what all happens in that aspect. So you remember I had told you that ELF has two views. One was linking view and one was executable view. So linking view had section header table and all the things in the sections. Executable view is more about program header table and the segments. Section header table at this stage is really optional. So the executables which are produced don't need to have section header table. Although in most cases you will find they do have because it simplifies some of the things in the tools which are kind of trying to read the ELF. So if you look into a program header so it is similar to how section headers were there that from ELF header you have a offset to program header table and program header table will describe each of the segments. Now each of the segment essentially will and remember the segment was nothing but collection of sections. So we said that all the text sections are combined together to form a segment. Now each of the section if you look at each of the segment has a type. Type will indicate what type of segment it is. Then it has offset which indicates where that segment begins in the file. Then it has virtual address and physical address. So this is the address at which this segment will actually be loaded. So when linker says this segment begins at virtual address 100 then loader is supposed to take that content and put it at address 100 in the memory. In systems which have virtual addressing the physical address could be actually same as virtual address because physical address is not accessible in a system with virtual memory. Then there is something called as file size which is saying how much this segment occupies in file. Then there is something called as memory size which is essentially how much this segment will occupy in memory. Any idea why these two could be different? Why could there be a segment which takes less space on file but more space in memory or different space in memory? So in what cases size of section could be different from when the section is actually loaded in the memory? Sorry, but that is not present in the object file. So far we never saw how mallocs information was present in the object. So you remember uninitialized variables and they were called BSS better safe space. So those were essentially bunch of zeros and we simply said we need to simply hold the size of how many zeros we want. But when we need to load it in memory we can't simply say assume there are these many zeros here. We have to actually create those many zeros in the memory. That's why the memory size can actually be different than file size. And then there are flags which typically talk about the permissions. You remember we had permissions like code section can be executed but not return and so on. And then there is section alignment which is how to align this section and what addresses. Now one of the things you have to remember is not all sections which are present in the elf form a segment. Segments are needed for things which are actually needed for program execution. For example does symbol table need to be there in memory? Symbol table it just used by linker to resolve things. Symbol table cannot be does not need to be loaded into the memory for execution because it plays no role in the execution of the program because it was an abstraction which was created for programmers. So not all sections in the elf become a segment. Now let's look at this code and with read elf dash el you can actually dump out the program headers just like section headers. So this was the section header which was there for the program and this is how program header looks like. So you can see that program header I mean this actually is not part of the program header but this is something interesting. So it says there is some address called 8048310 which is the entry point. This is the actual address where the first instruction to be executed lives. Is this address of main? Let's try that. So let me create a program. So I already have this. Now let's look at. So if I look at main, main's address is something like this 804840D which is not same as this. But let's look at a function called underscore start. Underscore start has that address. So what this entry point is telling is once you are done loading this elf start executing program from this address. Now since you know how to read elf and how to interpret elf you can simply locate this field and modify it and what will happen is the elf program will start running from that point. So if I put here address of something which is something which I want to execute before start or before main I could actually overwrite this somehow and then that function will start executing. Then we said that every program header or every segment has a type. Now the most important type is something called as load. Load is the segment which needs to be loaded into the memory. So text segment for example needs to be loaded as it is from file into memory. Data segment which is present in the file needs to be loaded into the memory. So all the things will be of that category. PH header is essentially the header which is telling link loader on how to load these things. And then there is few other things which we will see later. But remember the load is the most important thing which is coming from your disk executable file and getting loaded into the memory. Then there is something called as section to segment mapping. So this is essentially telling which all sections in the elf form segment. So this is saying segment 02 is actually formed out of these sections which are present. You don't have to worry about what these are in terms of I mean what does inter has, what does not ABI has and so on. But essentially what it is saying is all these sections which are present in the elf file form segment number 02. 03 is formed by all these things and so on. So you can see that using read elf dash L. So there are bunch of segments which are formed by different sections within the elf. Then every segment has permission. So this has read and execute permission. That means this is code section. But you cannot write into it. So if you try to write into the code segment, it will not work. It will result in a secfault. Similarly this has read and write permission. But if you try to execute, it will actually secfault. Remember yesterday we had seen example where I had my var which was defined to be a variable and in the other file I said it's an external function. And I tried to execute it and it didn't work because the address was in the data segment. And when I actually tried to execute it, it secfaulted because the data segment didn't have execute permissions. If I somehow data segment had execute permission, then it would have happily continued and executed that problem. Does that make sense? And file size and memory size we already talked about that these can actually be different. In most cases file size will be smaller than memory size. Any questions on the program header so far? So compiler does not allow you to change them. There are no options for you to make it change. But since you know how to read elf file. So for example you now know the structure of the elf file. So you are free to go and binary edit the file and overwrite it. For example what you can simply do is you can write a program which will read the elf file, examine all the things, overwrite some of the things and write back a binary file with different permissions or different these things. And it will actually do what you want. So that's the power of knowing the details because you can do something interesting which was otherwise in default case not possible. So for example you can change the entry point address and see what happens. It may not work. So whatever I am saying there are equal counterparts in the security side which try to prevent those things from happening. So it's not that just like things on the hacking side have evolved, same things have evolved on the security side. So they have equal counter measures to be able to do that. And we will see some of them as we get later. But essentially the idea is if you have a raw system which does not have all these things then you can actually see these things happening and you can try them for good reasons. Then abstraction of process. So we just saw that there is executable which has various information for loader on how to do things. We will see loader a bit later. We will kind of first start with abstraction of process and then get into details of other things. So can someone tell me what is a process? Must have learned in operating systems course. So that is the standard definition. So process is instance of the running program. And each program whatever you have runs in context of some process. And context essentially will be all the dynamic state associated with the process. So all the data associated with the process and so on will form the context. And why we have process as abstraction is because it kind of gives you two abstractions. Two key things are enabled by having a model of process. One is it gives you a logical control flow. What that means is it creates an illusion that every program which is running on the has exclusive use of the CPU. And this is provided by operating systems kernel by doing context switching. But it gives you an illusion that when your program is running only your program is running. But we knew when editor got slow editor was not the only thing running and someone had a cron job running which was sucking all the CPU. And the other thing processes as abstraction provide is the private address space. So just like processors as a resource process abstraction also gives an impression that the entire memory is available to only my program. And this is actually provided by virtual memory. Now this is how things are. So if you look at illusion every process thinks that I have exclusive use of CPU and memory. And CPU has a register so I have exclusive use of them. What is really happening though is every process gets CPU and memory only for chunk of time. So whenever the time for context switch comes the operating system will have to save all the state of the current program into the memory and then switch to the other process and load back its context. So context switching is nothing but load all the state of the earlier process which was stopped and before you do that you have to save the state of the current process so that when you resume it later it will start executing from the same point. So logically this is what is happening where process A runs for some time then process B comes then process C comes then process A comes and then process C comes. But as a programmer or as a user of a system what you will see is process A is running continuously, process B is running continuously, process C is running continuously without any interruptions. Any questions so far? I am kind of not going into all the details of operating system theories on how processes are and so on. So how do you create process? Now this again is an implementation dependent thing. One of the common models is something called as fork EXEC model which is what is used on Linux. Windows has completely different ways to handle processes. So what forks API does is it creates a copy of the current process. So whenever I fork something I am creating an identical copy of that. The copy which is created has its own address space. And what will happen is fork will return 0 to the child process and return child's process ID to the parent process. So once you call fork you are calling it once but the function is actually returning twice logically. Because once it is returning to the parent process and once it is returning to the child process. So if you look at this code, once I do fork the PID there are two copies of code now running simultaneously. PID will be 0 when the child process is running and PID will be actual ID of the child when the parent process is running. So if I check if PID is 0 and print this then child process will print hello from child and the parent process will print hello from parent. Does that make sense? And there is not much use of executing same program twice. So although we forked created a copy of it I really wanted to do something different. I don't want child to keep doing the same thing as parent was doing. So there is a different function called exec which will actually change the code in the child process. So anytime you have to create a process you have to go through this model where the parent process has to fork a child and exec will actually replace the child code. So logically this is what is happening. So parent is running, parent has its own stack, its own data and its own code. So let's assume parent was the bash which was running. I do a fork. So whenever you execute command ls for example on the shell this is what is happening behind the scenes. The terminal is going to bash is actually going to call a fork function. So parent will continue doing execution of the bash. A child will be created which is also exactly same but child will then execute exec function which will replace the entire content and child is now executing ls. Is this clear to everyone? So everything starts from fork and then you keep replacing content of the process using exec and that's how you run a new program or create a new process. Any questions so far? So quiz what will be output of this code? So one answer is two will be printed by parent process and four and three will be printed by child process. The sequence is not known but you know parent will print this and child will print this. One other thing is sequence in which parent and child will run is kind of non deterministic. Does everyone get that? So remember what happened is when child started executing it had its own copy of stack. So the x variable which is actually allocated on stack cannot be modified. The parent stack was not modified by the child. So when child did plus plus x it modified its own stack not the stack of the parent process. That's why this became four and it printed four and the child then decremented it and printed three. If you look at the parent, parent was not at all affected by this code and simply came here and printed two. Now let's look at virtual memory. So process as I told you also provides illusion of actually saying that every program has access to the entire memory. Now if you have a machine with 32 bit address or 64 bit address you can at max get 2 raise to n bytes of addresses. So if you have 32 bit machine you get up to 4 GB memory and if you have 64 bit machine you get up to 2 raise to 64 bytes memory. So you have this is your memory from 0 0 0 to whatever f f f f f depending on width of the address. Now one thing is you want to have private address for every process because you don't want accidentally to modify these things. So just like we saw in the prior example you don't want child to accidentally write stack of parent. So you want to ensure that you create separation between the processes in terms of memory access. So you don't want accidentally one process to modify something of the other process because it will cause other process to malfunction. Does everyone get that? Now do we have 2 raise to n bytes of physical memory? Maybe you have 4 GB RAM but 2 raise to 64 is just too much to ask for. So you don't have that much. Now you certainly don't have that much RAM for every process. So you cannot say my P1 has 4 GB RAM, my P2 has 4 GB RAM, my P3 has 4 GB RAM and so on. So you will have some memory but you certainly don't have sufficient to get all the addresses. Furthermore you don't have really sufficient memory to have completely separate memory for each of the process. So address space is essentially the layout of how the memory is organized for each process. So every process which we saw will have this kind of a memory map. Now this again is implementation specific. So this is again for x86 Linux. x86 64 Linux will have something different, Windows will have something different and so on. So all these things are kind of implementation specific. But if you look at the Linux, what Linux says its code segment always starts at this address. Remember this address we saw somewhere in the ELF. So this is where the code segment starts. Then you have all the data segment which has initialized variables and then you have BSS segment which is all the uninitialized variables. And these are all loaded from the executable. So the executable file which you had, had the information for these segments. So these are directly copied from the executable and loaded at these addresses. So wherever code segment ends the data segment starts, wherever data segment ends BSS segment starts. Then you have heap. Does everyone know what is heap? So typically when you do dynamic memory allocation using malloc or new kind of functions, you are allocating memory from heap. You are not allocating them from stack, you are not allocating it from data segment. So that all memory comes from this region called heap. And then we have something called as memory mapped region for shared library which we will see later when we actually see the shared libraries. And then you have stack which is actually growing down. And then you have the upper 1 GB space in Linux 32 is reserved by the kernel. So kernel actually has upper 1 GB addresses reserved for its own use. Now every process which is getting created, although will have different content in memory, every process has different data, every process has different code, every process has different stack. The layout of how memory is organized itself is going to be same. So every process will have its code at this, every process will have its stack just below the kernel address space and so on. Does that make sense? Yes. Yes, I mean it is a specification by ABI, but linker puts all these addresses. Remember in the program header we had virtual address and physical address fields. So those fields will be populated by linker using this layout. So that's why data segment does not have a fixed stack, code segment has a fixed stack. And depending on how much size code segment takes, the data segment will be after that and the BSS segment will be after that. So linker is responsible for only laying out these three things because information of stack is not present in the executable, information of heap is not present in the executable. No, assembler does not. This is where the linker, the fact that linker has to take all the things together and merge them together is this reason. Imagine something like this. So you had a linker which didn't concatenate all the text sections. Then what will happen is you will create holes here. And that becomes inefficient because what happens is there are some APIs which allow you to load these segments very quickly from the executable using a technique called as memory mapped files. I don't know how many of you are aware. So there is an API called as Mmap which essentially allows you to read something from disk and map it into the memory. So what linker does is linker will arrange segments in such a way that they can be Mmapped in one shot. Linker doesn't want loader to do Mmap multiple times because if it creates holes then loader will have to take portion from here, other portion from here, other portion from here. So this layout of the lower part is actually decided at link time not at the assembly time. Now kernel space is obviously part of the process but it does not mean that the process can actually access it. So process, if you start accessing this address in your program on Linux 32 you will actually get a secfork because you don't have permission to access these. If you have studied the processor architectures especially 80386 and later there is something called as privilege for every code. So this is running at ring 0 or ring 1 and user code is running at outer rings which have lower privileges. So you cannot go from outer privilege to the inner privilege without having some interface in between. So what happens is the program can access this space indirectly. You have to do a system call that's where kernel space will come into picture and do something for you. So you cannot access this directly. Kernel exposes some APIs which are further abstracted by libc or something. So for example whenever you have to open a file in your program what will happen is libc at some point will call an API from kernel which will actually do the file handling thing using the file system portion in the kernel. Whenever kernel has to do some context switching the kernel space will come into picture do the context switching and start the other process. Does that make sense? Now there is something like this which you are seeing. So I have said that beyond kernel space there is a random stack offset and just before heap there is random brk offset. Historically this was not true. Historically even stack was beginning just immediately after this address and heap was immediately beginning here. Now that created a problem for lot of things especially security because what happened is you know deterministically every time the program is run the stack is at this location, heap is at this location and so on and that was universally true for all the programs which were running on linux 32 bit. And that created lot of security problems because you can very easily write code which can assume those addresses and do something interesting. So they added something which is randomized. So they add some offset here every time you run a program stack is not at the same position. Stack will start at some different offsets and similarly heap will start at some different offsets. This is called address space layout randomization or ASLR. This is not that effective in 32 bit linux because what happens is there is not much address space to do much with it because you can't have large random offsets because your address space is already constrained. But on 64 bit variants where there is very large address space this actually plays much interesting role. Most of the systems which you will have will have ASLR enabled. I think kernel 2.6 or something started it. So most of the recent distributions should have ASLR enabled. Now what are the problems with respect to the memory management which we need to solve. We said that this is the address map which we want to create for every process. But we still have to we still know that we don't have sufficient memory and we cannot we need to ensure that there could be multiple processes active at the same time. So how do we fit everything into the actual physical memory we have. We don't have that much memory but we want to have multiple things running. And where are the different address spaces allocated. So I said that code segment will be here. Now everyone cannot get that address in the physical memory. The 8c0 address all the programs cannot start at that address because physically there is only one that address. So how do I allocate different portions of the address space in the physical memory. Then how do we protect data from each other. You don't want accidentally process p1 to write into p2 data and so on. And can we actually share the data. Let's say we don't want accidentally to share modify the data. But if two processes do want to share the data how can you do that. So these are some of the things which we will actually see. So virtual memory is essentially an indirection. Has anyone heard of fundamental theorem in software engineering. So it's a theorem which says that every problem in computer science can be solved by additional level of indirection. And we'll see some examples of that. So virtual memory most of these problems which we have are all solved by just creating one level of indirection. So virtual memory creates something like this. So every process does not access the memory directly. There is an indirection in between of mapping. So every process actually has some mapping which will get it to the physical memory. And this is what and every process what it views is actually virtual memory. Now address spaces so every process gets its own address space that we already saw. And the address space will actually be 0 to 2 raise to n. So on Linux 32 it will be 0 to 2 raise to 32. On Linux 64 it will be 0 to 2 raise to 64. The physical address can actually be smaller than that which could be m depending on how much RAM you actually have 2 GB 4 GB and so on. Now every byte in the physical memory actually has only one address because physical memory is fixed. There is no illusion you can create. So every physical memory has just one address. But every location in the physical memory can have one or more virtual addresses. Does that make sense? Because physical memory is there I might refer to the same location using two different virtual addresses. So this is how it works. You have process 1's virtual address space, process 2's virtual address space. Some of the addresses in the process 1 map to some location in physical memory. And some of the locations can actually be even on the disk which is called as swap space. Have you heard of this term swap space earlier? Whenever you were say installing Linux or something it asks you how much swap space to keep. And the typical guideline was keep it twice of the RAM size. The swap space is actually used to do some lazy thing which we will see a bit later on why we have disk also. But essentially you can think of it this way that your physical memory is actually extended by having disk also saving some stuff. Now how does actually things work? So CPU actually never looks at physical address. So all the addresses which we saw so far, all the pointers which are pointing to everything is pointing to virtual memory. There is no way for you as a programmer to get access to physical address of something. So whenever I say ptr is equal to and b it is not physical address of b which is stored in the pointer. So it is virtual address of b which is stored in the pointer. And what happens is as CPU is executing its various instruction internally it will send the virtual address which it is getting to a unit called as mmu which has a responsibility of translating that to physical address and then the physical memory will actually be accessed. And there is no bypass which is available for you from CPU directly to physical address. Does that make sense? Now why we have virtual memory? So we have created this thing what does it buy us? So it allows us to efficiently use the main memory and it does it by couple of ways. One is every process now has the virtual address space which is private to its own and there is this mapping function which is mapping it to physical memory. Now this mapping function can keep changing. For example first time when I look up address 0 it might be mapped to physical address 500. Later it might actually be mapped to physical address 10000. And this happens behind the scenes because what memory management unit and operating system together are they doing is they are constantly swapping in what goes what actually resides in physical memory at every time. So it is not that when your program starts running everything needed for the program is present in the physical memory. It will be brought into the physical memory only on demand. So whenever you access something it is brought on demand. Now what happens is because of this not everything of a program is always in memory and that is where the swap space comes into picture. So whenever you have something of a program which is not being used you will put that into the swap space and that is why you free up the physical memory. And the only time you will run out of the physical memory is when your swap space is full and your main memory is also full. Does that make sense? You will run out of physical memory only when your swap space is full and your RAM is full. As long as your RAM is full but you still have space in swap your programs will keep running. They will run very slow because processor and MMU has to constantly swap in and swap out but it will still functionally run. The other thing which this does virtual memory is it simplifies lot of job for programmer linkers compilers. Imagine if you are living in a world where there was no virtual memory then what would happen is programmer had to write programs where it cannot assume the entire address space is available. Linker cannot do the address space layout. So most of the things will get delayed to the loader and loader will become bottleneck. As of now loader actually is fairly straightforward because loader simply has to take whatever linker has done at link time and simply load it into the memory and that too on demand it does not do it by default always. So that simplifies the overall time you need to spend at load time which is more critical because load time is when your application is actually running. You do not want slow startup for your application and then it provides isolation of the address spaces because everything in virtual memory has permissions associated with it and as soon as someone tries to dereference it you get a problem because imagine everyone is trying to get the virtual address. No one has access to physical address. So how will you access physical memory of another process? There is no way for you to know where is the data of some process stored physically in the RAM and even if you knew it is at this particular address there is no way for you to write to that address because you can only generate virtual addresses in your program. So every program can only generate virtual address and virtual address may map to different physical address in different processes. So there is no way for you to actually access physical memory of other process. So this mapping will get to that. So how does the actual translation happens? So this is what happens is the CPU generated address is always a virtual address. So what CPUs typically do is they split this address into two parts. There is a page number and then there is a page offset. So you can think of it this way that your entire virtual address space is divided into a series of pages. Typically the page size is 4 KB or 4 MB. So you can imagine the virtual address space is split into set of pages. The page offset gives offset within the page and the page number gives the page number. So every address is translated to that. So you need some function which will translate the virtual address into physical address And this is done using a page table. So every process has a page table which is set up by operating system. And page table holds mapping that this page maps to this physical memory location, this page maps to this physical memory location and so on. So to see in diagram, so let's say you have these many pages. What this says is this page is mapped to this address in the physical memory. This page table itself will be configured by the operating system. But once it is configured processor and the memory subsystem can fetch these addresses from the physical memory. So it's a task which is done by both operating systems and processor together and that's why you learn typically virtual memory in operating system course as well as the processor course because both of them are playing some role with it. Setting up page table is job of the operating system. Actual using page table and reading the addresses and getting data from memory is done by the processor. Now some of the virtual addresses might not map to anything. So those are null. And if you try to access these addresses, you will get a sec fault. So the reason you get sec fault is for two things. You are trying to access a virtual address which is not allocated to any physical address. That's when you get a sec fault. You are trying to access a page which has permissions marked which are not for your case. For example, if you try to access a page which lives in the kernel space, then the permissions of that will be set up such that user program cannot access. Now let's go back to the quiz which we were discussing yesterday. What we said is if you return address of a variable on the stack to the caller function and the caller function tries to access it, it will still be able to print it although it's logically incorrect thing to do. Now why you didn't get a sec fault in that case? You are trying to access memory which you are not supposed to access. And the reason was even when the stack pointer was moved, it's not that the page for that was marked as null. To give an example, let's do this. So let's say I have some code. So let's say I declare an array of 4 kb. And let me access arr of 0, arr of 1, arr of 4095. It worked. Now let's try accessing something. This is outside the array. I still didn't get any sec fault. Let's try to access this. I still didn't get any sec fault. I got sec fault. The reason most likely is this offset falls into a next page which was not allocated. So whenever you do access memory which is outside some logical object, you are not guaranteed to get a sec fault. In fact, sec fault is a good thing because then you can realize there is some bug in your program. This was much more bad because what happens is let's say I was reading this somehow, then it gave me something. And this is much more harder bug to find in your program than when you get a sec fault because when you get a sec fault, you know that this is the point where it got sec fault. So let me check if I'm trying to access something illegal. In this case, there was no easy way for me to identify that I was trying to access something which was illegal for me to access. So these kind of bugs are very hard. Now there is one tool which I don't know if it is installed on this. If you run your program under a command called, so Valgrind is not running. So even Valgrind couldn't catch this. So there is a tool called Valgrind which essentially can detect some sort of logical bugs in your program, especially with respect to memory. So if you are trying to access memory which was not allocated or if you are trying to look at heap and dereference a dangling pointer and those kind of things, those are all detected by Valgrind. But even Valgrind is of no help here. So these kind of bugs are going to be nightmare, especially if they happen in a large software. So always be, don't try to write such codes. And then some of the addresses can actually be mapped into the disk. Now what happens is you must have heard of something called as a page fault. How many of you have heard this term, page fault? Now what happens is if your address is actually in the physical memory, then processor will happily go and execute and get data from that address. But if the address is present in the swap area, if address is mapped onto the disk, then you need to, the event is called as page fault. And that's where again operating systems come into picture and operating system will read that data from the disk and bring it back into the physical memory. Does that make sense? So all of this essentially allowed you to efficiently utilize memory because remember we were keeping things in physical RAM, only things which were needed. We didn't keep everything. And whenever we tried to access something which was not in RAM, but in the disk, operating system came into picture, brought something from the disk into the memory. And then it may do something like a page replacement if it doesn't have, sorry, if it doesn't have space in the RAM. So only when you run out of physical RAM and the disk is when you have a problem because then there is no way operating system can put in some data. And if you try to access something which is not mapped at all, not mapped to RAM or not mapped to physical disk, then you get a set fault. In all other cases, program will silently do something. And how many page tables you need? Correct. So you need one page table per process. So every page table has a process, every process has a page table which essentially tells how to map virtual address space of this process to the physical memory. Does that make sense? Now obviously the address translation is a costly activity. And this is what happens in that. So page table, how does processor know where is page table for process P1? Or how does it know where is process for page process P2? So there is a special register. In x86 there is a register called CR3 which actually holds the page table address. But essentially this register actually holds the address of the page table of the current process. And remember page table itself is also stored in memory. So page table is not something which has magical storage on the CPU. Page table is also stored in RAM. So this will actually hold the address of the page table. And page table will have whether it is a valid page or not. And then it will have a table which will essentially say that this virtual address maps to this physical address. Does that make sense? Now the translation itself is costly because what will happen is every time you are trying to access any memory location you have to first do the address translation. That means you have to first read something from memory because you need to at least read the page table itself. So it is going to be a costly activity. Now what happens is most of the times if there is a rule 80-20, most of the time is spent in the 80% of the code because you will have loops and everything. So most of the addresses generated could be together or related. So what you will have is you will create a cache of address translation. So whenever a process, whenever an address translation happens that virtual address 5 maps to physical address 100, you store that mapping into a cache which is called as TLB translation look-aside buffer. So what happens is every time you are going to translate you first look up whether this is present in the cache and if it is present you use that information directly. Otherwise you go via your normal page table translation mechanisms. Is it clear to everyone on how virtual memory works? So just to summarize virtual memory gave us a way to create illusion of entire address space being available. It allows us to efficiently utilize the physical resources because we keep only things which are needed in the physical RAM. And it allowed us to isolate data of two processes because no one can access physical memory. There is no way for a process to access other process data. And the address translation itself has a cache to make it fast. Now quiz, so let's say we started off with one of the requirements that if two process do want to share data, how will you do that? What can you do? So let's say there is some data which two processes want to share a physical copy. What will you do? How will you configure the virtual memory page tables so that both of them share the same data? So what he is saying is, so let's say virtual address 100 to 200 are reserved for that data. But does that really matter? Because virtual addresses will be translated to physical address. Correct. So what you will instead do is, see virtual address does not matter whether virtual address is same or different does not matter. What matters is the physical address needs to be same. So you can say process P1's address 100 maps to physical memory location 5 and process P2's location 10,000 maps to location 5. And this is how you can share say kernel data. See kernel, there is only one kernel. There are no multiple kernels. Now you don't want to create duplicate data of kernel for every process. So what you will do is every process page table will have that kernel's data lives at this physical memory location. And they could actually be at different virtual addresses. That does not matter. Virtual address is merely going to be translated to a physical address. Is that clear? Now when we say arrays are contiguous, are they contiguous in virtual memory or are they contiguous in physical memory or both of them? How many of you think they are contiguous in both physical and virtual memory? How many of you think they are contiguous only in physical memory? How many of you think they are contiguous only in virtual memory? Let's look at, so let's try to build this. So let's hypothetically assume they were supposed to be contiguous in physical memory. Then what happens is if you have a very large array, that means you need that much contiguous space in physical memory. So every time you need to bring that array, you need to have that much contiguous space in RAM. Now that fundamentally goes against the principle of efficiently utilizing physical memory. Because you don't want to create large contiguous chunks because then that forms a constraint. Because see whenever you have to look for free memory in the physical space, you have to find a contiguous chunk of that much. So that's design wise doesn't sound really promising that we don't want them to be contiguous in physical memory. Can they be non-contiguous in virtual memory? Can they be non-contiguous in virtual memory? Why not? Correct. So the entire indexing operation or pointer plus plus operation which you do relies on the fact that addresses are contiguous. So it has to be contiguous in virtual address space. So every time you have a big array, it's not that you are contiguously allocating that much physical memory, you are contiguously allocating that much virtual memory. And yes. Correct. So for example, if your array fits within one page like 4kb, then it will be contiguous. Now how will you do the mapping? You just need to say that address 0 to address 4k maps to this physical location. Address 4k plus 1 plus something maps to some other physical location. So the addresses in the physical space can be non-contiguous for an array. Now this is where the interesting part comes in. Does cache work on physical address or virtual address? So when we say there is L1 cache, you must have learned L1 cache, L2 cache and so on. So what is the principle of cache? That when I fetch something, I will fetch data contiguous to it because it might be accessed. So that's the spatial locality principle of the cache. So when I'm talking about spatial locality, am I talking about physical memory or am I talking about virtual memory? How many of you think virtual? How many of you think physical? Can you debate with each other and come up with a conclusion? So that might or might not actually be the legal thing. Now with that, your data graph is the base from you. Otherwise, it is not the physical memory address. So I think physically, your physical access is putting a physical address on the data and fetching data. So I think it is not just bringing that data but some more data along with it. So I think it is physical in that sense. OK. You have counterpoint to that. So if I have to hardware the cache, you can change what would be a data object. OK. Why? That's an interesting point. Can you say why you want TLB after cache? Any other points? Let's try to play both ways. So we'll derive what is the right answer. So let's say it works on physical memory. OK. Let's go with their solution. So what we are trying to say is whenever a memory access happens, first we will do the address translation. Right? Because for cache to work at physical address, first the address translation must happen. Right? So you must be able to translate virtual address to physical address. And then cache will look whether that physical addresses data is present in the cache or not. Right? And then if it is present, it will give you that. What are the downsides with that approach? Let's say you were doing the system design and he came up with this idea. What is the problem with this in terms of is there any correctness problem? Let's start with that. Is there any functional issue which can arise if the cache was supposed to work on physical address? OK. Then I mean same virtual address in different processes. Correct. Correctness. So everyone agrees there is no correctness issue with that scheme. Right? So if cache was supposed to work on physical address, there is no correctness issue. What are the performance issues with that? Correct. So address translation is the biggest problem. So what is the problem? So if cache was supposed to work on physical address, there is no correctness issue. Correct. So address translation is the biggest heavy weight because I wanted something to be looked up faster, but address translation is sitting in between and address translation itself may involve reading of the page table which goes to the memory and comes back and so on. So there is some performance issue. Now let's try to look at your side. So he's saying caches should be virtual address based. Right? What are the functional issues with that? Are there any functional issues with that? Correct. So this is a problem. OK. Two processes may have same virtual address which maps to two different locations in the cache. So address 500 in P1 might actually mean data X and address 500 in P2 might mean data Y. So if cache stores 500 holds data X, then that is incorrect in context of process P2. OK. So is this viable solution? Correct. So when you do a context switch, you can flush the cache and that will solve this problem. OK. So process context switch now has additional overhead of flushing the cache also. But if you do that, then there is no correctness issue. Are there any other issues? OK. What is the advantage compared to that scheme? So if you have a context switch, you have to fetch whatever was available. It might have been contiguous or it may not have been contiguous. Sorry? Correct. So address translation is saved. Right? So on this side, you have advantage because do we need to flush cache in case of context switch if you have a physical cache? Right? So you have advantage of no cache flush with their scheme and you have advantage of no address translation with your scheme. OK. So which one is better? It's not because see as soon as you flush the cache, it means all the references which are made now cannot be in cache and they have to go to wrap. So you have to again populate the cache with the cache entries. So it is equally bad. OK. Now what is actually done? OK. So we do both. OK. We do both. So have you heard of something called as on die cache, which is typically L1. Right? So L1 typically is virtually indexed. L2, L3, L4 are physically indexed. OK. The reason you do that is because L1 is on die. MMU is actually off the die. OK. So you don't want L1 to have to look up address translation and do that. OK. So you want L1 to work with virtual addresses. But beyond L2, which is off die, you are anyway given hope that you have to do memory access to some extent. So you better do the address translation. OK. Yes. TLB is in the MMU. So it is off die. OK. So you have to do address translation. Correct. Now processors are a bit more smart. OK. What they do is they start looking up L1 cache and they issue the address translation also to happen. So by the time L1 lookup happens, you would have address translation also done. And you may throw it away or you may use it. So processors try to do things parallelly when it comes to translation versus L1 lookup. Yes. So TLB is kind of fused within the address translation itself. So if you already have an address translation done, you use that. OK. Now some processors also go a bit further. OK. So what you have, the problem which we had with L1 is every time the context switch happens, you had to do cache flush. So you had to flush L1 cache. Some processors have something called as process identifier. OK. Which is essentially, so process identifier plus virtual address is used as the indexing scheme into the L1. So what will happen is everything you will, it is not a function of just virtual address. It is also a function of process identifier. OK. So you have to do cache flush. OK. So that is the process identifier.