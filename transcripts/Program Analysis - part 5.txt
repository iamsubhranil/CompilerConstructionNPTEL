 So, let us do a few more definitions. Just to revise I mean we construct these gen and kill sets. So, all definitions that are generated and do not get killed before reaching the end of the basic block are put in the gen set you know how to construct it and kill is all definition that killed whose area was resigned in the basic block as simple as that. Again as you understand that it is on that in the on this particular equation I mean if you say out of n is equals in of n minus kill of n union gen of n. So, given this equation these are the definitions, but if you change the equation you do the gen first and the equation like you have to do it differently. So, if you do this for instance then your kill should not contain the ones which get generated right. So, those are so nothing is set in stone you can define things the way you want to, but you have to make sure the other things are also consistent that is all. So, how to compute we have done that. So, there are many abstraction techniques about for arrays like for instance one abstraction is that you assume that each array is one variable right one large variable. So, any update so you forget what happens on an index you just say that if I have written a of 2 it is just writing to a right. So, essentially what will happen is even if it is writing to like a of 2 a of 3 will also get killed. So, you are your precision of an analysis will decrease, but you have to design analysis so that it remains sound even in under those constraints, but again there are other abstraction techniques of arrays which essentially make it better. So, you can so there is something called recency abstraction which essentially says that let me just keep the first few array indices separate I will treat them as separate variables and the rest of the array again I will merge into a one big aggregate variable. So, there are such techniques. So, in fact abstracting array is abstracting heap these are research topics by their own right and how to do it well and what applications they will work well at different things or different thing or so very good observation. So, at this point in time we will only restrict ourselves to scalar variables right, but there are modeling techniques which can reduce these accesses of these aggregate data structures also into an array or into scalars or something similar. So, you have to modify your transfer function similarly for these guys right. So, this we have finished right. So, essentially finally, we in summary the reaching definition analysis is a analysis which is done in the forward direction it is an any path analysis which meaning that reaching definitions arrive at any path I would put it in my set and it always tries to get you the meaningful solution is the smallest solution right where the size of the sets are the smallest right those are solutions which we are which we really want if you really desire. Now, let us come to available expressions. So, again the idea we have discussed it yesterday. So, I will not get into too much the idea is very similar you generate your gen and kill sets and your global equations look a bit different. So, you do an intersection over your outs instead of taking a union and the transfer function remains the same and the direction available expression is which direction it is also forward direction it is a any path or all path it is all path analysis and in this case do you want the largest solution or the smallest solution think about it. Now, again like I said whenever you are asked this question you should think about what applications you can put it to use. So, you can do this something called redundancy elimination right. So, what you can do is if there are. So, remember the example I talked about yesterday. So, if you have a equals x plus y here and you have another b equals x plus y and at some later point you say c equals x plus y then essentially there are two computations of x plus y along these two paths right. So, if I know that x plus y is surely available here then what I can do is I can simply do this optimization I can define an h equals x plus y b equals sorry h equals x plus y this I move to h this I move to h and this also I move to h this is an possible optimization. Now, the good part is I compute x plus y only once no matter which path I take instead of computing twice. So, for this particular case what will. So, now let us think about the safe solution which is the safe solution for a safe solution will should be smaller than the required thing or the larger than the required thing. So, it will be larger why larger why smaller. So, what can go wrong if with this analysis. So, what can go wrong. So, if I say that it is ok no. So, let us say n solution what what would like like it to be larger than the actual solution or smaller than the actual solution smaller than the actual solution why is that if it is larger than what happens yeah then we will think that x plus y was not available we might think it is available and we might just put a computation of something which is not even computed yet then it will put a garbage value in that location right. So, now the safe solution must be smaller so anything anything larger is unsafe that is what I am saying anything larger than the optimal size is unsafe what about the best solution the largest right the largest possible solution because it will enable me to do the most optimization larger than the largest would be unsafe right. So, the solutions can be from you can say that nothing is available that is a solution right I would I will not do the optimization anywhere that is ok that is safe right but universal set is not safe because then I would say anything everything is computed and I do not need to do any computation at all then everything is wrong you have not done any computation at all and you try to use use values which are not even computed. So, the so now we desire a solution which is the largest possible solution any solution beyond that the best solution is going to be bad is going to be unsafe that is not even a solution that is not even a correct solution the problem is I might assume here that say x plus y and x star y are available which was not the case right. So, then maybe there is a computation of d is equal to x star y then I will try to replace this x star y because I say that this is already available. So, I try to replace it by some h2 but then this one will be garbage because it was not even computed. So, let us say that it comes here maybe what happened was that there was a x star y here and I said h1 equals h star y. So, let us say the h was h1 was initially set to 0 initialized to 0 then it went here then it went here on this part it got computed to x star y on this part it never got computed right. But my available expression analysis is bad and it says that x plus y is available right. So, I see it is available in h1 good I substitute with h1 which is wrong because along this path there was no computation of that particular expression cannot be enough because what in the program executes along this path right then I then h is not even computed yes that is true but so okay. So, one way to think about it is yeah by definition that is not an available expression but I am saying see whenever we were computing things we saw that there may not be a unique solution there may be multiple solutions right. So, now I should have a way of saying just picking an arbitrary assignment to my sets and we should be able to say that is it even a solution which means that is it a sound solution or is it not a solution or it is a extra solution or a bad solution right. So, I should be able to say it is not a solution or not a sound solution I should be able to say that it is the right solution or it is a bad solution. So, I am trying to classify these three things right. So, a bad solution is a solution which is fine but it will inhibit your optimizations right. So, if you remove something from the available expression set that is fine why is that fine you will say that let us say x plus 1 was available but you said it is not available you have a smaller set then what will happen you will not do this optimization you will not do this replacement I lost opportunity but I did not harm anything now I mean my program is still run correctly but the dangerous part is if I extend the solution with something which was not even available if I extend it by x star y that is a dangerous case because then I would start doing replacements of things which are not even computed. So, whenever any sequence set of equations has multiple solutions that question is always there that what is which one is the right solution see the problem is that there is a engineering problem right at the beginning there is a engineering problem you translate that into a mathematical formulation to a mathematical form mathematical problem right. Now when you are analyzing this mathematical problem this mathematical problem can allow for multiple solutions right but all the solutions may not be the right solution for the engineering problem right so there is this modeling step where you model engineering problem into mathematical problem right. So, the multiple solution business I am talking about in this mathematical space which does not know what was the original problem I came from it does not know are you doing digital information are you doing our available expression the machinery is the same for all of them you give it to the solver fixed point solver it will compute something and give you the result back it does not know which one is the best one so you have to tell it that okay give me a solution which is the optimal solution is best in this time you tell me again why did we want that what was the optimization we were trying to do with that yes when can we replace it when it is a singleton set anything bigger than a singleton set we would not have been able to do it now here we are doing the other way around we are this is a all path solution all path problem right we say something is available if it is available along all paths right reaching definition was any path problem right we say something is reachable if it is reaches to any path right so here what we want to do is I do not want to see in this all path problem essentially we can say something is available if my if along all paths I get that expression if I miss any one path that is no more unavailable expression so had it been the case that I there is one path one path I miss that solution in that case that optimization may not be possible I may not be able to do that optimization because my set size is not the optimal set do we get this so this is confusing now your brain will get twisted because these four are like this that's why these are called these four classical problems and any data flow analysis you see they will always start talking about these four problems because like they like they try to really twist our brain around in a big way because these are four two of them in forward two of them back two of them are all two of them are any right so you get all the combinations so that is why it becomes so you have to keep on thinking which is the largest which is the smallest which case what happens are we good with this okay so let us go to the next one so here the I am sure you guys can write these equations now right again in this case what is my initialization for all basic blocks I start with the universal solution universal set why is that it is a solution in some cases not always no but your initialization is different right so initialization of your will it work for all cases at the beginning the initialize your initial set is empty no but still something will become available and that will seep in right so think about the case like you have x equals y plus z then something will get generated right so gents will be there but it can be yes yes so essentially one way to think about it is that you should not start from a point which is a potential solution start from point which is not a solution right remember anything larger it was not a solution was not a safe solution so it was not a solution allowed by the system equations right so if you start with that the system will do something and reach a solution and hopefully we are hoping that we reach the best solution which you do not know why should it reach the best solution but looks like think that it does so you guys are going to work it out right ok so next now quickly move to the next one but you guys should go back and think about it as slightly more deeply like what is going on ok live variable analysis so live variable analysis is the same business so this is the summary so you have in this case it is a backward direction problem and it is a any path problem so a variable is live if it is live along any path right so the analysis the transfer function and the meter you can understand and in this case we would like the smallest possible solution why is that again what is optimization we are doing with live variable analysis dead code elimination right so smaller is the set of live variables the more are the dead variables if more are the dead variables I can do more dead code elimination I can do the optimization more and what will be a bad solution yeah so if I have more dead variables then what was allowed or my live variable set is smaller right then I have a problem because I eliminate statements which have uses which are going to be used later those I will remove that is dangerous that will make the program go wrong agree cool so that is done so last is very busy expressions so very busy expressions let us now just go down to the definition ok so in this case again I am sure you can understand this you can figure out the reason for this it is a backward analysis and it is a any path or all path so it is an all path analysis this case we need the largest solution why what was the optimization we are doing with very busy expressions right so why do we need it to be largest more wasting I can do more code I can move to the earlier blocks right bigger is the set more is the more variables more expressions I can waste up what will be the bad case the bad solution right so we should not waste any expression which is not computed along all paths right so if my set is larger we are solved with respect to the correctness of the problem but not sound with respect to the like additional statements that you might add so there is a there is this ok so there is this what should I say requirement of an program optimization so whenever you do an optimization so generally the requirement is and that is what you prove whenever you have to design an optimization you have to prove that this optimization is one it is sound which means that for every possible input that this optimization can be run with the optimized program and the original program will match up on their outputs for every possible inputs their output should match up so this is what is referred to as semantic equivalence so that is one proof soundness proof you have to do the other proof generally you have to do is that it is beneficial which means that once I do this optimization there is at least there is so there is no path where I will increase computation so it is ok to do a null optimization which does not do anything that is ok but it is not allowed to do an optimization which increases computation along any path right even if you take computation from one path and put it in another that ok the time for this has reduced the time of this other thing has increased that is not allowed so either it should remain the same or decrease along all paths only then it is supposed to be a valid optimization so there is a separate class of optimizations called speculative optimizations where essentially sometimes you can use like data about frequency of paths that which paths are more frequent and you can do such weird things where you can increase the cost of one path but by decreasing something else maybe decrease the cost on the frequent paths but increase the cost on infrequent paths you can do such things in this class of optimizations but in traditional optimizations you are not allowed to do that so in that sense very busy expression is a problem because you are not allowed to increase computation so now essentially if you think about this whole gamut of optimizations we have sort of seen a wide spectrum of analysis and let us try to summarize how do these optimizations fit in the whole map right so first is direction the direction we have seen there are analysis which are forward analysis there are analysis which are backward analysis the thing is that whenever you have forward analysis there is a particular order okay let me just come here there are also analysis which are what are called bi-directional analysis these are very weird analysis their equations look very messy so there is how can you imagine what the these might look like so the equation might look like something like union over P which is like out of n is equal to union over predecessors of n of some n1 let us say and let us say union intersection over s elements successor of n in of some other set n2 so you are combining multiple analysis and so this becomes very weird and there are analysis like that which are what are called bi-directional they are some of them are coming from their predecessors some of them are coming from the successors what is bad with this I mean yeah I mean we had equations here we had equation we had equation here I mean at the end of the day this is a set of equation we are working with and would the same equation the same strategy I can still compute the same fixed point solution the same algorithm we can use yeah the same equipment some equation does not matter what they are what they are taking things from but what is not so good about these bi-directional analysis can you think of something you can get updated sample things here you can get the updated value of this updated value of this so I mean you are right but I just want to deal you more so what what is so what can I do in forward analysis or backward analysis that I cannot do in that in by that yes I can come up with a good ordering of these basic blocks in case of forward and backward analysis I can decide that either my predecessor should be computed before I do the particular node or the successor should be done but in bi-directional analysis what we do they are in both directions right so now we are stuck so that is why bi-directional analysis is becomes trickier to use the next is that it is about the property that we want so essentially we do we want the property to hold along any path or do you want it to hold along all paths that is one characterization of data flow problems and the next is the solution set do we want the smallest solution set or do we want the largest solution set right that is another characterization of data flow problems right and you can have a product of any of these possible combinations right we can have a bi-directional analysis which is any path and compute the largest or whatever you can come up with right so this is the roadmap so whenever you have a new problem to attack a new data flow problem to attack so these problems that we looked at were already done so we do not care about them so the question is now if you have a new analysis that you want to design how will you go about with designing that is important right now whenever you have a problem you should first try to answer them along these directions right and then you have to think about how to design the analysis so because there is so much commonality along for the different problems the question is that can I have a unified framework which can answer with where I can sort of abstract out the commonalities whatever are common I can separate out and I can plug in what are different I can just plug in those things and the whole framework will just work on its own right can I do that so that is the next question so can we detect common patterns right we have to figure out what is the initialization the so there is something about the initialization there is something about the fixed band computation and something about how many iterations it takes to civilize so essentially so these are the important things you have to keep in mind you have to keep in mind whether how do we initialize it the fixed band computation almost remains the same but what is what changes are these transfer functions and the meet operation so if you for the same framework that we discussed the same algorithm that we saw that would simply work for any dataflow problem if you plug in these three things the how we are going to initialize the problems the thus initial sets how what is going to be my transfer function and how do I join multiple solutions along successors of the decision to get the solution for the meter the in and out for respective sets okay so now let us look at what I promised that we will look at something called constant propagation so let us look at that analysis and I will ask you to design this analysis can you now think of coming up with this analysis so what is the constant propagation problem the constant propagation problem is that if I have some use of a variable let us say a use of y I would like to find out is it possible that y is a constant and if yes what is that constant value so can you come up with this I come up with this can you think how to do this analysis that too we saw but I want a more more powerful analysis so now you have to come you have to tell me these three things other things were done you have to tell me how to initialize it so you have to tell me what are the dataflow facts first so over what are we doing this computation then you have to tell me what is the initialization then you have to tell me the transfer functions and you have to give me the meet operations what are dataflow facts dataflow facts are whatever we are computing on like for instance in live variable analysis it was a set of all live variables for reaching definitions it was a set of all definitions or reaching definitions all definitions that reach in available expression and busy expressions these are the set of expressions right so these are the facts that that what are we interested in understanding no no reaching definition was one way of arriving at constant propagation but I want to now I want a more powerful constant propagation which is the analysis is designed to do constant propagation so now let's go to the basic question the basic question constant propagation asks is that can the value so the query would look like this so you will ask that for this value of y what is is it possible that this value is constant no matter what execution path I take and if yes then what is the constant that is the question I need to answer that is the problem I have so reaching definition is one way to answer it but what other other than that but can I like from grounds up can I build a constant so let's start with what are the set of data flow facts so what are what will you compute the value of so mathematically can tell me what is the set look like what would the set look like variable common values very good so the set of v1 value 1 v2 value 2 something like this and what can these values be what is this possible set of values but expression I cannot do anything with that no but I just want this answer that is it a constant or not I cannot say anything else like x is a constant or not that question is not important because I redefining x so had it been 2 or 3 I don't care but if y was 2 then I can do some nice computation here at compile time right so I only find out I will be more interested in wearing the constant values for users not so much for definitions so the question is that how would I so I would like to find out so this is my this is what my solution would look like at every basic block right so every basic block I'd like or every basic block I would like to find out that what value can take what variable can take what value and the question is that what can this be but that is not enough right I need to meet that value also right anything in the domain right so if it is a into variable that it can be anything right from like minus infinity to plus infinity or if you really think about like how to get integers fine right but mathematically let's keep it right so it is set of any constant can I give it a give it a constant in this range so it will be one constant value from there or let's say I define something called top and I'll tell you why I use top later hopefully which will which means that it is not a constant or let me just call it NC not constant right there is some value which may not be constant why cannot maybe not constant because let's say I have x equals 29 x equals 42 and this value is reaching here z equals x it is not a constant it can either if it comes to this part then it follow to if it comes to this part then it is 29 then I don't know so so I will either it will either be a value from this range or it will be just marked as not constant I don't know it's a constant it is not constant so what's the point which value do I put no I'm keeping it simple so let's say it's a vector of finite size I just keep all my variables and I for each variable I update it where with saying that it is not constant or if it is a value in that range so we are done with the dataflow facts the next things let's think about the initialization how do I initialize this set what where what do I know not constant is a trouble because if something is not constant then it can never become a constant right because it was already not constant so for initialization actually I need a different value which says that I don't know what this is right so I like when you get the full algorithm you will see why you need this separate value so I will say don't know dn right so I have so this initially it will be every variable marked to dn which says I don't know what this values yes yes yes yes NC value means I definitely know it is not going to be constant so now it's like a Poisson bit right so if something is not constant I do any operation with that that cannot be a constant anymore okay so now you have we have done with initialization now let's talk about the meet operator how will be so now I will have these solutions coming from two basic blocks and I want to find out what is going to be my solution here how do how will I merge so again it's is it a forward analysis or is it a backward analysis forward right I will keep on computing things as they go and if I want to meet how would I do the meet yeah yeah now you need the section each other so you need a more like a more sophisticated meet operator right so now you have to define a meet operator by giving a table right so that is the way to do it there is no other option so so you say this is the meet operation so the value can be actually a constant value say CI or it can be NC for one one path one of the predecessors and for the other predecessor it can be CK and NC then how will it happen how will I populate this table what is NC and NC this is NC right if both the direction I am getting NC then I this is going to be NC what is CK and NC NC there is nothing I can do one of them is not constant so no matter what happens I don't care what about this guy is again the same case what about CK and CI yes yes so here I will say that it is it is it is NC if CI is not equal to CK and it is CI if CI is equal to CK right so I wanted to take you out of the comfort zone things are not as simple as union intersection always your meet operator can be a slightly more sophisticated meet operation yeah yeah why not so this is the case right I just that does the next with this example in one one place it is assigned 29 the other place it is assigned 42 but you don't know runtime you do not know which path will be taken so I cannot surely say that this value is going to be 29 42 what about ha right sorry sorry I am sorry I should have made the don't know column also so tell me what should happen we don't know that was easy thing that okay now I can relax I've done one okay don't know don't know don't know don't know NC NC I don't care I can't do anything don't know an NC again NC okay then what I was on this table is symmetric so I did not actually feel both sides I can just fill the upper triangular so what about constant and don't know so it cannot be NC because don't know you don't know right what if it becomes CI you write don't know what is the value no but if you say don't know then it will always remain don't know because maybe that guy is eventually become CI so for the time being you can actually put it as CI if it gets later this don't know will turn into something eventually it will either turn to some other come other constant or some not constant so at the end of the analysis you will never have don't know anywhere right everything will get propagated right so then you will eventually be able to tell you that the final meet will happen and eventually some things can be NC things can remain NC right things can be not constant like this guy is not constant right this particular location this this particular location right you are getting from you are getting X equal to 42 and X equals 29 was somewhere right so then it will remain NC eventually so I am being optimistic let's put instead of putting don't know let's put CI which is the data here eventually that don't know will resolve something else and will update get updated anyway so I don't care so I do the same business right so I have a meet table now and you have to look up this table yeah it's constant but constant can move to NC no no no even at this location we when do when does analysis terminate when nothing changes across any basic block and in the while loop we revisit every basic block again and again when this location will get updated later but in that case it would not have been like the other edge would not have been in so if you're getting a don't know from one of the sites it means that basic block has not got traversed yet otherwise it cannot have remained don't know that's what I'm saying at the end of the analysis you'll never have don't know anywhere other than maybe unreachable code right everywhere else you will have certain value you'll either have a constant value or you'll have not constant you cannot it cannot remain don't know excellent question guys are asking really good questions so okay so what if we take input from the user what is that going to be what is that variable going to be don't know not constant not constant you immediately tell it not constant for parameters I mean if you're doing interprocedural analysis and those are the inputs for a function the parameters are the inputs if you consider the effect of other functions so right now we're doing this intra procedure analysis which is not looking at other functions what other functions are so again the other question is safety right so what is more safe putting a variable to don't know or sorry don't know is not even in the picture is it to putting a variable to a constant or putting it to a not constant which is safe not constant I'll run with optimization right but if I put it to like constant it's propagated and I'll reduce it and I'll replace it by wrong things that is dangerous I can't allow that to happen right so by same direction is putting something to not constant is always safe so putting the parameters to not constants is always safe I don't have a problem right so I'll not be able to do some optimization yes fine so if you are if you are ready to do an interprocedural analysis which looks at the effect of other procedures then maybe some of them will turn into constants great you can make use of it but till then you can keep it as constant if you don't want to do right okay so good so we have now the meat table now the other question is the transfer function more than two predecessors that was that was the case right why have predecessor one and predecessor two that was the meat is always between two predecessors the meat operation is always between two predecessors right or you are saying more than two so you take first take the meat of two and then apply that to the third apply that to the fourth take pairs like pair wise reduces right so where are we sorry I lost that the transfer function so transfer function how do we how do we model the transfer function so what is the transfer function given such a solution given you know this variable value binding at the beginning of the basic block you would like to find out the variable bind value binding at the end of the basic block so how do we do this why should you make it NC okay previous so what is so how do I write it you want to dictate me which one which one what for a variable assignment okay why NC why why if that gets calculated to the same constant if that expression that evaluate maybe it computes the same constant very good very good if it is a constant then yeah so we have to look at now this is very tricky because I will have to look at what is the operation happening right so essentially now let's say I have plus operator which is that I have expression like X equals a plus B right in that case I will have to decide that what is the value of what are the values of a possible what are the values of B possible if value of a is c1 this is c2 let's say c a cb then we have this can be not constant this can be don't know and it can be not constant and it can be don't know I have to write such a table for every single operation that I want to model because now you have yes so you have to do this is going to be c a plus cb you have to add them at runtime sorry compile time like analysis time you have to actually add 2 and 3 figure out that value and put it in the constant right what about NC business NC is going to be NC don't know is that troublesome thing so let's keep it don't know I don't even know what to do with it I mean let's not even bother so this will eventually get resolved anyway I mean I can be totally get a value and get result I don't that's not matter that's what I am saying so for the moment we just put a constant but eventually even that will get revaluated in here I mean you can change the I mean these are not set in stone it's just that I was trying to be like slightly more optimistic correct which is the again a little all the things will come and I'll eventually figure out but you can even that does not hurt because eventually things will get resolved only if all the branches are not don't know only then will that actual value be computed till then whatever it is there it will get overwritten by the next iteration so right so this becomes don't know NC k satas everything becomes NC I don't know why I'm filling this table but yeah but he get and this is the case right so you see that this part will be common to every operator no matter what the operator is this part remains constant I don't care right as long as one of the operators have any of these patterns I can pick a value from here only this particular part how would the constants have to be manipulated to get the new constant value changes depending on what operator means right awesome so that is my transfer function so now can you define precisely what my transfer function is so it's a big switch case it's a big switch case on the operator right okay if both the both are constants if the both the operands are constants then it's a large switch case depending on what case you land up with you will have to return that particular output that is how you will define your transfer function and how will define your meet operation the meet operation is also going to be a big if and else ladder depending on what values you get for these two things you'll have to compute and give the value whatever you know we are yeah like this is the meet operation is always a column predecessors so this is predecessor 1 predecessor 2 enough so so I get a constant value at the end of something only if I get a constant value for the same variable from the in of both the predecessors so the whole framework remains the same there is only thing see the how beautiful this framework is right my algorithm also doesn't change the algorithm that I put here except the place where I computed the the transfer function and the meat right other than that place everything remains the same all I need to do is I need to plug in these new guys these are the four so it's a very beautiful framework and all the correctness proofs I talked about all the termination things I talked about all of them work so again why should this algorithm terminate remember what was the termination argument there were two things right why why would it terminate what was the reason things would terminate bounded nature of the the data flow facts and and what was the other thing yes so about the nature of the function right these are the two things okay so now about the set of data flow facts is it finite or is it infinite finite yeah but every such mapping is a is a possible solution there my data flow the set of definitions was finite but now my set of values are not finite do you see this so I can assign a variable to any infinite any of the infinite values from minus infinity to infinity so does it break our correctness proof that is dangerous correct the termination proof no no we need the value otherwise how will it do the constant rotation I of course need that value so this is my solution this is what my solution looks like and it is an infinite set yes it is very much an infinite set because my variables can be mapped to any possible value from the infinite set so now just think about you have some idea no okay so what about my so what is the what about the nature of the computation can you think about the function f that we discussed for so long what does it what does that look like can you give some now these are not sets that you can say smaller larger you cannot say that unfortunately but still can you say something about what this thing is doing like for instance let's say at some point in time my for a given variable vi my current binding to the vi is don't know due to an update where can it go which all possible values can it get so it can become not constant or it can become some constant CI right if it is not constant then when can it go it cannot go anywhere it is not constant it has to sit in non-constant there is no way it can move what if it is a constant it can either go to not constant or it can remain at the same constant so though the total set is infinite but the sort of movement I can see of my data flow values that is sort of bounded right that is that is very interesting right because I can my don't know can at most become not I can act more the what is the largest path that can happen but don't know becomes constant and becomes not constant so three rather two transitions at most two possible transitions and within that something will terminate or not terminate I am seeing the worst case and the worst case it is this is the longest path right so the don't know can directly become not constant but that is the shortest path only one transition but I am saying the worst case the worst case it will be two transitions it can become a constant first time one edge becomes constant so I give it constant and then the second edge becomes some other constant and becomes not constant and after that it cannot move can it can it look at all your rules so you are you are at meet operation C1 and C2 right so so remember the algorithm where was the change the flag was checked on the ins right you were computing a new in which you are getting by taking a mute over the outs and then you were checking it with the old in old value of in right so let's look at the meet operator can it ever allow something else to happen so for instance if it is CI then there are two possibilities if CI and CK are not the same it becomes not constant and if CI and CK becomes same it becomes the same constant so there is no way that one constant can become another constant again remember the algorithm where was that flag being set let me just pull out that it's little far but maybe it's worth it oh shit this is the other direction don't don't look at this now forget forget forget yeah so this was algorithm right so this is not the whole algorithm but there is no space okay so so but whatever we can look at this now should be able to see the black from the red so essentially what are we going to do now my ins and outs would sort of change right I don't know my mouse works on yeah mouse works so okay so you have this ins and the outs will be initialized to what you will be initialized to the what is what what will they how will you rewrite this algorithm for that so ins and outs in me and out we will be rewritten to what all variables to do not know and the only thing will be different will be the input the the in in of B in of entry enough entry will be what so there are two possibilities either you put them to all constant so those both answers are right you can either put it to all constant as don't knows or you can put all constant as all variables to don't pose or you can put all variables to not constants both of them are correct solutions but you have to understand what is the implication of that so in the mathematical model we are doing this but what does it mean for the program putting all variables to don't knows means what no so think about un initialized values right so then what can happen is at the end of your analysis you can leave some values might be left off with don't knows right assume that there is some variable which has not even been used anywhere right so that variable will live in all the sets marked as don't knows so don't don't knows at what told that's okay but if something the analysis terminates and it remains don't know what it means is that it so it's an analysis which can give you garbage value also that's interesting thing right it's high-def of it right you simply set it to don't knows and at the desktop at the end of it if there are variables which are set to don't knows okay these are garbage values so you get it un initialized variables the other option is to set it to not constants right you can be treating garbage values are not constants can garbage value I don't know what value it is so I set it to not constants then your analysis will be clean in the sense that you will not be left with any don't knows at the end of the analysis both of them are right depends on what you want right no it will get assigned somewhere anyway now so somewhere X will become equal to 5 right so so the transfer function will make it 5 so okay so I really not tried it now so what is the transfer function for assignment X equals 5 how it will just make it that constant right we just do it for operators but similarly it had it been just X equals 5 it will simply set X to 5 yes the engine to the outset right so that's now for the rest of the analysis so in the beginning it will remain not constant but later it will become some value yes yes and that should surely happen because a variable state change changes right a variable becomes variables are meant to be updated whether because a variable can become one in some part and can become another constant two in some other part so when I said that it cannot become another constant it means at the same program point it cannot become another constant leader right please understand this difference this is a very important difference all my students get confused with this right so whenever I say that a variable cannot become if it's a constant two it cannot become a constant five later what it means is I am saying at the same program point it cannot become a constant five leaders but it should surely have two different values a constant two here a constant five here why not right I can set X equal to here and X equals 5 here after executing it the value will be 2 after executing the value will be 5 that is not a problem so the stabilization if you look at it the stabilization check is only here right it is only here where I'm checking the new in at the same location is the same as the what I got in the same location earlier the check is for the same location it is for the same program point not across program points I do not ever check across program points it does not make sense to check across program points right okay so okay good so now right so now what we check so we have run through the initialization after the initialization the flag true flag false all this business remains the same how will you change the transfer function after that is the update of the transfer function so how will you update the transfer function we'll have a big switch case no first is the meat so how will you change the meat now you have the table which tells you how to apply the new min create then the new in using the meat of the predecessors so we have you look up the table apply compute the table using a function as a sequence of ifs or a switch or whatever it is and then get this right then the final thing remains is to apply the transfer function so out B equals in B whatever so out here this will be a large switch case for every operation that you want to handle and again say your analysis you can decide okay I can only I want me want to do it with plus and minus with nothing else your choice right so you have a big switch case which will say for the given operators what are how do I transform my state my analysis right so this is the thing but why did we come here what did I want to show you what was the question so this all we can remember but there was one thing I wanted to mention which I am yes yes yes yes exactly right so check the case where the flag is being set look at line number 10 this is the place where the flag is being set it is set at the whenever the input differs right whenever the input differs I set the flag I don't care about the outs because input differs outputs output can possibly differ and I'll go to another iteration anyway right because our out is completely dependent on the input right completely dictated by the input of the given basic block right so my check is only on the on my input now let us go back to our so here we were so now the thing is that so because it is checked at the in the in can change only due to the meet operator right because of the operation of meet operator so now the meet operator check what can happen it can never move from one constant to the other because if it is not a constant other things will happen and if it is a constant so this is the case so if there are there the constants are different it will become not constant if the constants are the same it will become the same constant so there is no way this at the same program point it can move from 2 to 5 right so the longest path my updates can take is this path don't know to some constant to not constant right so it can at most have two updates if all variables so number of variables are bounded all variables can at most have two updates right so I must reach stabilizers so I should be able to stabilize it cannot become constant that's what so once I got a get a not constant any of them is not constant it can never become constant see the table see the meet table this is the meet table right again again that this is exactly what I am saying I only compare values at the same program point but down below is a different program point right I never it does not make sense to compare things across program points they are bound to be different actor at where at at so I am saying that where was flag set as true that is why I showed you that algorithm flag was set as true when the value of the in changed right at the beginning of the basic block how can the value of in change when you apply the meet operation right so so that was the argument so it means that my data flow fact the set of my data flow facts need not be finite even if it is not finite but if I can find out that there is the the number of updates to so there is a ceiling on where I reach finally and if I can take a finite number of steps to that particular final state at most if that thing is bounded number of steps I can take at most is bounded even then my algorithm will stabilize so constant propagation is an example of that does it make sense guys again please don't make this mistake I mean we never never never never never never never never never never never compare values across basic across program points right it does not make sense to talk about what happens here and what happens here this is less than this so what will happen this does not make sense right we only talk about has the value at the same program point change to something different that is all I care about right and every program point we can see the value can at most go through to updates if the total number of basic blocks are finite and if every location can at most go through to updates of every variable then I can this algorithm must stabilize so how many iterations at most two times the number of basic block or more constant variable right so it is times the number of variables right because it can happen that only one variable changes state then some other variable changes state and so on so I was asking that what on what the number of iterations would depend on so it can at most so every way so there are number of variables and every variable can go through an update and every variable can get updated at most twice right so every location can at most see number of variables times to number of updates and then there are basic blocks which can affect it so this is idea you start discovering constants and now this is the quiz question can you think of some difference other than the transfer function and all this business in how the analysis is structured can you think about difference in the interaction of the data flow facts between reaching definitions and constant like for instance again let us go to it so our the data flow set you can think of what I can think let me just put them in a similar manner for reaching definitions I can simply say that I have definitions d1 d2 dn and I can say that this reaches this does not reach this reaches this reaches this does not reach and so on for constant propagation I will have a similar thing for variables and I will say that this can well have a value 2 this can have a value 42 this is not constant this is don't know something like this right that is the difference now I'm sorry I did that is not what I wanted to say so sorry sorry sorry how do I wrap this yeah I can wrap this like this okay fine so now can you sort of see any difference between the nature of these updates of what happens in case of reaching definition and what happens in case of constant propagation so there is a big difference yeah agreed so that is sort of the any path all path sort of business but but that is let's say I'm saying factored in into the unified framework but I'm saying whatever whatever like this four analysis we had seen from there can you see how constant propagation is different it has it's slightly different read altogether there is one difference which is yeah but this set is finite if you think about this tuple is finite because this is a couple of all variables so this this tuple is finite so it just says key for so this is a tuple of all variables and it says that what value this variable has currently what this value this variable has okay maybe I'm not leading you the right direction but essentially what I wanted to say is think about the case here so does like let's say at certain point in time at a program point this is my yeah okay hold on to that thought I'll come to that later yeah but that can even happen here because through different paths the same definition right but okay there is something there I'll come to it but here one idea is there is that think about let's say my definition D1 was initially 0 and in the next iteration when I updated this particular basic block this became 1 can this change in D1 effect that some other definition D4 got in or did not get in can it no but that cannot ever have come there right even in the initial case it could not have arrived at that location so let's say I have a location at this location I say that let's say initially it was 1 1 0 1 sorry 1 1 0 0 and later it changed to 1 1 0 1 can this change to 1 effect something here at the same basic at the same program point not at somewhere else no maybe the I'm saying the future iteration so this happens in the future so so in the current iteration this is a transition that happened right and I say that later this particular update affected this other updates other affected something can it happen doesn't matter out is completely dependent on in so we argue about in or out it's a basically same thing I'm saying initially the set was 1 1 0 0 and later it moved to 1 1 0 1 so now what happened was this particular data flow fact initially we did not know it reaches but now I see that it got reached right can it affect the reachability of some other definition no right and that is why we were able to use this nice bit vectors because all of them I can just update in one shot I did not have to really worry about what can happen to the other but in case of constant propagation can that happen that because one variable I made it not constant some other variable can get affected because of that yes yes because it can go into a loop and then it can say okay now I see like simple case simple cases think of I plus plus initially you come here it will be say it I 0 and you will let's say this is happening in a loop right so it will say okay I can see a value of I as 0 and this guys don't know so fine then no this is affecting the same variable that is not saying that yeah but I had to say something like maybe I need a couple of statements I need I equals I plus plus so I plus plus become not constant and then X is equal to I right so now initially I was 1 and then X became 1 that was good but later because I became 2 like it became 1 here and then it became plus plus became 2 because I became 2 this guy becomes a not constant X has to become a not constant why is that because this becomes not constant right this will become not constant and X is equal to I this is not constant so the LHS also become not constant am I you guys have gone very quiet so I don't like quite no no no there are finite number of variables variables mean the same it is still I yeah so initially look at think of this particular set that the beginning of the basic block I is bounded to 0 because after this X I will become 1 right and then this loop will take it and again you take a merge with I is I equal 0 and that becomes NC two different constants one is 0 one is 1 it becomes not constant even before that even at the entry to the basic block entry to the basic block initially everything is don't know then initially the value X equals I seems in and then it becomes I plus plus that is taken out back into that branch so now on one branch it gets 0 the other branch it gets 1 and then I 1 is not equal 0 so it becomes not constant right and and see why did X become not constant why did X become not constant X did not do anything it did not change anything poor thing it became not constant because I became not constant this could not happened in case of a in case of reaching definitions right so this brings into a very interesting feature called separability so I we call analysis where one data flow fact cannot influence the other as separable data flow analysis frameworks reaching definitions available expression very busy expressions liveness analysis are all separable however constant propagation is an example of a non separable analysis right in fact there is a very interesting analysis called faint variable analysis faint variable analysis says that so live variable analysis we say that okay it is live if there is a use of a variable in the future if there is no use of a variable in the future it is dead faint variable analysis says the variable is faint if either it is dead that's okay or it is used in a used in an expression on a of a variable where the operands are also faint so if you have X equals Y plus Z if Y and Z are also faint then X becomes also becomes faint so it's used in a computation where others are also faint so it's like you are fainting they are not dead yet but like if too many people faint then they can already moved out so I can do a more like so there are cluster of statements who are not so useful but that will very well will not be able to use it because there is a cyclic so that becomes non separable why because something becoming faint depends on something else becoming faint right so X becoming faint depends on Y becoming faint similarly like X becoming not constant depends on Y becoming not constant like in this case happened right so X became not constant because because I became not constant right so I just finish it another so this is the idea of separability and this is the example that you talked about right multiple expression and you you pointed out right so multiple expressions can compute the same value right so what can what is happening along this program path in this program path I am getting a value which is X is equal to 2 Y is equal to 3 and Z is equal to 5 what along this program path again I am getting I am getting X equals 3 I am getting Y equals 2 and I am getting Z equals 5 so if I had got the constant values here I should have got what X is not constant not constant once it is 3 Y is not constant again 1 is 3 once is 2 but Z should be constant value 5 can you quickly do the constant propagation algorithm on this and see what happens Z becomes not constant why why did poor thing become not constant because what are we doing is we are computing the inset here and we are saying both this X and Y X is not constant Y is not constant so because they are not constant they are being used here so Z is also not constant so the value we compute is Z is also not constant and we miss out of an opportunity of optimization here is that could have been a constant 5 and I could have replaced this business here so this is one analysis where my optimal solution so remember initially we talked about this actual solution what we want to compute the solution we want to compute is to the program point I would try to take all paths that reach that program point and then compute their meets and that was what we defined as the MOP solution the meet of all path solution that is the best solution we should get right because I am actually looking at every execution possible and then seeing what are the states along every possible execution however whatever what we ended up computing using the framework the fixed point framework was what is called the MFP the MFP solution or the maximum fixed point solution maximum or minimum depending on whatever you like right so least fixed point or the maximum greatest fixed point whatever you like so it is the so we are computing the MFP solution in this case as it turns out the MOP solution and the MOP MFP solution do not coincide so if I take in the MOP solution I would have computed everything till this point and I would have seen that this value is 5 but if I do the MFP solution because I am first computing the ins and then propagating it I lose that information and I end up getting not constant for something which had which would have been more precise as 5 that is where the bad news ends so we essentially get a approximation so the problem is that is it a problem like did we lose soundness because of this no we could have done an optimization we did not do it fine not a good thing but we can live with it right but it does not become unsafe right so essentially it still gives you an over approximation so I define a new term called over approximation which means that I approximate but I approximate more so essentially I can inhibit optimizations but I will not lose out on soundness right so that is what is referred to as over approximation so we will end up doing an over approximation but the algorithm is not precise it is not precise it is not exactly computing what the MOP solution would have computed so our search for a unified framework continues and we will see that I thought I will be able to cover more but this was okay there is some bit of theoretical framework foundations which can actually say that why all this business works I mean so essentially whatever I said the number of steps required for updates are this and all you can actually put it in a very nice framework with a little bit of lattice theory and little bit of expand theory but maybe there is already too much of theory so we will see I see that I'll see the mood for the class tomorrow if we see that the class is in mood for that we will do that otherwise we will do something else of what so there is something called lattice theory so the data the idea is this the idea is there is these data flow facts is a so you know what is the lattice how many people know what is the lattice some not many people raised their hands yesterday so the so lattice is a set with an ordering relation right so you take a set and you define an ordering relation which says that the points in this set can be ordered by something is less than or greater than something right so this is referred to as any set with an ordering relation is referred to as a poset or a partial order partially ordered set right so it is a poset if for any two values in this domain so okay so there is also called something called a total order so I say something is a total order if every two values in this set can be ordered by that relation like for instance set of integers by the less than equal to operator is it a total order yes because I pick any two numbers give me any two national numbers I can tell you what which one is lesser than which one set of complex numbers under lexicographic ordering is it total order under lexicographic ordering you know lexicographic ordering that you first compare the first guy the real real parts if they're equal then compare the so that also forms a total ordering but the complex numbers under under complex ordering which is says that like I'll say that c1 is like i plus a plus ib is less than equal to x plus i i mean j so let's say this is these are the DLM complex parts so now I would say that this holds only if a is less than equal to x and b is b is less than equal to y if this happens only then I will say that this is less than this but think about these numbers i plus a 1 plus 2i and 2 plus 1i which one is lesser than which one can't say I this ordering does not give me a relation I cannot put any of them before the so if there are such elements then I call it a partially ordered set or a poset so there is an ordering but it's partially ordered like there are elements which do not have an ordering between them so now the cool idea is that these data flow facts can be put up as lattice elements by some ordering relation and this ordering is basically on this approximation basically so which I don't want to get into that so essentially now once you have that lattice you can actually talk about these these termination guarantees and computing the correct things on that lattice structure and the type of the function like for instance I said that even with infinite lattices sorry infinite sets I can have a terminating execution of my analysis like constant propagation why because in that case the lattice that you get has a finite height the height of the lattice is finite the number of elements can be infinite so anyway so let's see if we have taste for it tomorrow we can do more but as far as a practitioner is concerned we have sort of figured out how to do the analysis but if you do that you will actually have the mathematical foundation as to why this analysis is framework really works right so from a compiler's perspective maybe that is not so important but really understanding it it is really important to I mean you really understand insights that why does this is a value mathematically why does it really work and the good part is once you have put things in that framework the proofs are done in that framework and they're done with after that I would not have to do the proof for every analysis separately so it gives you a checklist that does this happen does this happen does this happen does this happen yes the analysis is out and it terminate done you do not have to do a proof for every analysis separately that is a that is what you get back from that.