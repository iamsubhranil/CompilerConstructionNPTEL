 Hi everyone, my name is Girish Bhaarambhe. I actually work in NVIDIA and I am responsible for the PTX compiler group within NVIDIA. So my work essentially involves working with the language design and some of the later aspects of the compilation like linker and loader related things. So this module is essentially going to talk about the things which happen after compilation. So far in first two weeks you have learned mostly what will happen during compilation right including optimizations and code generation. So this module kind of focuses on the other things which need to be there for program to work correctly. Now just to kind of give some examples of why this is interesting. Can everyone read the code? So can you tell me what will be the output of this code? What? Error. So how many of you think it is error? Why do you think it is an error? Correct. So the symbol gbl is being defined twice. So it should result in error. Now let us see this code in action. So I do gcc sample dot c. There is no error and it actually works. Any idea why? It is the simplest thing for compiler to realize that gbl is a symbol which is defined twice. Let us try something different. So I am defining a function called start in which I will do say printf hello. And let me call start. What will be output of this code? Hello. How many of you think hello? Some error, multiple definition of start. So whatever error we were expecting in the prior code we are getting in this code. Let us try something more interesting. So let us say I define an array whose size is say 3 gb. How much is 3 gb? 3 gb. Any idea what will be output of this? So someone is thinking it is related to how much memory compiler can allocate. Any other things? Let us try this. So it worked as expected. Let us try to increase size a bit. Still worked. Let us try to increase bit more. Still worked. Some reason this is still worked. So do we really have this much memory? Now something is, but this is size of array too large. So at some point it said I cannot allocate. But do we really have this much memory just prior to that point? So essentially this module is going to talk about these kind of things. Whatever happened behind the CS. So far like this was surprising. My compiler did not error out on a symbol which was defined one after other. What is the reason why it did not do that? Why was start a problem? I defined a function called underscore start and it was a problem. So outcome of this module is to teach you things which go under the hood. Whatever is not observable to you when you write program, but is actually happening within the system. Not just what happens, but why it is that way and how it is implemented. And that essentially will enable you to understand how the programs are actually being executed by the processor. Any idea why you should learn these topics? How many of you think it is interesting to learn about these topics? Almost everyone. So the reason you should learn these topics is because it makes you a better programmer. A better programmer is always cognizant about the fact that whatever things are happening in the system. Because that allows you to design your programs in much more intelligent way and to be able to exploit things in better way. For example, you have learned matrix multiply. And you probably saw an example where simply swapping the order of loop gave lot of performance. Why did that happen? Because you know in the system in which you are running your program there is a cache sitting there. And how to program things so that you efficiently utilize the cache. So knowing these things will help you become better at in general programming. It will make you really good at debugging. Because whenever you are faced with a nasty bug and you don't know why this is happening, like this in GBL, GBL thing, there is no sane way of thinking of why this happened. But if you know what goes on under the hood, then it is much easier for you to reason about those things. And then it enables you to build more systems. Whatever we are going to learn is going to talk about how a system is built and what all constitutes a system. Now when you are actually working in some environment, it's not that you will be able to utilize the ideas which you learn here as it is. But you will at least have foundation to think about where to start from. And new systems can be built using that. Now one disclaimer which I want to put here is most of the topics which are being discussed would be very specific to an implementation. It's not a theory. Unlike the first two weeks what you have learned, it was a theoretical foundation and it kind of hold across the various things. This topic is very much implementation specific. There are reasons why things are done this way for a particular implementation which may or may not hold in other systems. Most of the content here is specific to Linux and x86. Windows actually might behave very differently. So don't take this as a universal truth. But it is one of the implementations. And wherever possible I will try to point out that in other systems this differs. But most of the material is kind of Linux and Intel specific. Any questions before we kind of start? So this is the outline. Actually I have just too many things packed in these two days. So we will start with life of a program which essentially talks about how the program actually compiles to executable. Then we will go into details on how functions are actually implemented. Then we will go into details of object files which are produced by the compiler and assemblers. Then we will look into the assembler. We will look into linkers in very much detail on how linkers actually work. And some of the execution aspects are covered in the later half of the session where we will go into details of processes, virtual memory, how dynamic linking works. And then depending on how time permits we will go into details of various language features and how they are actually implemented in the compiler. So let's start with life of program. So what you write is some high level source code, CC++, Java, whatever. It gets translated into an assembly program by the compiler. And assembler will translate it into actual binary encoding. And this binary encoding is what actually the hardware understands. And you will run that program. Now if you zoom into these steps, what is actually happening is, so let's say you have program 1.c and program 2.c. You can compile it into an executable saying prog 1.c, prog 2.c and run that program. What is actually happening behind the scenes is you write your program in the text format as in something in ASCII form. You pre-process it. Pre-processor is essentially trying to handle all the things like hash include, hash if defs, hash defines and so on. And pre-processor is actually a language agnostic step. It has nothing to do with language as such. So pre-processor does not understand your program semantically. It's just a blind string replacement. And it will convert it into a pre-processed source. Then you will get to the compiler, which will actually do all the things which you have learned in last two weeks to convert it into an assembly program. Assembler's job is essentially to create object file or object program, which is essentially a binary version of the program. And then you get the executable file by feeding that object file into the linker. This executable you will actually run. This entire step before the loader is typically called as compiler driver. So when you say GCC, ABC.C, what is actually GCC is not really a compiler. GCC is a compiler driver. So GCC is internally invoking all these tools to achieve these steps. Now if we actually look into this, so let's get to a simple step. So let's say I have hash defined max 100. Now let's try to pre-process this code. So I can do GCC dash E sample dot C. And what you see here is actually all of these lines which you are seeing are actually contents of STDIO.H. So it's quite big. Let's step to the end. And you will see the max which was used is actually being replaced with the value of this. So this is the input which will actually be fed to the compiler. The next step which we saw was essentially the compiler which will actually produce the assembly file. So if we do that, so this is the actual assembly program which was produced by the compiler. Just to begin with you can see there is something called as main which was written in the program. There are some calls to functions which we had called like printf. There is a variable called array and it is taking 400 bytes because it was an integer and we had said array of 100. So you could see that somehow the program whatever we had written in C is also representable just that the instructions which are being used are assembly instructions. And then you can actually compile this to a sample dot O. We will see what goes into the object file in much more detail. And if you simply do this, then it is actually producing the A dot out which is the executable. And A dot out you can actually run and what will happen is loader in the operating system will come into picture and load that program into the memory. Now to zoom into details of each and every component, as I mentioned, preprocessor essentially takes the high level language source and converts it into the preprocessed source. And it is responsible for processing all the hash includes and those things. So linker is actually translating your high level language source into assembly program. And by now you have learned that this is a beast of the task. It is not a simple task. It is not merely one to one translation, but it also involves lot of analysis and optimizations. Compared to linker compiler, assembler is much more of a translator. So assembler does not actually do optimizations or analysis. And assembler is not just meant for translating the instructions into the binary encoding, but also handles things which we saw here like what is this dot string, what is this dot text. So most of the assembler supports some directives for you to write different parts of the programs And assembler is obviously responsible for handling them. And assembler will create the object files which have the information for linker. Now linker is essentially responsible for taking bunch of object files and linking them together. Linking would essentially involve resolving references. So for example, when you write program, when we wrote this program, we didn't know what is printf. So linker's job is to figure out where the printf is and kind of pull that in and so on. So linker is responsible for figuring out where is something used and where is something defined and connect those two things together. Any idea why we need a linker? So why do you want to write code in multiple source files? Yes. Correct. So that is actually one of the key ideas of why you want to write code in multiple source files. So essentially, let's say you have a very big code, let's say 1 MB worth of C code. And let's say you change one line in it. What will happen is entire compiler needs to process the entire code again and again for every small incremental change you are doing in the code. And that's not developer friendly because imagine every time you are compiling a program, your compiler takes five minutes to compile it because it has to compile large chunk of code, then that's not acceptable. So that is one reason. Any other reasons? So for example, why can't I create a source file, a separate source file and give it to my friend and say use this and he can just hash include it. So I don't write the entire source in one file, I hash include it. The separate compilation problem still remains, but it at least solves my distribution problem because so one of the things which Aman was saying is essentially you don't want to write large programs because it kind of reduces the modularity because it kind of becomes a problem. So you want to write it as collection of source files and it allows you to do code reuse because once I have written some code, I can reuse it at multiple places. For example, let's say you have implemented some set of functions for linked list. So linked list, insert, delete, whatever. And you want to keep reusing it across your multiple projects, then I can simply have linked list source code somehow included in my client code and use it. So essentially it allows you to create library of common utilities. One of the interesting aspect is allows you to distribute pre-compiled code. So for example, let's say you have implemented linked list and you want to share it with your friend, but you don't want to share in a form of source code because you don't want your friend to look at how you have implemented linked list. You are okay him using the linked list, but not how it is implemented. So pre-compiled code, if you give to your friend, he can only see assembly. From that he can still reverse engineer it, but less likely. Source code is much more verbose to understand compared to assembly. And then the separate compilation is the main reason where you don't want to keep recompiling entire parts of your code. But are there any advantages of keeping source code in one file? Are there any reasons why I want to keep source in one file? Correct. So optimizations, how does compiler work? So for compiler to optimize anything, it needs to have visibility. So when main calls foo, if compiler has no idea what foo is, compiler will have to make conservative assumptions that foo can do anything. So for example, if you have a global variable, it may have to assume that foo may modify that variable. So I can't reuse the value or foo may do something which I don't know about. So it reduces visibility of the compiler. So there are cases where you may still want to keep things in single source. But obviously then you lose the benefit of separate compilation. So it will be a tradeoff between how much optimizations you want compiler to do on your code versus how much modularity and separate compilation benefits you want. Now there are link time optimizations also. So linker can also do optimizations because at link time you will have everything coming together. But the thing is linker is always a bottleneck in the overall compilation process because it has to process thousands of objects feeding in. And if you start putting a lot of complex things into linker, your overall link time is going to increase. Plus typically linkers are designed to have very minimal information. Let's say you had to do optimizations at link time and the code is already scheduled and register allocated. Then can linker really undo all the decisions which compiler took? It's very hard for linker to undo the... So for example, compiler made some conservative decisions. Linker has to undo them. But to undo them, it has to change the code which was generated by compiler. And it has to change everything. As soon as it touches something, it kind of will need to touch most of the parts. So it becomes a practical problem that linkers cannot do all kinds of optimizations which a compiler can do. There are few things which it can do which we will see later. But primarily, remember this, by breaking things up, you are reducing visibility. And every component which we are going to study in the whole thing will have some visibility and some things which it has to assume. And for things which are unknown to that component, it is going to reduce the chances of possibly interesting things happening in that component. So essentially the object file which is generated inherently is going to be platform and architecture specific. And I cannot ship that... So for example, let's say you have compiled something for Linux. I cannot use that on Windows as it is unless I am doing some sort of virtualization kind of thing. And then I have my source and I need to distribute it. You need to distribute different versions of your code, of your object files which are specific to platforms. That's why every library or every ISV or every provider of source code, for example, when you download any software, you get option of which platform you are working on. And that is the reason because the code is going to be different. Does that answer your question or you had something different? Okay. Now coming to Loader. So Loader is essentially part of the operating system. And Loader is going to take your executable file and whatever command line arguments which you have given to run your program and is actually going to load it in the memory and execute it. So Loader's job is essentially to read the program from the disk where it is stored. So wherever you have a.out stored, it will read content of that, load it in the memory and start running the first instruction of the program. Now what is the first instruction in the program? Is it main? Correct. It's underscore start. And that's why we got that error earlier that there is something called underscore start already defined. And we'll see details of that later. So what is need for a Loader? Yes. Okay. Any other answers? Okay. So essentially if you had a system where only your program was running, nothing else, then technically there is no need for a Loader because your program has entire access to all the physical resources in the system. But we don't live in that world. We have processors with support multitasking. And every time you are running something, there are already running programs behind the scenes. So someone has to manage what is being run, where and who is allocated what memory and so on. So that's why you need a Loader because you have multitasking systems. So you have physical memory, you need to somehow distribute it across the programs which are already running. And you need a program which can look at the current state of the physical memory and ensure that correct things are loaded. That's why you need a program which will actually look at the current state of the machine and do something appropriate. And every time a program is run, the situation might be different, right? Because the system load might be different. So Loader cannot, I mean a program cannot be, cannot have all the information pre-baked in because it is going to be dependent on the time at which it is being run. And addresses will actually be unknown because we don't have the full view of what is happening in the memory. So if you look at it, this is kind of summary of what all is happening. So we had source code, right, which got to the compiler. You possibly had multiple source codes which produced relocatable object files which were fed to the linker along with some static libraries probably to form an executable. And executable was given to Loader and Loader had something called dynamic libraries which was actually executed. We will see details of the later phases here. Relocatable object files, linkers, static libraries. So beyond compiler, whatever you see is what is being covered in this model. And typically whatever is happening on the upper side of the screen is at compilation time and whatever is happening below is actually when you run the program. Any questions so far? Yes. What are the different directions? What are? So essentially we will get to some parts of it but just to answer your questions quickly. So essentially if you look at your program, your program largely consists of the instructions which are going to run and some data for the instructions, right. You might have global variables, you might have static variables and so on. So there are some directives which are essentially used for representing data in your program and there are some directives which are used to represent instructions in your program. Typically the instructions are called dot text. So anything with dot text is going to be set of instructions. And anything with, actually this was a slightly bad example, I should probably. So one second. So typically then there is data for you to declare. Then there are these kind of things, type and other things which we will get to a little later in the session on what they are trying to represent. Any other questions? We will get to that. So before we kind of start, I will just give brief intro of x86 assembly so that you are aware of the sample codes which we are going to look at. So x86 has actually bunch of registers. So EAX, EBX, ECX, EDX, ESI, EDI, these are 32 bit registers and these are actually general purpose. So by general purpose I mean you can use them for your general register allocation and so on. So holding variables in the program or doing anything with respect to spills, other things which you might have learned, these are the registers which are actually being used. And these are 32 bits and there are lower bit variants of this. So whenever I refer to AL register in my program, I am referring to the lower 8 bits. So whenever I refer to AH, I am referring to the upper 8 bits in the lower 16 bits and AX is essentially the upper 16 bits. So this forms EAX registers. And then there are two special registers ESP and EBP which we are going to learn a bit more later in the session. And x86 actually has two syntaxes. So x86 textual program can appear in two syntax formats. One is called as Intel form and one is called AT&T form. So in Intel form, the differences are mostly syntactical. There is no difference in the set of instructions and so on. So the operand order is actually on the Intel syntax, the destination appears on the left side before the source. So this instruction is actually saying move value 5 into register EAX. In AT&T syntax, it's kind of reverse. Sources appear before the destination. So 5 is being moved into EAX. Now the registers in Intel syntax are referred directly EAX, EBX and so on. In AT&T syntax, they start with percent. But the instructions need to have the mnemonic which represents the size. For example, here we had to say move l to say it is a word movement or move b to indicate it's a byte movement and so on. In Intel syntax, there is no such suffix. And immediate need to have dollar as the prefix. And essentially the addressing modes look something like this. You don't need to kind of know all of these details, but when you read the sample code, this might act as a reference. And most of the assemblers support Intel syntax, but the glue assembler which is on the Linux actually by default supports AT&T syntax. There is a way to make it support Intel syntax also. But most of the examples in this slide set are going to use AT&T syntax. For example, this is actually AT&T syntax. So you can see that percent is being used before the register names. There are suffixes which are being used to indicate size. And source appears before destination. So this instruction is actually saying move content of EAX into ESI, not the other way around. Now this is how your sample assembly program will look. You might have some data. And this dot byte is saying how much size it is occupying. So var is a 64 byte data. So it's an array probably of 64 bytes. X is a word, array of words. So it has three elements, one, two, and three. And then I have text program, which is essentially the set of instructions. And this is saying move the value of EBX is actually a pointer. So whatever it is pointing to, move that value into EAX. This is saying move two into that pointer value and jump. Are we OK with this? So you need this much understanding to understand the rest of the slides. The next part of this session is going to focus on how actually function calls are implemented. So essentially, programming languages support creating abstraction of functions. So you can think of function as block of code, which is typically intended to do one or related task. You write functions because you want reusability in your code. So for example, if you have done linked list insert once, you can put it in a function and call it multiple times and don't have to redo the same code again and again. So they provide you to write modular programs. And typically, every function will have a name from which you will identify that function. It will have parameters which are used by function to actually do the computation. So for example, if I want to compute power of two raised to five, so two and five can be parameters which dictate what should be the function's output. And just like they have input parameters, there are written values. So typically, a computed value from a function can be written and function will have body, which essentially is set of instructions, which will actually do the computation. And function can have local variables to actually allocate memory and do some stuff with it. Now every function typically has a single entry point. So whenever you call a function, you always start at start of the function. And the caller function, the function which is calling is called caller, that will typically be suspended when the callee is being executed. So whenever you call a function, the function which has called the other function will be suspended and the callee function will start executing. And once the callee function is done, it will return the control back to the caller and then caller will continue. So essentially, so let's say I had main, I call foo. So main is now suspended. Main is not actually executing. Foo will start executing. Then foo might execute some instructions and then call a function bar. Then it might call foo bar. Then foo bar returns to the caller function. So a function will always return to its caller function. And then bar will return to its caller function, ultimately to main. And then main is again free to do another set of function calls. This is how standard functions work in languages like C. Now there are alternative execution semantics also. What we have seen is the standard function. But there is something called as co-routines. How many of you have heard of co-routines? Two people. So co-routines are essentially, you can think of them like some sort of resume and return kind of a control flow. So when you call a co-routine, you initially start at the start of the function. And you can return to the caller function. When you are called again, you don't again start from the start. But you start from the point where you were suspended earlier. So it's more like a resume and suspend semantic. So the caller function will call it. Then co-routine will start executing. Then it will suspend for some time, return back the control to the caller. And caller can again resume it from the point where it started. Just like C, don't support co-routines. Newer standards in C++ have added support for it. But there is also something called as asynchronous execution. Where what I said earlier is whenever caller function calls the callee function, caller function is suspended. But in asynchronous executions, that may not happen. That means whenever caller function calls the callee function, both caller and callee are still active and executing concurrently. And then there is something in functional languages called as higher order functions, where you can return functions as values and so on. This will actually be covered in much more detail later this week when there is a module on functional programming. So all these semantics which I just mentioned are out of scope for this module. So we are not going to talk about how co-routines work. We are not going to talk about how concurrency works. And some higher order functions will actually be covered in this next session. So we will stick to the details of how traditional function calls are implemented. So what is needed to implement a function call? So here I have a sample code which is saying that main is calling a function foo. Main passes a parameter p which is accepted as m and then foo can have local variables and return a value which can then be used by main. So what all things you think are involved in a function call? What all is needed? Yes? Sorry. Return address is 1. Correct. There is some need to be able to pass parameters. Yes? Correct. So you need some way to say what was the state of the caller. Any other things? Return value. So essentially we need some way to transfer the control from caller to callee and this will involve return address because you somehow need to get back to the caller. You need some way to pass the parameters and get the return value. You need some way for the callee function to create its own local variables. And you need some way to do register usage which is essentially the context what you are talking about. So we will go into details of each of these steps and how they are implemented. So the transferring control mechanism essentially requires when the caller is executed, when the caller function calls a function, somehow the transfer needs to happen from caller to callee and once the call is done, somehow we need to get back. So most of the processors have instructions like branch and jump where they say that once the branch is done, it will start executing from that program counter. So we can very easily implement function call using that. So x86 has an instruction called jump which we can use. So essentially in main I am calling a function foo and I can simply do jump to foo. That is sufficient. I can simply say jump to foo and that will jump to foo and start executing that. But how do we get back to the caller? So how does callee know where we need to return to? Any idea? So it needs to return to the next instruction following the call. So we can save that address somehow where we need to return to and use that saved address to jump back. So we know that when the function call happened, we need to come back to the next instruction following call instruction. So we save that address and we do all the function which is being called and come back to the address which was saved. Which address really we already answered? And where to save it? You are jumping ahead. We will get there step by step and I will show you why stack makes most sense. So why can't we simply do, why do we even need to save the address? Why can't we simply jump back to the address? So for example, I know that JMP is at location this. In foo, I can simply say jump to this location which is the instruction following it. I can do that. Here I said jump to foo and in foo I said jump to this address. Will this work? Why it will not make any sense? So one of the issues with this is essentially it will always return to the same address. So you cannot call this function multiple times because remember the semantics were from the point where it is called, you need to return to the instruction following it. If I do it this way, I will always jump back to the address. So if this function was called only once, if foo was called only once, this is actually something which will work. So it is not a problem. It just that it is not general purpose. So if compiler could prove that this function is called only once, compiler is fairly, compiler is legal to do this optimization where it can simply say that jump back to this address and you are done. You do not need anything else. And that is where visibility comes into picture. So for example, if foo was defined in other file, will compiler ever know whether it is called only once or multiple times? No. So by putting things together, you increase the visibility of compiler, but you obviously have other disadvantages. So you will have to trade off. What is the other disadvantage of this? So that is one disadvantage that we cannot call this function from different locations. Any other issues with this? Sorry, someone said something. Recursion, correct. So this will actually also not work in recursion. Why will it not work in recursion? Correct. In fact, recursion has at least two call points, one from the main from where it is called and one probably within the function where it is doing a recursive call. So it kind of falls into the same thing that a recursive function will be called multiple times, once from the original caller and once probably from within the function. So it has to return to two different points. So that does not work. Now let us say I store it in a global variable. So I create a global variable and before I do a call, I save the address of next instruction into the global variable. And in this function, I return to the content of that global variable. So if there were multiple calls, just before the call, I will update the value of global variable and save it. Will this work? Why? Lot of variables is one problem. So how many variables you need? Will you need one per call or one per function? Why do you need one per call? What do you mean by calls are nested in this case? Correct. But that will still come down to one per function. Correct. So recursion cannot be done. But if recursion is not a feature in the language, then I can actually get away with one per function, not one per call. So for every function, I will have a return at a global variable which holds the return address of that. Wherever I am calling that function, I will save. So when I am calling function foo, I know I need to update variable called rate addr underscore foo. Whenever I am calling function bar, I will do rate addr underscore bar. Whenever I am calling function xyz, I know I will use a variable called rate addr underscore xyz. Recursion doesn't work. Any other issues with this scheme? Every time do you know which function you are calling? Correct. So when you have function pointer, does everyone understand function pointer? So pointer to function, right? And it can be changed to different pointers. So if this call was actually a function pointer call, then I don't even know which function I am going to call. So which global variable will I use? I don't know whether it will be rate addr foo or rate addr bar or rate addr xyz. So two issues with this scheme. Recursion doesn't work. Function pointers don't work. If those two are not the features in your language, you can still use it. Let's use a register. We have registers. So let's say just like global variable, why can't I store it in register? So let's say I say eax is my register where I am going to store it. So just before the function call, I am going to store it in eax. Then do the call and then use the content of eax to come back. What are issues with this scheme? Okay. That's actually a good point and we'll come to that. So one problem which one person is saying is that there are limited number of registers. We saw in the previous slide there were six general purpose registers. So we have only six registers. And the other issue is that the function itself may want to use eax for normal use, then I might end up overwriting it. Any other issues with this? Can I implement recursion using this? No. By default we cannot use. So I'll somehow need a way to save the value of eax, then I could do that. And if you call some other function bar, then I need to have either a different register or do something with it. So this does not work. Plus the other problem which kind of the other person was coming to that architectures typically have very few registers. And I may not want to waste my register for a return address. Because you must have already learned that register allocation is hard. And if you have fewer registers, it becomes further hard. And registers are key for performance. But if your architecture has a lot of registers, this may or may not be a concern. So there are some processors which may have large number of registers, in which case it might be okay. So far we have seen that somehow caller and callee need to agree on where the return address is. And return address is specific for every invocation. It's not once per function, it's once per call. And it needs to be that's why instantiated every time a function is called. And so far the schemes which we saw had some limitations. Now coming to the answer which was already given, stack. So essentially the intuition of stack really comes because you have programming languages which have recursive functions. If you didn't have recursive functions, some of the schemes which you already saw earlier like global variable would have actually worked. But since languages support recursion, inherently the notion of stack comes into the picture. So stack is a region of memory that will work in last in first out manner. I hope everyone understands what is a stack data structure and so on. And most of the processors which are modern do have native support for stack. By native support I mean they inherently implement stack in the hardware. So x86 actually has a stack which grows down. So it starts at some higher address and as you keep adding stuff into the stack, it will keep going down. Stack will always grow down in x86 style architecture. And x86, remember we saw something called as ESP register, which I said is not a general purpose register. So ESP is actually a pointer which is pointing to top of the stack. So ESP is always pointing to top of the stack. Then x86 hardware supports instructions to push new stuff onto the stack. So there is an instruction called push L. Remember L is a suffix to indicate 32 bit data. So push L is pushing 32 bits into the stack and it will increment the stack pointer automatically. So what push L instruction is doing is essentially it is subtracting the stack pointer by 4 and copying the data which is given into the stack location. Pop does the reverse of it where it will update the stack pointer to actually go back. So push is going to decrement the stack pointer and pop is going to increment the stack pointer. Everyone understands? So this is x86 64 architecture which is the 64 bit variant of the x86. Whatever most of the laptops which you will have will be 64 bit. So x86 64 actually needs a stack pointer which is 64 bit because the addresses are all 64 bit. So it has a register called RSP which is a 64 bit register and it will decrement the push instruction will decrement stack pointer by 8 and pop will increment the pointer by 8. Does that make sense? So x86 architecture had 32 bit stack pointer, x86 64 had 64 bit stack pointer and x86 also supports two instructions. Instead of using jump, we were manually trying to jump to some location. There is a support for call instruction and call is essentially going to call the function but it is not simply that. It is implicitly going to push address of the next instruction on to the stack. So call instruction is actually doing two things. It is pushing address of the new next instruction on the stack and then it is jumping. So it is similar to update stack jump. Written instruction is actually going to do the reverse. It is first going to pop whatever is top of the stack and then going to jump back. Does that make sense? So essentially just to demonstrate, so let us say I had this code. So I have a call instruction which is calling some address and the next address is this. Then my current instruction pointer is pointing to the call instruction and stack state is something like this where it is pointing to some data into the stack. What will happen is whenever I do a call, the address of the next instruction is going to get pushed on to the stack. The stack pointer is going to be updated to point to that and my stack pointer is decremented because stack is going down and my instruction pointer is updated to the actual function call which is being done. So instruction will start executing from that function. Return is going to do the opposite. So when I was going to do return, my stack pointer was pointing to the return address and my instruction pointer was pointing to the return instruction. The return instruction is going to update the stack pointer by 4. So it is going to actually increment it and popping the data which was on the stack and jump to that address. So just to summarize, call instruction will push things on to the stack and start executing the function. Rate instruction is going to pop the return address from the stack and going to start executing. Kids, what does this code do? It will give the address of LBL. It will give address of LBL. Any other answers? So remember, try to do. Call instruction, what is it going to do with stack and then what the next instruction will do? Can you explain how? Correct. Does everyone understand what happened? So call instruction, before doing the call, is going to push the address of the next instruction. The next instruction itself is whatever is being pointed by LBL. So the address of LBL is what is pushed on to the stack. And call is going to start again executing from that instruction itself. And then you hit this instruction pop and pop is going to pop whatever is top of the stack. So it will actually pop the address of LBL. So it is essentially getting address of the current instruction itself. Are there any issues since there is no return? Yeah, so return is not a must instruction for every call. As long as you keep your stack in the correct form, it does not matter whether you had a ret instruction or you did not have a ret instruction. Can someone tell me why we do this? Is there any use of this code you can think of? To get address of the current instruction, is there any use of that functionality? Why do you want address of current instruction? What can you do with it? Does everyone understand what he said? So there is something called a set jump and long jump in C. So you can think of it like similar to exception handling in C++ where instead of returning to caller, immediate caller, you may return to arbitrary points. So let us say you have main was calling foo, foo was calling bar, bar was calling foo bar. You can actually jump back directly from foo bar to main without having to go through this chain. But that is again out of scope for this module just for the record. I am not going to talk about how set jumps and long jumps are implemented. Any other use you can think of? It does not seem like something very common. But we will see tomorrow where to use this. So remember this that way to get address of the current instruction is by doing this where you call the next instruction and you pop from the stack. And we will see tomorrow where we actually can use this trick to implement something. Now so far what we have seen is how the control transfer happened. Any doubts in that? How call and return happened? Yes. What is the function to call? Correct. So that goes into, so proper use requires just before the write instruction to have written address on the stack. If you do not do this, then you have arbitrary code and then anything can happen. So if you do not, that is why compiler generated code typically will not have such things. And whenever you are writing, that is why you need to be aware of these kinds of things. So if you are aware of these semantics, then you can generate code that way. What will happen is if you have arbitrary thing on the stack, it will simply happily use that address and start executing that. No. Can you write analysis which will detect if stack is in the correct state? So let us say you are seeing the assembly code. How will you analyze that stack is in the correct shape just before the return? Do you always know? What if there is a loop with n and I am doing something? I mean you could do something statically, but may not be possible always. But in simplistic case, yes, you could say that there are three pushes here and three pops here. So I must be in good shape. Correct. Yes. So now let us look at the local variables. So a function can essentially define local variables which it can use for its operation. And there are two types of local variables, CC++ supports. One is variables which have auto storage or default storage. So whenever you write something like int var, it is something with the auto storage and you can also write something as static. What is the difference between the two? Yes. Correct. So essentially, the scope of the variables is only within the function. That means I cannot access CNT outside this function. Same way I cannot access var outside this function. But the lifetime is actually different. So var and arr are actually have lifetime only of the function. So whenever the function is called, they are created and whenever the function returns, they are destroyed. Static variables on the other hand are created for the entire life of the program. their values even if you call the same function multiple times. And when do you think are auto variables allocated? So every time you call a function, you need to allocate them. So every time a function is called, they are allocated, initialized and deallocated every time the function returns. Static variables we will see later on how they are allocated and what happens to them. So where should we store auto variables? Still go step by step. Can we store them in registers? Why not? Correct. So the first problem is limited registers. But are there any other cases? Okay, so we will come to that. So let us say I have a function, let us say which it does not call any other function. Let us hypothetically assume. Yes. So that is one problem you already discussed that you have more number of variables than you have registers. So let us say I have array of four elements. Can I store it in four registers? So let us say number of registers is not an issue. For now. That is always a practical problem. But for now, let us assume you have either two very few local variables or you have many registers. In that case, what are the problems? Can you still have? So let us say you had five variables and five registers available. Can I always store my variables into registers? So that is also not an issue. Correct. So essentially, you might have ISA constraints, where a particular register may need to be free for some reason, because the instruction implicitly modifies it and so on. Leave that aside. So let us leave aside all the register related issues. You have sufficient registers. You have registers which you can truly use for your purpose and there is no other issue. So why do you think that matters? So one issue which she is saying is data types might create a problem. But so to answer to that, what might happen is you might waste space. For example, you might say that even character I am storing in a 32 bit register. So it is a waste of space in the register, but that is not the most interesting problem. Let us say array. So let us say I have declared an array. Array of four and I have four registers available. Can I store array in? How? So when I do EA, somehow when I do address of EAX plus one, do I get EBX? Okay. Correct. So that is getting to core of the issue. So essentially think of it this way. So let us say I had declared a, can I use whiteboard? I will switch to terminal. I think that might be easy. So let us say I have this function foo, to which I get an index. And I declare int error of four. And I do printf percent d error of i. How will this code work? So let us say I have put this in EAX, EBX, ECX, EDX. How will that work? Correct. So one bad way is to do this. So it kind of undoes whatever optimizations you have learned and says generate bad code. So bad option. The fundamental reason for this is what she was trying to say. That there is some indexing which is happening in my code, but the registers which I have are not indexable. So I cannot do. So if I had something like this, base register plus i gives me base reg of i. If something like this was there, then I could actually do it. Okay. So if my hardware had something like this, where if I do r0 plus 1, I get r1. r1 plus 2, I get r4 and so on. Then I will actually be able to do it. Otherwise the only option is to do this bad thing. Any other reasons which will force you to not use a register. So we saw one case where indexing is causing a problem. Any other issues why you cannot use registers? Correct. But that is something register allocation should have already done. So whenever you had a variable v, it was let's say allocated by register allocator to eax, then register allocator is keeping the map of some sort. That wherever var is used, use eax. Correct. So, yeah. So large, I mean data which does not fit into registers, there could be two reasons for it. One is you don't have sufficient registers or the data size itself is not sufficient within the registers. And then second is this indexing. Any other reasons? Let me write some code. So let's say I had var. So let's say I had var,var2. I do something like this. Now can I put var and var2 in registers? Why? Correct. So registers don't have address. So whenever there is a variable whose address is taken, I somehow have to put it in memory. Otherwise, there is a problem. What if I do something like this? I mean let's say compiler was still put var in eax and var2 in ebx. Why can't compiler do this? If condition ptr is equal to content of star ptr is equal to content of eax, else star ptr is equal to ebx. Why can't compiler do this? So I have still allocated var in eax, var2 in ebx and I did something like this. Is it valid? Is this code valid for compiler to translate to? Correct. So it goes back to the same point of registers not having address. But the practical problem to be able to translate it into this is it has to be able to analyze that ptr actually points to these two variables. And in a sufficiently large code, it's going to be a Herculean task. You've learned pointer analysis. How was it? So it kind of starts getting impractical to be able to do it precisely. So that's a, I mean, if you could figure out that this variable is only pointed by this pointer and it's always dereferenced this way, in theory you can do this. But it goes back to the problem of pointer analysis being very hard. So these two, apart from not enough registers, whenever you have address of a variable taken, you will have challenge storing it in register. And whenever you have indexed accessing, you have problem because typically, hardware don't have indexed register accesses. So can we store them in global variables? So whatever local variables I had, can I create a global variable corresponding to that and allocate them in global variables? What kind of scope violation? So this, remember this is being done by compiler, right? So compiler, it's not the programmer which is trying to do. So let's say for every local variable compiler allocated a global variable while generating assembly and use that. So just, so for example, for in this case, it created foo underscore var as a global variable, foo underscore arr as a global variable. What are the issues with that? But for every function, it will create a, I mean, it has to just create a unique prefix, right? It can do that. Any functional issue, why you can't create a global version of a local variable? So one answer is it could impact how much memory is available. That's strictly not true, but yeah, it could be a consideration. Correct. So what will happen is as soon as you create a global variable, you are breaking the recursion semantics because every time the function is called, you have to somehow initialize it properly. And what is, this is the exact same problem we had with the return address. Global variables will create one copy of a variable per function, but is that what really local variables are? Local variables are ones per invocation of the call, right? And that's how they get the semantics. So this will not work in recursion. What about stat? Right? That was the answer. And where we have static local variables, we'll see later. So stack is essentially where actually local variables are stored, right? So there is a term called a stack frame, which is essentially the stack which is allocated for a function. So every, so region of stack which is allocated for a function is called a stack frame of that function. So for example, this is the stack frame of foo and this is the stack frame of main. So we already saw when the call happened, the return address was actually pushed onto the stack. So this return address lives here. There is something called a saved EBP, which we will get to later. And then whatever local variables I have, I have stored them. So I have stored var, I have stored err of 6, err of 5 up to err of 0. And my stack pointer points to the bottom of the stack. This is one of the ways to implement an activation record. Is everyone aware of something called as activation record? So essentially, whenever you do a function call, right, whatever state is needed to be saved is called as activation record. And stack frame is one of the ways to implement an activation record. There are different mechanics. For example, coroutines and higher order functions require a different type of activation record than a stack frame. So it will vary based on what are the semantics you are trying to support. So stack frame is not the only activation record. Depending on the feature you are implementing, it may be different. Yeah, so any questions so far on this? Now how do you allocate space in the stack? So let's say I am compiling this function foo. I know it requires 7 integers for err and 1 integer for var. So I need 32 bytes. So what will compiler simply do is it will insert an instruction called subtract 32 from ESP. So whenever you look at the disassembly of any function, you would see an instruction like this, which is essentially allocating space in the stack. So whatever is your requirement of that function, those stack pointer will be subtracted by that much. Now how do you access actually the variables? So what is address of the var? So whenever I want to refer to var in the program, how will I refer to var? So my stack pointer is here and my var is here. So I could simply compute its address based on stack pointer. So I can compute how far is the variable from the stack pointer and use that. So in this case, it is 28 because we had 7 integers in between. So var will be at stack pointer plus 28 address. So what you will see is if you look at the actual generated code. So what you can see is there is some subtraction of stack pointer which has happened, which is essentially allocating stack. And then you can actually see the stack pointer being actually. So you can see that the variables are being accessed relative to the stack pointer. Now frame pointer, which is EBP. Any idea why is that needed? Why do we have a pointer which is pointing here and stack pointer is pointing here? Sorry. Correct. I will get to that. Yes. But how will frame pointer ensure that? But so are you expecting hardware to report some sort of error when someone accesses ESP plus some offset which goes beyond EBP? Is that what you were expecting? Okay. Yes. Correct. Okay. So far we have three answers. Any other things then we will try to summarize. Correct. So first I will kind of answer your questions, your things. So one is there is no such mechanism which tries to check whether we are trying to access something in the caller frame. There are some security measures which do that, but by default those are not what happens. To answer your question, your thing, see whenever you are referring to the variables, even in case of recursion, you are always referring to the variables which are in your stack frame. So for example, when I am, let's say when foo calls foo, the second foo is still referring to the variables which are of that instance of the foo. There is no way to say refer to the parent's variable. So there is no need for you to generate code which will try to access. So for example, when let's look at this code, so let's say this was calling foo recursively. There is no way for me to say access var which is of the parent. There is no construct and see which will allow you to do that. Whenever you refer to var or arr, you are always referring to the ones which exist in your stack frame. So this will always be the same place from your stack point. So what happens is as stack keeps growing, the offset of the variables will keep changing. So var at this point in time is at 28 offset from ESP. Let's say within the same stack frame, I allocate more stack. Then relative address of var from foo, from the stack pointer is going to change. See if stack is going down, then the var also is going up from the stack pointer. So it might be at 28, then it might become 32, then it might become 36 and so on. So what happens is it creates issues because the same variable has different addresses at different points in time. But if you look at the frame pointer, frame pointer is always at the start of the frame. So all the variables are always at the fixed offset from it. So for example, var is always at EBP plus 4. arr of 6 is always at EBP plus 8, EBP minus 8, minus 12 and so on. No matter how much stack grows, once you have allocated something on the stack, it will always be at the fixed offset from the EBP. Does that make sense? And you can actually access it like this. So 5 is being moved into EBP minus 4, which is what is this statement trying to do. Fair enough? Now why can't compiler handle this? See the reason which I gave is kind of bit not good because what I am saying is it is a complexity to track what is the address of var relative to stack. So why can't compiler do it this way? So we have var and in the current set of code, var is at ESP plus 28. Then compiler updates the stack pointer. Remember compiler is updating the stack pointer. So compiler allocates space in the stack. Then it knows now var is now offsetted at 32 instead of 28. So why can't compiler do that? Does everyone understand what I am saying? So essentially what we said is from EBP a variable is always at fixed offset. From ESP it is going to be at variable offset at different types. Now the question is why can't we have compiler track it? So compiler can track variable var now is at ESP plus 8. At some point when the stack pointer is updated variable var is... Yes? Correct. So it is a complexity which you have to deal with. So let's say I had a loop something like this. So in this region of code let's say var is at ESP plus 28. Then I have a loop which creates additional variables. So stack pointer is updated and then int dummy 2. So my stack pointer further goes down by 8 bytes and now here I access var. So var is now at actually address ESP plus 36. But what if this was a loop which goes on from i equal to 0 to n types. Can compiler handle this? If value of n is fixed then it can. If value of n is unknown then? The language semantics say these are created at every iteration of the loop. I mean you can save the space. You don't have to allocate new space every time. But the semantics are these are not created only once. They do get created at every iteration. I'll come to that. It's bit more subtle but I'll come to that. So essentially actually compiler can deal with this. Instead of accessing this as ESP plus 36 it can use a dynamic offset. So it can keep adjusting the dynamic offset also as it is incrementing the stack point and access. So this is not actually a problem. Yeah but that all it can account for in the offset computer. See the stack pointer is being manipulated by the compiler. So it knows at which points how the offset. So think of it this way. For every variable compiler keeps track of what is the offset of that. In common cases it will always be a constant as in an image. But in some cases it may be a dynamic value which adjusts itself in tandem with how the stack pointer is being updated by the compiler. So if stack pointer increments this offset value also increments. If stack pointer decrements this value also decrements. So compiler can deal with it. But in fact you could see that. So in the code which I had shown you earlier compiler is doing accessing variables relative to stack pointer. Now what actually happens is frame pointers actually form a linked list of frames. So remember this saved EBP is actually holding the original EBP of the main. So this is actually a content which is pointing to start of the stack frame of main function. Then main will have its own thing. So every time I am calling a function and I am saving my EBP what is happening is I am creating a reverse linked list from one function to other prior function to the prior function to the prior function. And this is actually used by debugger to implement backtrace. So how does debugger know where is the caller function? It knows by reading the EBP value this value at EBP and then reads that and goes to that and goes to that and goes to that and so on. Does that make sense? So frame pointer is serving two purposes. One is it is giving you a way to access your variables at fixed offset. So that reduces the complexity which compiler had to deal with. But let's say your compiler is sophisticated enough to deal with that complexity. Then still frame pointer is useful for tools like debugger, profiler or any binary tool which is going to operate on the program to figure out the call stack. Now what I did here I kind of went over quickly. So by default when you compile using the compiler it is going to use the frame pointer. So if I generate my assembly it is actually going to access variables using frame pointer. But I have an option to say f omit frame pointer to tell compiler please don't use frame pointer for being a frame pointer use it as a general purpose register. So frame pointer can actually be made a general purpose and in that case I will actually have all the accesses relative to stack pointer. Yes. Correct. So that's what I was saying earlier. So let's try to do that. Let's say I had this code. So here I know because things are statically allocated. Now here the problem comes because the offset of var has suddenly changed. And let's say here I create a few more variables new var then here offset has again changed. Now one option which you have let's say you were writing the compiler what can you do. So one option you have is you get the initial thing and here whenever you create new variables you update your stack pointer. So you keep track of how your stack pointer is being updated and change the offset for every variable accordingly. Now compilers actually do something more simpler than this. Compilers actually don't. So if you look at the semantics of the language it says dummy and dummy2 are created every time this loop is entered. But compilers actually don't do that. What compilers will do is here only they will allocate space for space for var arr dummy dummy2. See because when compiler is analyzing code it is doing multiple passes. It's not that it is interpreter. So it's not that it doesn't have visibility of what is going to happen next. It does have. So it is going to allocate space for everything. At this point it is just going to insert code which will reinitialize dummy and dummy2. It will not actually allocate more space into the stack. So compiler will actually try to allocate most of the stack space at start of the function. So let's try to do let's look at this. So let's say I had var and var is equal to 7. So if you look at the sub l instruction it occurs only once. It didn't occur in the loop. So what compiler simply did is it figured out how much stack space is needed and whatever is needed it allocated it at once. So by then it would know where I am putting var, where I am putting i, where I am putting dummy. So that simplifies the tracking part for compiler. But it's more of an implementation simplicity nothing more. In theory compiler should be able to track whenever stack pointer is updated update the corresponding offsets. But compilers kind of hesitate to do that because it causes unnecessary work. So we saw that frame pointer is useful for referencing variables within the stack frame. But also it forms the ability to create backtracks. Now to be able to figure out now what all is needed to return to the caller. So remember what all happened within the callee function. Apart from the body of the callee function itself callee function did update the stack pointer. And coming back to the point which you were raising earlier what will happen when I do return when the return address is not at top of the stack. So we need to ensure just before return the stack pointer is always pointing to the return address. And compiler can simply do that by updating stack pointer to move point to the frame pointer and then return. Because see what compiler is doing is it is moving the base pointer into the frame pointer. So stack pointer also started pointing here. Then it popped the EBP from here. That means stack pointer is actually pointing to the return address and then it did return. Does that make sense? So just before returning you had allocated space into the stack. So you need to ensure the stack pointer is now pointing to the return address so that when ret instruction executes it will actually return to the correct address. So this is essentially called as a stack clean up from callee function. So there are two terms which you might hear. There is something called as function prologue and then there is something called as function epilogue. So the set of instructions which you see at the beginning of the function is essentially function prologue. And function prologue typically includes saving the base pointer, original base pointer, updating the stack pointer and then subtracting space into the stack. That means you are allocating space onto the stack. So all of these instructions will happen at start of the function typically. And at the epilogue which is just before the return you will update the stack pointer to point to the base pointer. You get the value of the base pointer and you pop the return address when you do the return instruction. So these two instruction sequences are called function prologue and function epilogue. And you can actually see them in this. So if you look at this, so here we see saving of the frame pointer and subtraction of the stack pointer and pushing the original value of the base pointer. So this forms the function prologue pretty much. Epilogue actually is kind of handled by instruction called leave. So in x86 there is an instruction called leave which essentially does this. And then there is also an instruction called enter which is equivalent to this. So you either might see these instructions or you might see a leave instruction or enter instruction. Any doubts so far? So can you tell me what is output of this code? How many of you think it is garbage value? Two people, three people. Garbage okay. So four or five people think it is garbage. What do others think? Undefined behaviour. Can someone explain what is the problem in this code? Yes. Okay. Then what is the problem with that? So does everyone understand the problem? So remember what we did. In the prior thing I said that once just before the function is returning the stack pointer was updated. Right. So it was pointing to now the caller function. So what happened in this case is I created this arr variable and I returned address of it. And before I returned the stack pointer was updated. So this was no longer in the stack. Now the question will it actually print garbage or will it print valid value? I guess it prints some valid value because the stack manipulation has to be defined to implement the increment of the stack. Correct. So that is the difference between someone knowing the internals versus someone just saying this is not going to work. See these type of things will actually help you debug things easily. This code will actually print a valid value. So from if you just observe the execution of the program there is nothing wrong with it because you will still get the expected value. And the reason for that is stack allocation, deallocation simply move the stack pointer. It didn't really modified the content of the memory. So unless something happened with modified that content you would still have that value. So in most of the cases when you run this program you will actually get a correct and expected value. It just that the code is still buggy. The code is still buggy because in some cases it may actually result in set-fog. Now he coined something like undefined behavior. Does everyone know what is undefined behavior? So if you read any language standard, especially C, C++, it has three terminologies. It has something called as implementation defined behavior. So what C says is if your program does this then it is implementation defined behavior. The conditions are there are just too many conditions. But essentially whenever you hear a term called as implementation defined behavior what it means is C language doesn't say only this thing should happen. C language says compiler and the platform and the system it's free to implement anything for this particular case but it must document what it is doing. So for example C might say that size of integer is implementation defined behavior. So C says I don't know what is size of int but every implementation must define it. So this is implementation defined which means implementation must document what is the size of integer. There is something called as unspecified behavior. Unspecified behavior implies it is again not spelled out by the standard that in this case this should happen but it is a bit lower than implementation defined where compiler does not even need to document what it does. And we will see one example of unspecified behavior later. And third is undefined behavior in which case C standard says anything can happen including compiler generating code which will do RM minus RF slash. So compiler is free to do anything if your code has undefined behavior. And these kind of things are defined because they give flexibility to the compiler and optimizers to do something.