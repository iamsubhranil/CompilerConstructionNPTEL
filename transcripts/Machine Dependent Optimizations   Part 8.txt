 Okay, so good morning and welcome to session. So we were looking at basic block instruction scheduling. Yesterday we saw how cycle-based instruction scheduling works. We had some trouble with the operation-based instruction scheduling. I have now fixed that, so let's see how that works. In cycle-based instruction scheduling what we have is that we have a ready list and we go from time step t all the way till all the operations are scheduled and what we do is that at each step we try to schedule one of the ready operations, one or more of the ready operations depending on the availability of resources, right? And then also we do this in the decreasing order of priority of these operations. If once all the operations that can be scheduled in the current cycle are scheduled, we include the cycle and then we also make sure that whichever other operations which have become ready we will put them in the ready list, reorder them in terms of their priority and we keep doing this every cycle. So it's essentially going from time step 0 to all the way up to time step t and at every time step you try to schedule all possible ready operations that are possible to be scheduled in that cycle. And as you construct the schedule you also maintain how many resources are being used in each cycle. So these resources which are being allocated for various cycles, that resource allocation table is something that you maintain as you go towards, go in doing this schedule. Now let's see how operation-based schedule works. Operation-based schedule works in a slightly different way, okay? So initially you start off with an empty schedule and then you compute the earliest start time and latest start time of all the nodes v in that graph, right? And once you have completed doing this, then what you do is that you first select an operation with the highest priority, okay? It could be at any cycle, but you first select that particular operation and you try to schedule that operation from that operation's earliest time, EST or ASAP. EST and ASAP are the same, right? This is called as soon as possible time or earliest start time. So I could have strictly speaking used EST, but yeah, maybe we should correct it, right? So that this we will say as EST, okay? So you start off that operation, I mean you try to schedule that operation starting from its earliest start time to the maximum time, right? So the next time could be a very large value. So you try to keep scheduling these operations, right? And then try to schedule it at that particular time step where there is no resource conflict and then add this to the schedule. Again as you construct the schedule, you maintain the resource allocation table like what we discussed in the previous class, right? Remember that you have a table of all resources, correct? So maybe integer unit, floating point unit, load store unit, okay? Time step 0, 1, 2 and so on. So if you schedule an operation V, let's say at time step 2, then you mark that as V depending on which function unit it is supposed to be using, correct? Similarly, if the next operation is some U which is getting scheduled at time step 1 and let's say that's a load store unit, then you put it over there, right? And as you keep doing this, you kind of mark all the resources that are being used. So if there is any other operation, let's say, which also has to be scheduled at time step 1 in the load store unit, then you find out that the load store unit is already busy. You cannot schedule that. If the load store unit is a pipelineed function unit, then you can schedule another operation at time t plus 1 or t minus 1 wherever it is. But if it is non-pipelined, then you have to wait at least until t plus k cycle, okay? That's really how you do that, okay? So once you schedule an operation, you check what is the time in which you have scheduled this operation. If the operation has been scheduled at its early, if the operation is scheduled beyond the max time, that means that you are unable to schedule the operation. That means that because you are trying to schedule other operation with the operation, you are unable to fit it in, okay? So what you need to do is that you need to do some backtracking, right? Okay, I am not going to go into more details of this, but this backtracking also does a lot of other things. But for the time being, we'll simply say that backtracking takes care of all those complications, right? Okay, you are able to schedule, but you scheduled it beyond the earliest start time, right? Then you also have to go and then change the successors' earliest start time because this node has been scheduled a little later. Those nodes can only start like that. There is also one other step. You also have to check all the predecessors to make sure that the predecessors which have already been scheduled, correct? This node has been scheduled early enough so that they will get the thing. So for example, let us say that nodes v and u are already scheduled, and I am now scheduling a node w which requires two units of latency, correct? So let us assume that there is a node w and then there is, sorry, it has a dependency to v, right? And let us say that this has a latency of two cycles. Now if w is scheduled in this time step, then there is a problem because v has already been scheduled, w must have at least a latency of two cycles, correct? And that two cycles if it is not met, right, then the schedule is an invalid schedule. So that also needs to be checked. In fact, the way in which it is checked, which I have not shown here, is that this max time for each operation that you see here is actually appropriately chosen. I didn't show because it is very complicated, right? If no successors have been scheduled, then the max time can be anything. But if a successor is scheduled, then that max time is actually successor schedule time minus this execution time. You cannot schedule it any more than that. And then if you will come out, you will again try to do a backtracking. So this is essentially how you do operation-based schedule. That means that you have this table, resource allocation table, on which you try to put these operations. And as you keep putting these operations, you ensure that all the dependences are satisfied as well as all the resource constraints are satisfied. If you are violating anything, you try to go backtrack and then try to remove this operation and schedule it in a later cycle. If you try to schedule this in a later cycle, that might affect something else, okay? That may have to be scheduled earlier or later or whatever it is, right? So one has to look at all of those complications in the backtracking algorithm. We will not go into the details of those things for the time being, okay? That should give you a general idea of how the instruction scheduling works. Again, the important point is that you want to make sure that all the dependence constraints are satisfied and all the resource constraints are satisfied. This is essentially what it is. So try to fit this in this table so that these are satisfied. There are several heuristic methods which try to accomplish this, okay? Now let's see, let's go back to this particular example which we were seeing earlier, right? And let's try to look at possible schedules for this. In this particular graph, you can see that for each node, I have actually put the earliest start time and the latest start time for them, right? You can see that this is the critical path. That is nodes add one, subtract two, multiply and add two. That is the critical path. All arcs have a latency of one cycle except when it is annotated. So here the latency is three, here the latency is two, and so on, right? But these are one, okay? Now what I have done is I have computed the earliest start time as well as the latest start time for all the nodes. Now on the right hand side, I have shown you a schedule, right? This is a valid schedule, okay? Here again I have assumed one integer unit, one floating point unit, and one load store unit, okay? That's essentially what I have assumed. Or you don't even have to think of it as a floating point unit if you want, just think of it as a multiply unit, okay? Three functional units, right? This is the schedule. Is this schedule correct? Does it satisfy all the dependencies? Quickly check. Add one, subtract two, correct? So this one cycle latency between them, that has been satisfied. From subtract to multiply, there is one cycle latency that has been satisfied. From multiply to add two, three cycle latency. On the other side, add one to load one, there should be a one cycle latency, more than that, okay? And then from load one to add two, there should be two cycle latency, okay? That has also been satisfied, okay? Now this one, add three to load three, yes, sorry, load two, that has also been satisfied, okay? Now could we have done better, right? Could we have done better? So multiply to add is three cycles, so that is the latency. So if this add is here, then this multiply here is fine, okay? So you couldn't have finished earlier than seven cycles in this schedule, correct? However, right, if I try to reschedule it such that add one, subtract two, multiply and add two are scheduled at its earliest start time as well as its latest start time because their values are same, then what happens? Then I am able to finish it in five cycles. Look at the previous schedule. In the previous schedule, okay, right, in the previous schedule, add one was scheduled at its earliest start time. But what about subtract? Subtract was scheduled beyond its earliest start time. It is correct. It is legal. Nothing wrong in that. But you are not going to get the best schedule, right? So because subtract two was delayed, multiply was also delayed and then add two was delayed, okay? Why was subtract two delayed? Subtract two was delayed because add three was scheduled in cycle one and add three is a non-critical operation, operation which is not in the critical part, right? But somehow we scheduled that operation and because we have scheduled add three, subtract two could not go there because they both use the same function unit. It is a resource constraint. Therefore, subtract two went to cycle two and because subtract two went to cycle two, you have a total of seven cycles. Whereas in the next schedule, what we did was we scheduled add one, subtract two, multiply and add two, all of them in their earliest start time, right? And as soon as we have scheduled them in the earliest start time, we were able to get a schedule which is of length six, okay? The add three, which is a non-critical operation, could have been scheduled in time step two, which is well within its earliest start time to latest start time. It is between those values. But last time we greedily scheduled this ahead and because of that, a critical operation has to be pushed down. So when you do instruction scheduling, the priority function is essentially supposed to ensure these things, right? And you should use priority functions which kind of help you do this, right? That's really what it is, okay? Okay, let's move on to the next topic, which is about global instruction scheduling. So far what we have seen is that we have seen instruction scheduling within a basic block. And when you do instruction scheduling within a basic block, the advantage you have is that you have no problem. The instruction can be moved anywhere within the basic block as long as it will be data dependent, right? But can you move an instruction beyond the basic block, right? That's the question, okay. We will actually discuss that when we have the control flow graph which is in the next slide. But before that, let's ask why do we want to do things which are beyond basic block? Why not just limit ourselves to basic blocks, right? That's the first two questions. If we can do well within the basic block, then we are good. Don't need to do anything more, correct? But then we know that typically basic blocks are small. They have about six to eight instructions or six to ten instructions. So the opportunities that you have to move operations is actually limited. In the other examples, we kind of nicely put things so that we still had some opportunities. But in practice, this may not be the case. And you may not be able to take care of all the stalls by doing reordering within the basic block because typically some instructions like multiply or divide could take as many as 32 cycles. And when they take that many cycles, you need to have more instructions so that you can reorder them to get whatever, you know, stall cycles that you have you can avoid, right? So if you only limit yourself to basic block, then the opportunities that you have for scheduling is kind of limited. If you go beyond basic block, then it helps. Okay? So one way of increasing the basic block size is unrolling of loops. When you unroll the loops, the basic block sizes increases, right? But okay, we also want to do things which are beyond basic block so that you have more opportunities. But let's see what is the problem in trying to do something which is beyond the basic block. Let's take this example. Again, it's a very simple example. So you will still see only a few instructions, but in a real example, maybe there are more instructions. But we want to fit everything in a single slide. We have to take it as small examples. Okay. Now here is the code for the program or the program segment which is on the left-hand side. Let's not worry about whether it's right or wrong. It should be right. Okay? Or maybe there are some minor mistakes which you can fix it. Right? Now what do you see here? A sequence of about 11 instructions. Right? How many basic blocks are here? Okay. One basic block? More than one basic block. Okay. That's fine then. Yeah. So what's the difficulty? So let's go from instruction I1, okay, all the way up to I6. That is basic block, one basic block. Then what happens? I6 jumps to I9. Okay. So I9 must be sort of a basic block. Okay. And now let's look at between I6 and I9. They are in the same basic block. So that's another basic block. I9 is a jump point. Okay. Oh, I must have missed a branch instruction somewhere over here. Okay. Yeah. So I9, I10. Okay. So I9, I10. Okay. Should be a, yeah, I think that should, oh, let's just look at this. Yeah. Oh. You people are not following me correctly. So there is, this is the first basic block. I missed this branch not equal. This is the first basic block. And then since it jumps to I7, from I2 to I7 or I2 to I6, there is another branch. So there is another basic block. Then I7 to I9 is a, sorry, I7, I8 is another basic block. I9, there is a branch. So there is another basic block. Correct? So there are four basic blocks. I missed this branch not equal. Okay. So let's look at this code. Right? And then separate this out in terms of basic blocks. Now it is clear. Right? So these two are one. These two instructions are in basic block one. These four are in another basic block. Okay? These two in basic block. Why is I9 a separate basic block? Where is the branch? Yes. So I9 is a labeled instruction and there can be a jump to I9. Therefore, it is a sort of a new basic block. Okay? Correct. Okay? So this is how the control flow graph looks. Now if you want to do basic block instruction scheduling, then essentially it means that you try to do scheduling between these two instructions. You try to do scheduling between these four instructions. You try to do scheduling and so on and so forth. Correct? Can I move any of these instructions from here above this basic block? In basic block scheduling, that is not possible. Correct? But if I need to do that, then I have to be careful. So that's really what we are going to see next, which is what we call as global instruction scheduling. Okay. Let's see what happens if I just tend to do, try to do basic block scheduling. Correct? So again, we will assume that there is a one stall cycle between a load instruction and a subsequent add instruction or a subsequent instruction, which is dependent on that. So between this load and this branch, there is going to be a one stall cycle delay. And I can't do anything about it because I don't have any other instructions which I can put within this basic block to do that. Now the next four basic blocks, again there is a dependency from load to add. Okay. And then from add to store and then from store to no. No, that's all. Okay. Right. So that's the end of another basic block. Let's just look at the basic block boundaries. That makes it easy. Okay. Correct? So this is the best schedule that you can do. At best you can put the branch instruction in parallel with the store instruction. Right? So here you are not able to do any scheduling because there was a dependency. And because of this dependency, you also have to incur a stall and you couldn't hide that stall with any other instructions. In the next case when you are doing the scheduling, because of this load and add instruction, there is a stall and you couldn't really do anything except this two instructions scheduled together. Right? Then in the next case you can actually do a move and store instructions in parallel. But we have left this cycle blank. I will come back to this why we have left this cycle blank. Okay. Leave that for the time being. Then of course when I come to the merge block, correct that merge block is essentially having these three instructions. These two instructions are independent of each other. I can do them in parallel. Then there is a branch instruction which is dependent on this heighten instruction and therefore I am doing this. Okay. So if I want to schedule this control flow graph or the instructions in this control flow graph using basic block scheduling, I will do something like this. Right? And what does this really mean? When I follow this graph, okay, so let me just go back. Okay. Let's just stay here. So when I execute the then branch, I take this path, okay, and then from here I come here and then complete. So it should really be nine cycles. Correct? So you can just follow this one, two, three, four, five, six, seven, eight and nine cycles. When I go through the else branch, it's going to take three cycles on this side plus two cycles here and then two more cycles there. So seven cycles. So what I am going to call is that the then branch is going to take seven cycles and the branch is going to take, sorry, the then branch is going to take nine cycles and the else branch is going to take seven cycles. This is the best that I could do. Okay? So we are unable to move instructions beyond the basic block because we are doing basic block scheduling and this kind of limits what we can do. Right? So if you want to move instructions beyond basic blocks, then you have to make sure not only the data dependencies are satisfied but also control dependencies are satisfied. Okay? Now what we will do is that we will try to form what are called regions or things which are beyond basic block and then try to do instruction scheduling, but when you move instructions beyond the basic block, you also ensure that the control dependencies are satisfied. I will explain to you what I mean by this, okay, again using our example. When you do global instruction scheduling, what all you need to take care of and what are the different ways of doing global instruction scheduling. Okay? Now given that that you want to do beyond basic block scheduling, first of all, what should be our objective? Right? Look at this instruction schedule. If I want to do instruction scheduling and if I want to do global instruction scheduling, what should be my objective? My objective is that overall program execution time should be minimized, but then which is what is overall program execution time? Right? Am I going to follow the then branch all the time or am I going to follow the else branch all the time or am I going to follow either one of them and I do not know what is really going to happen. Correct? Because if I let's say optimize on one side, right, and if the program takes the other part, then I am going to be penalized, right, or if I optimize on the other side and it takes this part, then also it is not a good thing, right? So, what we want to do is that we want to make sure that the overall program execution time is minimized. So, typically when you try to do things like beyond basic block scheduling, you collect information like profile information. That profile information is often helpful for you to understand more often which path is going to be taken. Supposing I know that in this particular program, the path B1, B2, B4 is going to be more often taken than the path B1, B3, B4. Let's say this path is taken 80 percent of the time and this path is taken 20 percent of the time. Then I have an additional information which says that you can try to optimize your code in such a way that the overall execution time is minimized assuming that B1, B2, B4 is going to be taken 80 percent of the time. That means that you will try to do your optimization such that the length of this path is minimized. And when you try to do that, you wouldn't even mind if the length of the other path is slightly increased, right? That's really what you try to do. So the idea, yeah. Right, right. Minimizing the... Okay. So the trick is the following, right? So if you want to... So let's say there are three possibilities. Okay. Let me just try to work out. Okay. So one possibility is what I want to do. Right? Let's say this path is taken 80 percent of the time. This path is taken 20 percent of the time. Now you're trying to say that, look, instead of trying to minimize the frequently taken path, let's try to minimize the max of the both. Okay. So let's say that you want to minimize the maximum of the two paths. So if you want to minimize the frequently taken path, let's try to minimize the max of the both. Sure, sure. Okay. If the profiling information is not reliable, then what you say would be useful. Correct. Correct. But then just want you to understand that doing the maximum of the minimum... Sorry, minimum of the maximum, right, is not always a good idea. Okay. So what do you want to optimize? Tell me again. You want to? Minimize the maximum... Minimize the max of path P1, P2. This is what you want to optimize. Correct? Now let us say that if I... When I do this, correct, it so happens that both sides turn out to be 88. Correct? Whereas what I could have done is that I could have done this side with five and this side with eight or this side with even why eight? Nine. Worse than what you have done. Correct? Then as long as it is going to kind of follow this 80-20 rule, I would say that you should follow this 80-20 rule, I would still be faster than you. Right? Yes, what you say is possible and those are all things that can be looked at, but there are advantages as well as disadvantages. Right? Okay. Now let's see... Okay. So our idea is to reduce the overall execution time of the program even if it means that you are going to have a longer execution time. Right? And you may ask why one of the paths would have a longer execution time. Because now you are going to move instructions beyond the basic block, you are going to go across the boundaries. Right? So when I move some instructions across the boundaries, it may cause certain side effects. And I may have to undo that side effect on the other side or I may have to recompute something on the other side. That might increase the number of instructions on the other side. So that is why it would increase. Okay? And whenever you have this kind of control flow graph, not all paths are equally likely. Some paths are going to be taken more often than the other. And if such a situation exists, then doing this profile-based approach always helps. If both paths are equally likely, then of course you have to choose a different strategy for optimizing it. Right? So the first scheduling that we are going to discuss is the trace scheduling. And in the trace scheduling, essentially you are going to rely on this profile-based information. And using this profile-based information, you are going to optimize a path which is more often taken, which is what is going to be called as the main trace. And you are going to optimize that path. The other path you will actually not worry so much about it. Okay? So the idea is to decrease the execution time of the path which is more often taken, which is what we are going to call as the main trace. Okay? Thus the decision should favor more frequently executed path. And here it is called the main trace. Okay? So let's see how this works. We will again take the same program. Right? I will take the same profile information that we have. So what I know is that this path is more often taken than the other path. So this path is what we are going to call as the main trace. The other one is the off trace. So now what I will do is that the instructions which are in basic block B1, B2, B4, I will allow them to move anywhere. Right? For example, if I allow this load to move to this B1, remember that it is writing into R3. So if the other basic block is using R3, then I would have a wrong value. Correct? But in this case fortunately it is not. So it is still okay to move this load to ahead of this. Because you remember, I mean, recall that this load is loading a value into R3 register and that R3 is not live in this basic block. Correct? R3 is not being used in basic block B3. Therefore it is okay to move this load instruction ahead of this control flow. You are not necessarily creating any side effect. Okay? Now what about this add instruction? Can I move that also? I can. Because here, right, in B3, R4 is first defined. Okay? So that is also okay. It is also not live in, in basic block B3. Right? So it is possible for me to in fact move both of these operations, right, beyond basic block B2 into B1. And when I try to do that, okay, when I try to do that, I may be able to reduce the overall schedule length of this. Similarly I can possibly move some instructions from B2 to B4 also. Or some instructions from B4 to B2. But when I move some instructions from B4 to B2, I have to be careful where I should jump from B3. Correct? Because these instructions have to be executed when you come through the path B3. So those things need to be taken care of when you try to do the beyond basic block scheduling. Again, going back, the trace scheduling, what it does is that it first identifies what is the main path, which is the most frequently executed path, and tries to do instruction movement across the basic blocks in the main path. Okay? Right? So there are conditional branches and transitions from other, okay. So when you form this trace, what you can see is that, right, the trace has three basic blocks, right? And you can see that from the middle of that base, middle of that trace, you can actually exit to some other basic block. And similarly into the middle of a basic, into the middle of the trace, you can also jump in. Right? If I put the sequence of instructions from B1, B2, B4 together, then you can see that there is an exit from here, from the middle of the block, and there is an entry into the middle of the trace. Right? So when we talk about a trace, the trace is no longer a single entrance, single exit piece of code. It can have multiple exits. It can have side entrances also. Correct? That's really what we are trying to talk about here. Okay. These control flow transitions are ignored when you do the trace scheduling. But as you move operations above or below the basic block, if they create any side effect, for example, if they overwrite any live invariables, then you have to make sure that the semantics is preserved somehow or the other. Right? That is very important. Similarly, if you move from the merge to above, you have to make sure that that value is appropriately reflected in the other basic block as well. Okay? So that kind of, you know, thing has to be taken care of while you do the scheduling. Let's again look at how we can do this thing. Take the same example. Now you are only looking at the control flow graph. Now what we will do is we will take these two instructions and then try to move them to basic block B1. Okay? As I mentioned earlier, instruction I3 is writing into R3. Unfortunately for us, R3 is not live in this basic block. So moving here will not really affect the semantics of this program. The control flow graph, nothing really gets affected. Similarly, R4 is not live in here. R4 is being defined. So I may do the add and then immediately I will overwrite by the move instruction. That's okay because if I am coming on this path, I need to have R4 as my value when I come here. Right? So again, understand that the R4 value that you are using here, right? The R4 value that you are using here, right? Should be the R4 defined here if you are coming from this path. Right? Whereas if you are coming from this path, then it should be this R4. Correct? So by moving this R4 here, it's not really affecting the semantics because if the control takes this path, R4 is again redefined. Right? And that value is what is going to be used here. So we saw that these two instructions do not have any side effects if they are moved to basic block B1. So they can be moved. This movement of instructions is actually not dependent on the profile information. Right? This is actually based on the control flow graph. Whether it goes through this path 80% of the time or it goes through this path 80% of the time, these instructions can be moved. But let's say I move these two instructions here and this control flow is such that this path is taken 80% of the time. Then what happens? I execute these two add instructions every time when I am coming through this path, which is actually redundant, which would have increased my execution time. Right? Semantically it is okay to move this. That has nothing to do with the profile information. But from a performance point of view, if I move these two instructions into this basic block, right, my performance will be hit if the profile says that this path is going to be taken 80% of the time. So that is why I am taking instructions from the most frequently executed basic block and I am trying to move them. I am not trying to move instructions from here. Right? You got that. Right? So these two instructions can be moved to basic block B1. Correct? I think that's all that we are going to do in this particular example. And then because you are going to put all of these basic blocks, I mean all of these instructions together in the trace, after instruction I5 is executed, you are going to execute instruction I9, this control instruction is not really required. I will show you where that is. Okay? And you will introduce one control instruction from here, which will actually go to the middle of this trace. I will show you that over here. Okay? So this is how the reordered instruction looks like. And this is the main trace that you can see. Right? So this is our main trace. You can see that the load instruction, which was over here, has been moved to the first basic block because there is no dependency between any of these instructions, the load instruction. It can be scheduled in parallel with the I1 instruction. Okay? Then there is a branch instruction and this dependency is being satisfied by scheduling this instruction over here. Right? So both, I mean, if you just go back here, this dependency between R3 to R3 and this dependency between this load to this branch instruction, both of them are satisfied with this one stall cycle in between. Right? And then of course you have executed the remaining instruction from basic block B2. Then you have instructions from basic block B4. Then after that you have instructions from basic block B3. Right? So now let's look at what happens to the control. You execute this instruction. And then if the branch condition is true, sorry, if the branch condition is true, that means that you are going to take the else branch, you will jump over here. You will not execute the remaining instructions. If the branch condition is false, of course you will execute the remaining instruction. And after you come to the else branch, that is when the branch condition is false, you will try to execute these instructions. But then after executing these two instructions which are from basic block B3, you have to go back to basic block B4, which is instruction I9. So you have a branch I9 instruction over here, which takes you back here. So that's really what it is. So we have moved these two instructions from basic block B2 to B1. And the main trace takes six cycles to execute. Okay? And this is the off trace, which is the path which is not most often taken. So from instruction B3, I mean instruction I, or time step three, you jump to instruction over here, execute these two instructions, go back, execute those two instructions, and then come out. Right? Remember you are organizing this instruction sequences of basic block B1, B2, B3, B4. Now you have done B1, B2, B4, and then B3. You have also reversed the order so that frequently executed instructions which form the trace, right, are together. So these are B4 blocks, right, from basic block B4. Any questions? Yeah. You can use that kind of an information for branches you know it is more often taken. And there are many other if statements also where you can say it is more often taken and less often not taken. Can you give me some examples of a simple if condition which you will take less often? So you write an if statement and you know that, right, it is unlikely to be taken or if it is taken something is bad. Right? You always put these assert statements, you always put, right, if something like this happens, right, then something bad has happened, I have to take care of that. Right? They are always often not taken. Right? Typically what you do is that when you want to do something like trace scheduling, you need to generate this profile information. So for that particular piece of code, you run it on a set of sample inputs and then see for these sample inputs, right, which path is more often taken and which path is less taken. Right? That that information you always generate. Go ahead. No. So you are asking whether I should only combine three or five or eight or something like that. You want to actually combine them as big as a thing possible, which allows you to do, which allows, which gives you enough opportunities for you to do the trace scheduling. Right? In other words, what I am saying is that you try to form your trace in such a way that the probability of the trace is high enough and you have enough instructions in the trace so that you could do instruction scheduling. Right? So, so see, remember I showed you a very simple example of one if then else. Right? Now let us think of two if then else, one below the other. Then how many paths do you have? Four. If you think of three, eight. Right? As you have a lot of paths, you have to think of three. Four. If you think of three, eight. Right? As you have more if then else together. Right? What is going to happen? The probability of going through any one path is going to become lower and lower. Correct? So you can see, for example, now the question is, should I take two if then else and then form my trace or should I take five if then else and then form my trace? Let us try to ask that question first. Correct? If I do two. Right? And let us say both of them has a good probability of certain paths, then by doing this probability calculation, you can find out yes, still you will have, let us say in both cases 80, 80, then still you will have one path which is 60 percent or 64 percent. Right? But if I go one more level, then one path which is 50 percent. If I go one more level, 40 percent. Right? It keeps decreasing like that. Right? So beyond certain point, you are going to find that more or less all paths are of equal thing. Then you do not have the notion of the frequently executed path. So you keep this notion as long as one path is more frequently executed than the other path or a set of paths is more frequently executed than the other path and you try to optimize. But if you keep putting things together and together, you are now going to have a large number of instructions, but you do not necessarily need that many instructions. Typically, if you have about 60 or 80 instructions, that is more than enough for you to do any instruction scheduling. Beyond that, you do not need. So, you keep collecting these basic blocks until you get, even 60 is actually very hard to get. Right? Typically, when these people do beyond basic block scheduling, they are still talking in the order of about 20 to 30 instructions and not more than that. Beyond that, I think your path probability is going to become very, very low. Right? That is really what you talk about. Right? Any other questions? Okay. So, as I mentioned earlier, when you are moving these instructions beyond the basic block, you have to make sure that they do not have any side effect. If they have a side effect, then you have to produce the appropriate code on the other side to ensure that it is satisfied. Okay? Now, I am not going to have an example for these side effects because I thought there are too many things to cover that. Okay? The last point here is that we mentioned that the main trace takes six cycles and the off trace takes five cycles, seven cycles in this particular example. Okay? Now, again, we will go back to the same point that this load has been moved, okay, ahead of this branch and we said that it does not have a side effect in terms of R4 being, R3 being live in this basic block. Okay? Now, that is really what we are talking about. When you want to move the load instruction ahead of the conditional branch, you have to make sure that the destination register R3 should not be live in the other basic block, off trace block. Okay? So, if it is, then some additional copy code has to be inserted here which ensures that the old value of R3 is restored there. Okay? This is another interesting point. When you move these instructions across, you may also end up having these additional problems like exceptions. Supposing, let's say, I move this instruction and this load causes a page fault, right? Maybe this condition was if pointer not equal to null, correct? And here you are trying to dereference that pointer. Now, by speculatively moving this, what you have done? You have done what you are not supposed to have done, correct? So, when you do this kind of a speculative code motion, right, these kinds of problems could happen and you probably require some support from the architecture as well that for such speculative code motion, right, the exceptions need to be handled in a special way. Okay? Again, we will not get into the details of this in this lecture, right? But if those things are not there, then moving instructions like load across the conditional branches would be a problem. Okay? You have to understand that. Moving add is okay as long as the destination register is not live in. But again, you can say that add might cause, let's say, an arithmetic exception, right? You are trying to add or multiply or something, then let's say that some error has happened. Again, if an error happens, there is a speculative code motion, then there is a way by which it has to be handled. Okay? So, let's move on to another type of beyond basic block scheduling, which is what is called a super block scheduling. Okay? Now, what we saw in the, what we saw in trace scheduling is that trace is a block which consists of a sequence of instructions. It can have multiple entry points. It can have multiple exit points, right? So basically, the structure of a basic block which is single entrance, single exit, we have completely removed it and then said it can have multiple entrances and multiple exits. Okay? Because of this multiple entrances, yeah, not, yeah, more because of this multiple exits, it has to do a lot of bookkeeping work to do the, to keep the code correct. Okay? So, there was an alternative proposal which instead of saying having multiple entrances and multiple exits, allow only, right, single entrance and multiple exits, right? So, super block is again a sequence of basic blocks put together, okay, that has a single entry but multiple exits, right? Let's see how that works and because of that, okay, you are, I mean, certain bookkeeping activities which were required to be done in trace scheduling need not have to be done, okay? But again, formation of super blocks creates additional optimization opportunities and you form super blocks again using some kind of a profile information. Okay? Let's again take the same example, right? Let's take the same example and let's assume that this path is more often taken, right? This path is more often taken, right? If I want to form a super block involving B1, B2 and B4, correct, and this path is not taken, I can exit from B1 because multiple exits are allowed but I can't enter back into the super block, correct? So, what I need to do is because I can't enter into the super block, I will actually replicate this basic block B4 two times which is what is called tail duplication, right? So, here is how you do tail duplication, right? The same control flow graph, now B4 is copied into B4 prime, correct? And now this part forms super block and this is the other set of basic block. In fact, you could have formed one super block like this and another super block like this. That's also fine. In fact, what happened, sorry, not even super block. In fact, now what happens is because after B3 only B4 is being executed, these two will be merged as a single basic block, correct? These two will be merged as a single basic block. So, you have one super block here and then one basic block here and then in both cases you kind of enter back into this, right? So, this is essentially what is called tail duplication. Tail duplication means what? Again, it duplicates code. So, there is a cost that is involved in that, but then it makes certain things simpler, okay? Now, if I, once I form the super block, if I try to do the scheduling for this, this is exactly what will happen. Again, I have moved the load instruction from B1, sorry, from B2 to B1 and I have scheduled them exactly the same way that I have scheduled in my trace scheduling. Then there is my store instruction and now I can actually put the other instructions together. Remember earlier I was, I had to put ITEN separately because there was a side entrance which was coming in. Now, because there is no side entrance, I can even aggressively schedule ITEN one step earlier. So, overall this will only take five cycles, right? And then when I don't take the super block path, when I go for the after trace path, I jump from this branch instruction straight away to here and here I have put single basic block of all the instructions, right? And then again it branches back to instruction I1, okay? So, essentially it's a moving of instructions and you can see that there is a single, I mean, a single entry, but two exits, one exit from here, another exit from here, two exits, okay? And this is again for the other block from here you branch back to this one. The main trace takes five cycles, the super block. This is the instruction which was moved compared to the trace scheduling and the other one takes six, five. So, we were able to schedule things better. So, both trace scheduling and super block scheduling relies on the fact that some path is more often taken than the other path and you have information about it. Sometimes this information may not be available, right? Sometimes the information available tells that both paths are equally likely. Sometimes it may also be the case that, well, this basic block B2 and basic block B3, the then and else blocks, have exactly one-one instruction each, right? That is also possible. Very small basic blocks. A few instructions may not be one, maybe one or two or three or whatever it is, right? Can we do something in those cases? Can we come up with a better scheduling method in those cases, right? For that we are, okay, let's skip this slide because this talks about how to enlarge the super block. Again, you know about these techniques like loop unrolling and other things. So, the third global scheduling method that we are going to talk about is what is called hyper block scheduling. In hyper block scheduling, essentially, you do hyper block scheduling if you don't have the profile information or if the profile information takes both paths are equally likely or the profile, I mean, or the control flow graph is such that the then block and the else block have very few instructions. In order for you to do hyper block scheduling, you need to have certain hardware support. We'll talk about that, right? So, again, as I said, both branches are equally likely, okay? So, the kind of hardware support that you need for this is what is called predicated execution, right? So, how many of you know about predicated execution? One, two, three. How many of you have heard conditional move instruction? That is predicated execution, right? Conditional move instruction is predicated execution. I'll show you what it is in the next slide, okay? So, let's look at this particular example before we go to our bigger example, okay? So, here I have a branch equal instruction, right? And then an add instruction in the then block and a subtract instruction in the else block. And then, of course, a branch here to come back and then merge point. Again, simple control flow graph, okay? But the then branch and the else branch each have only one instruction, right? Now, if I have predicated execution, then what I do is that instead of doing a branch instruction here, I will set a predicate instruction, set predicate instruction. That will only set the predicate blocks. Then I will execute both the instructions in the then branch and the else branch with the respective predicates. And then I will come back and execute the merge instruction. So, let's see what that is, okay? So, instead of the branch instruction, I replace it by a set equal instruction, which sets the predicate P1. So, what are we testing? If you are testing R1 is equal to R2 branch to out 1. Instead of that, I say set predicate P1 if R1 is equal to R2. And if P1 is true, what should I do? I should execute the subtract instruction. If P1 is false, I should execute the add instruction. So, I am going to say do the add instruction if not of P1. Do the subtract instruction if P1. But in practice, what is going to happen is that this sequence of instructions or all of them are going to be executed, right? The meaning for if P1 here is not execute this instruction if P1 is true. It is actually execute this instruction, execute this instruction. If P1 is true, write the result value in R7. If not of P1 is true, write the result value in R4. In other words, in predicated execution, what you are doing is that you are performing the operation, whether you write the result in the destination or not depends on your predicate value. That is why it is called predicated execution, right? Okay? Again, let me repeat this. What we have done is that we have replaced the branch instruction with a set predicate instruction. That only sets the predicate. Then, the instructions from the then branch as well as the else branch are included, but with appropriate predicates. And what this predicate tells is that, okay, execute this instruction, write the result in the destination register if P1 is true. Execute this subtraction, execute this add instruction, write the value in R4 if predicate of P1 is false. That is what it says, right? The meaning of this is execute, do the write back stage only if the predicate is true. See, if your instruction fetch decode, execute, everything you will do. Only write back is done or not done depending on the predicate condition, right? If the predicate is false, then this essentially amounts to doing no op. You perform the operation, but you are not going to write the result in the destination location, right? Okay. Now, yes, Chris. Good. Okay. Somebody else can answer the question. Why would this be good? Okay. Before we see why would this be good, let us see what this has done, right? On the left-hand side, you have a control flow graph with four basic blocks, each having one or two instructions. What do you have on the right-hand side? What is, how many basic blocks are there on the right-hand side? One. Only one basic block, correct? In other words, what you have done is your control dependences are now turned into data dependences, right? And because you have removed those control dependences, you have actually made everything as a single basic block, correct? Now, assume that you are allowed to, you are actually doing this. You are not going to do this if you can only execute one instruction every cycle, right? Then it is not going to make sense. I have the capability to execute multiple instructions, right? And I have enough hardware resources and I want to do aggressive instruction scheduling. That is why I went beyond and then said, okay, even though you may not be executing both of this, I will still try to execute both as long as the overall execution time is minimized, correct? So, I will do this only if my overall execution time is minimized, correct? This is not instruction schedule. These are the sequences of instruction. Same function is also, oh, yes, different function unit, yeah, yeah. But what if I have two integer function unit, right? So, you will do that only if it is profitable to you, not otherwise, okay? Always make sure that we will do this only if it is profitable, correct? But then, yes, there are, you go for all of these things because you have more functional units and you have fewer instructions to fill them. You want to reduce the stalls, et cetera, et cetera, okay? That's the reason. Again, we will show that in the context of our hyper block scheduling how it helps, right? Okay. So, again, back to our example. Now, let's assume that both of these branches are executed more or less the same time. Here I have assumed 65, 35, but you could have put any other value also, correct? Now, we can say that you set the guard appropriately and this set of instructions, they are all executed. The result values are written in the destination register if the guard is true. Similarly, on this side, these instructions are executed, but the result value is set in the destination register if the guard is false, right? Sorry, again, whatever is the guard, the guard is true. I shouldn't say guard is false, right? P1 is false, but the guard is true. Of course, this branch instruction can be eliminated because now what you are going to do is that you are going to treat this whole thing as a basic block, right? The moment you try to do predicated execution for this, right, these are going to be predicated by P1, these are going to be predicated by not of P1, then all of these so-called how many instructions? So, 4 plus 4, 8 plus 3, 11 instructions all put together will be a single basic block. Of course, this might be knocked off because it's not required, right? So, let's see what happens, okay? So, we have the load instruction, right? And here, of course, I have speculatively moved the load instruction, and here is where I am setting the predicate, and then these instructions are executed, okay, based on the predicate. You can see I5, I8, I7, and yeah, those are the instructions which are executed based on the predicate, okay? Of course, these are the instructions from the merge point, so they have to be executed irrespective of whether P1 is true or false. But now, I have scheduled them such that both paths take exactly on the 6th cycle, right? In the earlier case, it was 5 and 7 or 5 and 6 or whatever it is. Now, I have done this, and of course, it's also the case that we do not know which path is going to be more authentic, right? So, this is another mechanism by which you can do instruction scheduling beyond basic block, okay? Now, we will also talk a little bit about the space ordering problem which we have been referring to yesterday when we talked about register allocation, right? So, when you try to do instruction scheduling, instruction scheduling aggressively tries to move instructions so that your stalls are kind of taken care of and instructions are scheduled in parallel, right? And as you move these instructions up and down, you are essentially increasing or decreasing the live ranges, right? And if register allocation is going to be done subsequently, then what happens is that it might have so happened that you would have stretched some of the live ranges. And because you have stretched some of these live ranges, okay, you will have more conflict in your register allocation. That's possible, right? But if you first try to do register allocation and then do instruction scheduling, if you do register allocation, then all registers have, I mean all instructions have their registers already assigned, right? And because remember we saw an example yesterday, right? Because you are trying to reuse the same register, you would have created anti- and output dependencies. Now anti- and output dependencies prevent you to move your instructions beyond those points. So, you will have less opportunities when you do instruction scheduling. So, this is the conflict or interaction that happens between register allocation and instruction scheduling, right? More often than not, you typically do instruction scheduling first and then register allocation, right? This is typically what is called pre-pass scheduling, okay? And when you do pre-pass scheduling, instruction scheduling is done first. It may increase the register pressure and therefore your register allocator might end up doing more space, right? On the other hand, if you do post-pass scheduling, right? Register allocation is done first, right? It will introduce anti- and output dependencies and that will limit the opportunities for instruction scheduling. One other thing is that when you do this instruction scheduling first, right? Now you have introduced some spill code. The spill code also needs to be scheduled, correct? So, you will do one other pass subsequently of instruction scheduling just to take care of that. Only those spilled instructions should be rescheduled, okay? There are also proposals which kind of integrate this register allocation and instruction scheduling. Trying to do both of these things somewhat together. It is possible to do that and there are several proposals which try to do that as well. Okay, let me give you an example of how these things kind of interfere with each other in this particular case, right? So, let's look at the program on the left-hand side, right? Actually, it should have been better if you have come from the right-hand side to the left-hand side, but let's, okay? So, let's say if I do instruction scheduling first and I have achieved this schedule, right? Now you can find out that since this is a basic block, you can say that, right? It's like register requirement is five, right? Its register requirement is five. But if I have done register allocation first and moved instructions around a little bit, okay? I could have got this and then subsequently if I have done instruction scheduling, I could have got this schedule. And in this schedule, I will only require four registers, right? So, again it depends on, you know, how these things work out and there is nothing which can say that this is better than the other. In some applications, it may turn out to be, you know, doing instruction scheduling earlier is better. In certain other applications or programs, it might so happen that doing register allocation first would be better than the, better than doing instruction scheduling first. But in general, people have found doing instruction scheduling first as a more appropriate option, okay? Now we are going to move on to the next topic, which is software pipelining, right? Software pipelining is again another instruction scheduling technique. It's an instruction scheduling technique for loops, right? And for now, we will actually assume that the loop body is a single basic block, right? So, we are essentially going to have a control flow graph which is like this. I have a control which is coming in, loop body, which I execute. I iterate several times and then I come out. Now, if I do only basic block scheduling, forget for the time being about the other global instruction scheduling that we talked about, they are not relevant here because if you try to look at it, right, you have only one basic block here and that's going to be repeatedly executed. You try to move these instructions outside of this basic block, you are going outside of the loop. So, that is not going to make sense here. Global instruction scheduling like trace scheduling or, you know, super block scheduling isn't going to help if you have a single basic block, right? We will come to some more complicated examples if time permits a little later, but for the time being, we will only worry about single basic block, right? So, none of the techniques that we have talked about earlier is going to help. Anything that we can do is only basic block scheduling. But basic block scheduling has its own limitations. So, let us see what we can do, right, and how we can improve this. Again, we will look at it from an example point of view, right? So, this code is familiar. Something similar to it is what you have been doing in your lab assignments, correct? A of I is equal to A of I plus S and I want to do it n times, okay? So, this is the assembly code for this. This is actually the MIPS or 10,000 assembly code or MIPS assembly sequence, right? You have the load, add, and so on. Again, we saw the same example even yesterday, right? The same set of instructions, right? Now, this is the data dependency graph for this example. Let us try to understand the data dependency graph a little bit here, right? So, again as before, nodes represent instructions and arcs between nodes represent dependencies. Those parts are very simple, right? So, you can see that there is a dependency from the load instruction to the add instruction because F naught, which is, right, stored by the, I mean, which is the result destination of load is what is being used by the add instruction. Similarly, there is a dependency from the add instruction to the store instruction and so on. Now, let us look at this add instruction. What does this add instruction do? This add instruction is essentially incrementing the pointer. And the same add instruction is being used in the, sorry, the same R1 register is being used in the subsequent load instruction, right, as the pointer. That means that there is a dependency from this add instruction to the load instruction in the next iteration, correct? That dependency is actually what is being shown here by this green arc. And this circle here, dot here, essentially says that this is a loop carried dependency. That means that the value produced by the ith instruction, ith iteration is actually going to be used in the i plus one iteration. We formally define loop carried dependencies subsequently today, but for the time being, this definition is good enough, right? Similarly, if you look at the add instruction, it is dependent on itself, right? The value produced in the previous iteration is what is going to be used in the current iteration. The same is also true for the subtract instruction, right? I think yes, okay. And also for the store instruction, remember, right? Add to store, there is a dependency because store also uses the same pointer, right? So, after I increment the pointer, the incremented pointer value is going to be used by the load instruction. So, after I increment the pointer, the incremented pointer value is going to be used by the load instruction, store instruction, and also the add instruction in the next iteration. So, all of these dependencies that are there in the green are loop carried dependencies, right? Similarly, there is a loop carried dependency for subtract. This dependency, all of you understand, correct? Any questions? Of course, we will again put the latency values, associated latency values with the nodes, and I will only write the values which are greater than one, right? If I do not write anything, it is equal to, okay? Any questions? This is a data dependency graph. When we talk, yeah, you have a question, yeah, sorry, yeah. Loop carried dependency from add to, why is it there, is it? What does the add instruction do? No, no, in this particular case, what is it trying to do? It adds the value of r1 with 8 and puts it back into r1. So, the value produced by this add instruction in the ith iteration is what is going to be used by the same add instruction in the i plus 1th iteration, isn't it? Any other questions? Okay, so when we talk about software pipelining, we always talk about this instruction sequence and constructing the data dependency graph. And because this is a basic block which is going to be repeatedly executed, you are going, this is a basic block inside a loop, you are essentially going to see some amount of loop carried dependency. It is always going to be that, okay? In this case, okay, you can also see that there is a cycle in the graph, right? For example, you can see that there is a cycle like this. This is a self-loop, right? There are no other cycles other than these two self-loops. But in some other case, there could be a cycle involving multiple instructions or operations. Okay, now if you try to do basic block scheduling for this loop, assuming again one integer function unit, one floating point function unit, and one load source function unit, right? Again, the latencies that we are assuming are similar to what we have assumed yesterday, two and three cycles, right? Then this is the schedule that you produce, right? There is a load instruction, the dependent add instruction is delayed by one cycle, the dependent store instruction is delayed by two cycles, there is a subtract, and there is an add, and there is a branch, right? Now, because it is a basic block instruction scheduling, right? The next iteration can only happen after the first iteration is over, because I cannot move any of these instructions beyond this boundary. So, in this case, each iteration takes six cycles to complete, right? Even though I have many functional units, and I can issue multiple instructions together, I still take six cycles, which is actually six instructions all put together, correct? That's really what I was able to do. Okay, now we'll do the known technique. Supposing I unroll the loop twice, right? And then try to do the scheduling, what would happen, right? This is the unrolled version, again you can see two load operations, two add operations, two store operations, and so on. But these control operations like this integer add and subtract, and branch, they were executed only once, and these are appropriately adjusted, like what we have seen earlier, right? Yesterday we saw five times unrolled version, this is a two times unrolled version, right? Now, if you schedule this sequence of instructions for the same architecture, then this is what you're going to get, right? Again, the two load instructions, the floating point add instructions, which are dependent on them, the dependencies are satisfied, then the store instructions, right? So, this essentially gives you that the basic block scheduling of the unrolled loop is seven cycles. But now it is seven cycles for two iterations or 3.5 cycles per iteration. So, from six cycles we have actually come down to a 3.5, right? If we have unrolled it more number of times, would it have helped? Unroll it three times. What would it be? Somebody can quickly do the calculation. If I unroll it three times, then it would have taken nine cycles for three iterations, that is three. Unroll it four times. It would take two more, right? It would take ten cycles. Why ten cycles? If I unroll it four times, right, there will be four loads and four loads, four adds and four stores. Will it be ten cycles or will it be eight cycles? I cannot do it in eight cycles, right? Can I? Okay, you can try that. You can try to see whether you can do it in eight cycles, right, or whether you take nine cycles to do that or ten cycles. Okay, but I will show you a method where I can take two cycles per iteration on an average, right? So, this is essentially how you do unrolled loop, basic block scheduling and still you can achieve something, right? Essentially when you do basic block scheduling, you are limited by the fact that, right, the iterations of one loop is not overlapped with anything. So, here the iterations of the unrolled loop, right, is not overlapping with anything else. Now, let us take the same example, right, without unrolling, what I will do is that I will allow these instructions to overlap across iterations. That means that, as I mentioned earlier, software pipelining is an instruction scheduling for loops. It is actually a scheduling which is beyond basic blocks. That means that you are going to actually allow movement of instructions beyond basic blocks, right? Where is the basic block boundary? Here is where the basic block boundary is. Now, let me put the next iteration two cycles later, right? Again, I am satisfying the dependences. Okay, I am satisfying the dependences that are there. And I am making sure all my resource constraints are also satisfied. Okay, there is still I have to worry about this add instruction to load instruction dependency. I will somehow take care of that. We know how to take care of that. By essentially using appropriate offsets, we can actually relax that dependency. But other than that, you are okay, right? And so, I can introduce one more iteration, right, in the overlapped manner, right? People follow, right? So, what I have done is that this is the schedule for the first iteration. The schedule for the next iteration, I do not wait for the first iteration to be complete. I kind of overlap with it. And I overlap it after one cycle. That means starting from cycle two, I kind of overlap. And I do a similar thing for the third iteration as well. Now, look at the instructions which are in cycle four and cycle five, right? That has all the instructions in the iteration. I have one load instruction, one add instruction, one store instruction, one integer add, one integer subtract, and the branch naughty, right? In other words, what this has come is that this is essentially the sequence of operations that need to be performed within a loop, right? But I am performing, let us say, the load for the ith iteration, the add for the i minus oneth iteration, and the store for the i minus second iteration, right? Instead of performing the load for a of i, sorry, I am performing the load of a of i, the add for a of i minus one, and the store of a of i minus two. That is really what is happening here, right? Now, if I keep repeating that n minus two times, then I would have done all the iteration, because for the first two iterations, I have done the load here, right? I will do one add before this and one add after this, right? And similarly, I will do two stores after this, and I will remove this unnecessary instruction that are not required, okay? So, my software pipeline kernel is essentially this, okay, with those instructions as the prologue and these instructions as the epilogue, correct? We will come to how exactly to do this and then see this with all the register values, etc., etc., in the next few minutes, right? So, you are trying to unroll the loop and write a schedule, you are able to get certain things, but now what I have done is without unrolling the loop, without having multiple load instructions, right? I have only one set of instructions within the loop, right? I was able to get the same thing. This is essentially what is software pipeline schedule, right? Okay, so here we are able to execute each iteration in two cycles, and we were overlapping instructions from different iterations in this kernel, and this part is called the prologue and this part is called epilogue. Unwanted instructions are kind of removed from here, right? But these add instructions, this add instruction is required because this is the remaining add that you are supposed to do, and these store instructions are required. So, you can add again depending on whether the value of R1 is live after the loop or not, right? They may be required. So, the only thing that we are worried about is how do we adjust the index register appropriately. We will worry about that as we go back. Okay, so here is the same code, okay, with all the register values and other things shown. So, you can verify whether it is right or wrong, okay, and if there is some problem, you can adjust it appropriately, right? So, what you can see here is that I have taken a different set of min, that I started off from four here, I have started off from three or something, but other than that it is the same. So, you can look at these two instructions and you will see the same, right? Okay. So, another way of viewing this is as follows, right? Let us again look at it without looking at the resources because earlier we looked at it in this form, you know, each one corresponds to one resource and what operation is going in that resource. So, the time being let us not worry about the resource and then put all operations in iteration 0, one below the other. Iteration 1 was started two cycles later and iteration 2 was started two cycles later. And we have formed the software pipeline kernel, right? Now, as you keep executing this, eventually what this means is that you are essentially executing the ith iteration of load, i minus 1th iteration of floating point add and i minus second iteration of store in the software pipeline kernel, right? That is essentially what you are trying to execute. Similarly, the subtract is for the i minus 1th iteration and the add is for the i minus second iteration. So, the load and store, the load and store registers have to use the appropriate index values so that they are not, I mean, so that they are correct, okay? So, to summarize, okay, if you are doing basic block scheduling, this is essentially what you are doing, right? Each iteration is scheduled. Maybe there is some point until which the parallelism picks up and then after that it ramps down. Similarly, ramp up and ramp down because you have only so many instructions in the basic block. You have to first generate those values, use those values and then the iteration has to end. So, it is typically having a ramp up and a ramp down phase. When you unroll the loop and schedule, you are essentially avoiding this in between ramp up and ramp down. If I am unrolling it two or two times, then this can be kind of avoided because I have now twice as many instructions. I can reschedule and I will have fewer ramp ups and ramp downs. If I am doing software pipelining, this is what I am doing, right? I ramp up once and then I have a continuous stream of parallelism and then I ramp down in the end. Pictorially, this is what it means, right? So, typically compilers would try to do software pipelining, especially for VLIW processors, where the instruction level parallelism has to be exposed by the compiler, right? Good. So, now this is all fine. We saw these examples, right? And we could have just finished with those examples and then said that is a summary slide that nicely summarizes everything. We can go home. But we are compiler writers, right? Which means that we have to generate those software pipeline schedules. How do we do that, right? Let us try to talk about that in the next 20-25 minutes, right? Again, when you do instruction scheduling, the instruction scheduling has to satisfy all the dependences, okay? And it has to satisfy all the resource constraints. Now, so dependences from one iteration to a later iteration is also possible, which is what we called as the loop carry dependences that also needs to be satisfied, okay? We need to kind of identify this repetitive pattern so that kernel can be executed, okay? When you have this software pipeline schedule, we have an initial phase which is what we call as the prologue and then we have a final phase which is what we call as the epilogue. This in between phase where we are repeatedly executing this instruction, which is actually the loop part is what we are going to call as the kernel. And the length of the kernel essentially tells you your throughput of the software pipeline schedule. In this case, it has a, I mean it takes two cycles to execute one iteration, right? So, this is what you are going to call as the initiation interval, right? Initiation interval because that is the delay between each iteration, right? Every iteration starts after I-I cycles or initiation interval cycles or two cycles in this particular example, right? So, keep that in mind because that is important for the throughput. If I have started these two iterations, let us say four cycles apart, then my initiation interval is four. If I have started it one cycle apart, my initiation interval is one. Smaller the value, higher is my throughput. So, the problem here is that we want to construct the software pipeline schedule. That is given a weighted dependence graph. We want to derive a legal schedule which is time optimal. That means has minimum initiation interval. And that schedule also should obey the resource constraint for the machine. Again, this problem is an NP-complete problem. So, we will only use some heuristics to derive this, right? So, the schedule is said to be legal if it satisfies the resource constraint and the dependence constraint, including loop carry dependences. Now, again what is time optimal? We say it is time optimal if there is no other schedule which is faster than that. And in the case of a software pipeline schedule, right, the speed of that schedule or the execution time of the schedule is defined by its initiation interval. So, a schedule is said to be time optimal if its initiation interval is lower than or equal to any other legal schedule for that loop, right? Because that is the lowest possible that you can achieve, okay? So, now let us find out before we construct the software pipeline schedule, let us find out that how do we identify what should be the initiation interval? Somehow in our examples by magic, we took the value 2 and it worked out. Why did not we try 1? Why did not we try 3? 3 of course is not interesting because 2 itself gave a good answer. So, there is no point trying 3, right? But what would have happened if you have used a value of 1? Would we be able to do a schedule for that? We should try that, right? But let us say in general for a given loop or for a given DDG, what limits the initiation interval, okay? There are two things. One is the data dependence. Another is the resource constraint. So, the data dependence limits the initiation interval and we are going to call that value as the recurrence MII. So, this is because this loop carry dependences are there in the loop. They are going to limit it in some sense. Similarly, because you have limited number of resources, there is going to be a recurrence minimum value which is called recurrence MII, right? That is also going to limit the initiation interval. We will give you an example of these two things as we go by, right? Of course, the minimum initiation interval that you can have should be the maximum of these two things, right? Whichever one is higher, that is the one that you are going to take. And any initiation interval, legal initiation interval that you have for the loop has to be greater than or equal to MII, okay? All right. If I i is equal to MII or if you find an I i such that there is no other schedule which has initiation interval lower than that I i, then it is time-optimized, okay? Now, there are again two approaches for doing software pipelining. One is called the operational approach and the other one is called the periodic scheduling approach. We will see both of them. Again, there are many methods which follow either one of these approaches, okay? Let me try to go to, let us just talk about recurrence MII first and then say why it matters, right? So, I will give you an example because this example that you see in the next slide may not be a good example. So, let us look at a simple graph, something like this, right? And let us assume all the edges have a weight of one, okay? And this is a loop carried dependency. So, basically I have three operations A, B, C, right? B is dependent on A, C is dependent on B, right? And A is dependent on C, but from the previous iteration, correct? So, supposing I have a situation like this, then can you tell me how fast can I schedule these operations for this loop if you have infinite resources? That is the question, right? Each one of these operations take one cycle, right? How fast can you schedule these three operations? Even if you can overlap whatever extent that is possible, you have unlimited resources, etc., etc. So, let us see. First I schedule A. I have to schedule B only after A finishes the execution. Similarly, I have to schedule C only after A finishes the execution. Now, what about the next A? Could I have started it immediately after the first A or along with the first A? Could I have started the second iteration of A at cycle one, cycle two or cycle three? No. When is the earliest that I can start that? After C, correct? Then B, then C, right? So, now what is happening? You have a minimum interval of three. Why was that happening? Because of this dependency, this dependent loop, correct? So, no matter how many resources you have, if there is a loop, right? And that loop has a single loop carried dependency, then you will not be able to start this earlier than that, right? Now, right? Okay. It may be a little difficult for you to work it out, but let us see whether this works out. If I have a loop carried dependency of two, what does that mean? It essentially means that A of i is dependent on C of i minus, sorry, C of i minus two, correct? Not C of i minus one, but C of i minus two. If it is loop carried dependency of one, it is, right? If the loop carried dependency is equal to one, then it is A of i is equal to C of i minus one. Whereas, if the dependency is two, it is i minus two. Now, if I have C of i minus two, then what happens to my schedule? I can do A, I can do B, I can do C. Now, what about my next A? I could have started anywhere because A of i plus two only depends on this C. A of i plus one does not depend on this C. So, let us say if the previous C was here, correct? Then I could have started my next A right here itself, right? Then I could have started it in any of these places, okay? Pictorially showing this is going to be difficult, but you will see that this value is going to be something like three by two, which is one and a half, okay? If you had four operations or let us say if this has a latency of two cycles, then what happens is that you will have two such things being scheduled. So, what happens is that in four cycles you have two iterations. That means that i i is two, right? So, that is essentially what this particular thing is saying. That whenever you have a cycle, sum up all the values of the latencies of all the nodes and divide it by the number of loop carry dependencies across all of the arcs. So, let us now go to what is called the minimum recurrence value, recurrence i i value, M R i, minimum recurrence, recurrence M i i, right? Recurrence minimum initiation interval, okay? So, okay. So, that is recurrence M i i. Resource M i i is calculated depending on the number of operations that you have and how many of them are pipelined and how many of them are non-pipelined and so on, okay? Again in this example, recurrence M i i is calculated. You only have self loops and because all of these self loops are one cycle long, correct? Your recurrence M i i is one, right? Okay. So, this is as far as recurrence M i i is concerned. Let us see how resource M i i is calculated. Resource M i i is calculated based on the fact that how many operations you have in the loop and how many functional units you have, right? Supposing let us say I have three operations and only one function unit, then in each one of those cycles I have to execute those three operations. So, my minimum resource M i i will be 3 divided by 1, right? Let us say if I have six operations and let us say I have three functional units, then I could have scheduled it in two cycles. So, resource M i i essentially tells you what is your minimum i i from a resource perspective, whereas recurrence M i i told you what should be the minimum value from a recurrence or a loop perspective. So, these are the two values which are going to kind of determine what your i i is going to be. Let us not worry too much about this non-pipelined function unit because it is a little bit more complicated. So, again I calculate the resource M i i for different functional units in my program. Remember that there is one load and one store operation and there is a single load store function unit. So, 2 divided by 1 is 2. Similarly, there is one floating point operation, one functional unit. So, it is 1, whereas for integer there are two operations and one function unit are for 2 and for branch there is one operation and one function unit it is 1. So, the minimum resource M i i is the minimum across all of these functional units and sorry it is a maximum across all of these functional units and that is essentially 2. And your minimum i i is essentially the maximum of recurrence M i i and resource M i i which is also 2. So, that is why you are able to construct a schedule with a initiation interval of 2. If you have tried with an initiation interval of 1, you would not have been able to schedule it. Why? Because you have only one of each of these functional units, right? But you had 6 operations. Obviously, you could not schedule all of them, right? So, that essentially tells you how to set your minimum initiation interval, okay? Now, in order for you to construct a schedule, what you need to do is that you will start off with a value of i i which is equal to minimum initiation interval. And then try to obtain a schedule for that particular i i. If it works out for you, good, then you got the schedule. If not, if you are not able to do the schedule, then you will increment your i i and keep trying, okay, right? In the first approach, what we do is that we have the operational approach where we try to emulate the loop execution under the machine model and then try to see whether a pattern emerges, okay? This is a kind of a slightly difficult approach. I keep trying to schedule these operations as and when they can be scheduled and then kind of see whether for certain overlaps, whether it gives me a pattern, okay? This was tried very early in the late 80s, but there is a more efficient approach which is what we call as the periodic scheduling approach. And we will try to understand this approach better, okay? What happens in a periodic scheduling approach is that remember that in our schedule, right, every operation is scheduled in some time step, correct? And then the same operation is scheduled in the next iteration after how many cycles? Let us say the load was scheduled in time step 3, the load for let us say iteration i was scheduled in time step 3. What about the same load instruction in times in iteration i plus 1? It will be after you remember the schedule, right? So, we had so you remember in our schedule, right, we had the load instruction, right? And this was our kernel. Let us say if this is t equal to 3 and this corresponds to iteration 2, correct? Then we can say that the load for the second iteration is scheduled at time step 3. My question is what would be the time step in which load for iteration 3 is scheduled? 5, right? Because it is going to be scheduled after two intervals. So, it is going to be 5. The next iteration is going to be 7 and so on. So, if I know when this load, right, if the load, okay, for the 0th iteration, if it is scheduled at time step t naught, correct? The load for the i-th iteration is going to be scheduled at this, isn't it? Because 0th iteration is scheduled at t naught. First iteration will be at t naught plus ii. Second will be at t naught plus 2ii. Third will be at t naught plus 3ii and so on. This schedule form is what is called the periodic schedule form. And that is really what you are going to use, okay? Let us see that in the next example, in the same example, right? So, the schedule for the i-th iteration, right, the schedule for the i-th iteration of node v is going to be at time ii, which is initiation interval, multiplied by the iteration count plus the scheduled time of the 0th iteration. That is really what it is, okay? Now, you can see that the i plus 1th iteration and the i-th iteration are exactly ii cycles of what? That is fine. Now, in our examples, these are the scheduled times of initial iteration or iteration 0 for the various operations. And for example, if you are interested in calculating what is the scheduled time of the floating point add instruction in cycle 1, it is basically 2 times 1 plus whatever is the initial scheduled time plus 2 is equal to 4. Or if you want to find out what is the fourth iteration, something like that. So, we are going to make use of this particular, right, form of scheduling. That means that all operations, right, are going to be scheduled exactly, I mean all operations. Let me put it this way. Different instances of the same operation for successive iterations are going to be scheduled ii cycles of what, right? That is basically what we call as the periodic schedule form, okay? We will just do this slide and then probably take a break after that. So, what we are going to do is that initially we have to calculate mii. mii is the minimum initiation interval. This it turns depends on recurrence mii and resource mii, right? So, the maximum of recurrence mii and resource mii is what is your mii. So, you start off with that mii as your ii, right? And then you try to schedule the operations, right? And you schedule the operations assuming that it is in the periodic schedule form, right? If you are able to schedule the operation, right, you go to the next operation and then keep scheduling. If you are unable to schedule, then what you do is that you remove some already scheduled operations and then try to reschedule this operation. If you have tried this a number of times and you are still unsuccessful, then you go back, increment your ii and try to do the schedule. So, this is the overall picture of doing software pipelining. We will go into the exact way by which it is done in certain schedules after we come back from the break. Thank you.