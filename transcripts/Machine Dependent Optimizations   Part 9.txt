 So, let us now see the details of how exactly we do the schedule, right. So, to model the resource constraints in the software pipeline schedule, similar to the resource allocation table that we talked in the case of instruction scheduling, we have a reservation table which is called the modulo reservation table, right. And this reservation table has exactly, it has as many columns as the number of resources, but what would be the number of rows? The number of rows should be equal to the initiation interval, right, because you are only talking about a schedule of length ii, right. So, this modulo reservation table is of ii columns and f rows, where f is the number of resources that you have, right. And it essentially talks about whether a particular resource is being used in that particular step in the modulo modulo schedule. So, for example, for our example ii was 2, so we have basically 2 rows and we had 4 functional units, right. So, now when you do the scheduling, if an operation is scheduled at time step t, then t mod ii is a time step in which it is needed in the resource modulo reservation table, that is how you mark that thing, okay. So, when you do the modulo scheduling or software pipelining, you have to ensure that it satisfies the resource constraints, it also satisfies the dependence constraints, both of which we have to make sure it does. So, resource constraint is ensured by using the modulo reservation table. Let us see how that works. So, each entry in the table records a sequence of reservations, right. And you say if again we will take the example and then write it down that might be easier. So, for example, let us say the load instruction, right, was scheduled at time step 0, right. Then we will say that in the modulo reservation table load is scheduled at time step 0, right. Let us say if the store was scheduled at time step 5, right, store is also a load store operation, time step 5 is 5 mod 2 which is 1, right. And let us say the floating point add was scheduled at time step 2, then over there, right. Like this you put these operations in the modulo reservation table, right. Let us say the subtract was scheduled at time step 4, then you put it over here, right, the integer subtract. Now, what about the integer add, right, in our let us say if you want to schedule the integer add in time step 2, 4 or 6, is it possible? No, because the subtract is already there. So, you can only try in the other time step, right. If it can, so for example, if you are trying time step 4, it is not possible because subtract is already there. Then you will try time step 5 and then see whether it can be scheduled. If it can be scheduled, you put it there. If not, you go to the next time step, but that next time step is again time step 0. So, there is no point trying to do from t to, so you only need to do from t to t plus i i minus 1, no point going beyond t plus i, right, because it is a modulo reservation table. So, this is essentially what we need to understand in terms of resource constraint, correct, okay. So, again the heuristic approach uses the earliest start time and the latest start time, right, and try to use that as some kind of a priority. Try to schedule nodes which have less slack first and then nodes which have more slack later, okay. And as I mentioned earlier to schedule an operation, you have to see whether the resource is available. You use the modulo reservation table and then you check a time step t minus i i, sorry t mod i i, not t minus i i, okay. This is again a bin packing problem because you have i i cross f slots and you want to pack all your operations in that such that they satisfy the dependency constraints and it is NP complete, okay. So, here is roughly how the iterative modulo scheduler works. Let us try to look at it. So, you start from i i is equal to minimum i i and if you are not able to get a schedule in this i i, you will increase your i i by 1 and keep trying and you will keep trying until some maximum amount of things that is possible. After that you are going to say maybe it is not worth trying to do the software pipelining for this and then come out. So, you select an operation with a higher priority and then for that operation between its as soon as possible time to as late as possible time, you try to schedule it, correct. And in the time step in which you are trying to schedule, you want to essentially make sure that there is no resource conflict in the modulo reservation table. If there is no resource conflict in the modulo reservation table, then you try to add this operation to the schedule by marking those resources in the modulo reservation table, right. If your t is greater than your ALAP time, that means that you have only fixed this much from the start time, earlier start time to later start time, but if you have exceeded, then maybe something has bad has happened and you can try to fix this by doing some backtracking. So, what you are going to do is that I was unable to schedule it between my earliest start time to the latest start time. That is why my t is greater than latest start time. Then what I am going to do is that I will force it in the latest start time and in order for me to force it in the latest start time, I may have to eject an already scheduled operation from the schedule, right. So, in the modulo reservation table if you go back, right. So, for example, let us say I was trying to do a schedule, but let us say there was already an operation in that slot, maybe this floating point add. Then what I need to do is that I have to eject, right and then introduce the new operation here and then try to see if add can be scheduled in a different cycle. So, this is essentially the backtracking that we talk about, right. Eject conflicting operations from the MRT and then you compute the ASR PLAP time of the remaining operations and you kind of repeat this process, right. So, for every operation you have what is called the earliest start time and latest start time starting from the earliest start time, you try to schedule it at a particular time slot where there is no resource conflict, right. If you cannot find such a thing, then you try to force it at its last latest starting time and that would mean that some operations would get ejected. That is ok. We will try to schedule them again. Then you compute the earliest start time and latest start time of all operations, redo your priority and you keep iterating this. But it may so happen that one operation will eject the other, that operation will again eject back this one. If it happens for multiple times, then that is where your budget will exceed and once your budget exceeds, you come out of that you increase your I I and you try for the next I I, ok. So, this is how the software pipelining algorithm works. So, again for our example, right, let us see what happens. We found that our minimum I I is 2. So, let us start off with I I is equal to 2, right and let us try each one of these operations. So, first let us take the load operation and try to see whether we can schedule it at time step 0, right. It is possible to schedule because no resources being used at this point. Then let us take the add operation. The add operation will have an earliest start time which is 2 cycles after the load operation. Therefore, it will only be t equal to 2. Now, again if you try t equal to 2 at that particular slot, there is a resource which is freely available. So, add can also be started at t equal to 2. Then the next is store. Store can only start at t sorry, store can only start at t equal to 5, right and 5 is essentially time step 1 in the modulo reservation table. So, you can put it there. Now, what about the integer add and the integer subtract that times are given here and you can try to see that they are also not conflicting. Then lastly you have the branch instruction that is also not conflicting. You can put it in that, right. So, this is how you try to do software pipeline schedule, ok. So, what we can do is I have another 3 or 4 slides which is about register requirements in modulo scheduling, ok in software pipeline schedule. Maybe I will just skip this thing because it will be going into a lot of details, ok or maybe just I will tell you what the problem is and then we will come back and look at it, ok. So, let us now look at having scheduled these instructions like this in the software pipeline schedule. Let us see what is really happening with regard to the registers, right. Now, think of this as iteration i, i plus 1 and i plus 2, but a software pipeline schedule only has one of them, right. For our purpose we have actually written down successive iterations, but the schedule itself has only one iteration in the code. Now, the load instruction is going to produce a value which is going to be consumed by the floating point add instruction, but in our schedule, right if you look at it, right, ith iteration of the load, i minus 1th iteration of the add and i minus second iteration of the store are together in a loop, correct. So, when I unroll this 3 or 4 times then essentially this is what I get. This load corresponds to the ith iteration, this add corresponds to the ith iteration and this store corresponds to the ith iteration. I have marked them as load 1, add 1 and store 1, right. Similarly, yeah, this corresponds to the i plus 1th iteration, this corresponds to the i plus 1th iteration. Now, let us look at the register requirements and their live ranges, right. So, load produces a value which is being consumed by the add, right. Whereas, add produces a value which is being consumed by the store, right. Now, this is for the add instruction. The add instruction produces a value which is being used by the store instruction as the index, right, pointer. Therefore, its live range starts from here and goes all the way up to here. Now, this is for the subtract instruction. The subtract instruction produces a value, right, which is going to be used by the branch instruction, ok, the same thing being repeated in the next one. Now, what are we seeing, right? Let us look at specifically the register used by add 1, right. The register, let us say if f naught is the destination register for add 1, correct. Now, when is that being used? It is being used after three cycles in here, but before that I have one more add instruction which is trying to rewrite into this. In fact, this f naught register and this f naught register or this live range and this live range, they are conflicting, but we have the same register being used for that, right. So, my add instruction here, right, what are the register values for this? May be f naught f 2 f 4, correct. Remember that in the code there is only one version of the code, one iteration of the code. So, it uses the value f naught f 2 f 4. So, when I come back in the next iteration, I am trying to redefine that value, but this redefinition is happening before its use has happened. That means that before the store takes the value, I would have overwritten it. In some sense, it is like a war dependency, right. This is the read that should have happened and this write should only happen after that, but we have scheduled it so that now it is going to happen before. In other words, a single register is not enough for add. You need to know how two registers because it has two live ranges, this and this and they are overlapping, correct. So, you need more than one register for f naught itself. Similarly, if you look at the green one, you have three, right. There will be one more green here, right. So, there will be three that will be overlapping with each other, correct. So, this problem is essentially the register allocation problem in software pipelining, okay. Now, if you are looking at the rolled version, the compact version that is what is going to really be in the loop, this is what it is. As you can see that the floating point operation is going to use a register which is going to overlap with itself. The subtract is going to use a register which is going to overlap with itself. This means that this has to be allocated two registers and this has to be allocated three registers, right. So, again there are hardware support to do some of these things and in the absence of a software hardware support, you can also do it in software by unrolling the loop and doing necessary renaming of variables, okay. So, I think with that I will stop our discussion on register requirement for modular scheduling or software pipelining because otherwise it gets too complicated. But if you have any quick questions, maybe we can take it at this point in time. Any questions on software pipelining or otherwise we are done with this, okay. This actually shows you the details of how to do the renaming and things like that. I will give you the slides. You can go through them, right. You do not want to go. This is again a taxonomy of all software pipeline schedule. . More registers? More registers. Okay. Then? Again you are thinking in terms of adding the spill instructions and stores, spill loads and stores, but those spill loads and stores also need to be scheduled in this corner. . Can all of them registers are dependent? Sorry. . Okay. . Okay. . So, essentially from whatever you are saying what I understand is that see there is a situation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Making it hard for me. . . This is what you got the register classes, is it? . Okay. Sorry, let us just try to write it down. S 1 is equal to. S 1 by S 2. Ok and you have only one register. I mean if you have to use both of them from register and you have only one register, obviously the implementation is impossible. Should not arise. If it arises you cannot generate code, right. See essentially what you are saying is that you only have op register register of a situation, right. And you say that I have only one register, then code generation itself is not possible, correct. I do not know whether it is a corner case or this is an impossible situation, ok. So, see your question is well understood that when I have multiple live ranges which are overlapping and then I have a situation I do not have enough registers. Then one of those live ranges have to be spilt, right. So, maybe this live range is spilt between this point to this point, so that you will only have exactly three live ranges which are overlapping, right. And in any instruction you are going to have at most two operations. So, before that operation you can always spill all other things and you need to have at least that many number of registers to do things. Because it is very in the life you can have tried with the spilt with the number of registers. Ok then what happened? So, it is kind of like number of registers out of the range. Absolutely, right. So, code generation cannot proceed. Further. Yeah further. So, the minimum is probably to and then beyond that it cannot do it, ok. Alright, so any other questions? For the register allocation, ok. So, you want to have what are called rotating registers, So, little bit complicated this is similar to what they have as register windows in spark architecture. So, I have to go into a lot of details that is why I want to skip this thing. Offline we can talk about it during lunch or something, right. So, again I have the details in the slide and maybe it will definitely have the keyword and if you do Google search you can get all the details that you want. Otherwise feel free to contact me, right, ok. Alright, ok. Now, we talked about code generation, we talked about instruction scheduling, we talked about software pipelining, we talked about register allocation. The last component that we wanted to cover is something which is relating to again certain machine dependent optimizations which pertains to memory hierarchy and which pertains to parallelism, ok. Again I have quite a few slides for the next session. I will not do all of them, but I will do a quickly some part of that and then show you what can be done. Now, let us just try to get through this memory hierarchy little quickly because all of you would have done a course in computer organization. So, let me try to get this thing. So, whenever we talked about pipeline instruction execution, we assumed that processor can do instruction fetch in a cycle or can do load or restore in a single cycle and this is because we have always assumed that there are caches in our architecture, right. And this cache essentially is a part of the memory hierarchy that we talk about and all of you understand this memory hierarchy. So, there is no need for me to go. Particularly what we want to talk about is that we have there is multiple levels of caches L1, L2, L3 and beyond that there is memory, right. If all of you know how caches work, then I can quickly skip some of this discussion and then go to the next one. But just to be very brief on this, what we see here is typically we have the processors. The processor gives an address which is typically looked at in the L1 cache. L1 cache is typically a split cache for instruction and data. If the data is available, then it is given to the processor and typically that takes about 1 to 2 cycles. If the thing is not available in L1 cache, then you have to go to L2 cache and if you have multiple levels of hierarchy, if it is not in L2, you will go to L3 and so on, right. So, again you all know about temporal locality and spatial locality, right. Do we need to explain that? Temporal locality, spatial locality, no need, right. Okay. So, let us look at this particular equation which is of interest to us, right. So, whenever I talk about a memory access, right, I talk about an average memory access time and if I have one or more levels of caches, then my average memory access time is essentially determined by the hit time of the first level cache, if the data is available in the first level cache and if it is not, then I have to fetch it from the subsequent level. So, there is a miss penalty, the data is not available, there is a miss penalty and there is a miss rate. So, how often I miss and every time I miss, I have to have a miss penalty. Then this miss penalty itself will depend on whether I have the next level of cache or not and again that equation would be something very similar to this equation, whatever that can be fetched from L 2, it is a hit time of L 2 plus miss rate at L 2 multiplied by miss penalty of L 2. So, it keeps going like this, right. Our idea with the caches is that you want to essentially make sure that the average memory access time is reduced. So, for this, the important parameters are hit rate, miss penalty and miss rate, okay. If you are an architecture student, then you will worry about all three of them and you try to see how you can minimize them. But for the compiler course, what we need to worry about is that since we do not have hardware in our control, I cannot change the hit time or the miss penalty, right. I can only influence the miss rate. So, let us see how the compiler can influence the miss rate. For that purpose, we will try to understand how the caches work and what can be done. Again, just very quickly hit is when you find the data in the cache. For our purpose of discussion, we will limit ourselves to one level of cache and then talk about it, right. Hit is when you find the data in the cache, miss is when you do not find the data in the cache. Hit ratio is the number of hits divided by the number of accesses. Miss ratio is the number of miss divided by the number of accesses. Hit time, of course, we do not, I mean it is the time to access from L1 cache. Miss penalty is the time you take to get it from the, from outside of the cache, okay. Now, we also know about different cache organizations, do not we? What are the three different cache organizations that you are familiar with? Direct mapping, associative, set associative. So, these are the three different cache organizations that you are familiar with. So, they are essentially determined by these three, four questions. Where is a block placed and that is depending on whether it is direct mapped, set associative or fully associative, where it is being placed and then once you know what is the organization, you know how to find the tag and valid, see the tag and the set index bits and the offset bits. And for our discussion, we will not too much worry about the replacement policy or what happens in the right, okay. So, first let us quickly talk about direct mapped cache. In the case of a direct mapped cache, each memory block is being mapped to a unique place in the cache, right. And that unique place is typically identified by a hash function and this hash function is typically a mod function that we use. So, if I have for example, let us say 16 blocks in the memory and 8 blocks in the cache, then of course, block 0 and block 8 will be mapped to cache block 0. Block 1 and block 9 will be mapped to cache block 1 and so on, correct. So, that is really what happens. So, this again you must be familiar with. Let me just get all of these things. So, in a direct mapped cache, if I assume that each block consists of 32 bits, 32 bytes, then my offset is 5 bits. And if again I have done this for a specific case assuming that I have a cache block of size 16 kilobytes with 32 byte blocks, then I have 512 blocks. So, the index will be 9 bits. The remaining are tag bits. All of you are familiar with this, right. You no need to go through this in any more detail, correct. Okay, good. So, then what we do is that you essentially take the address, use the index bits, index into the cache, right and then do a tag match. If the tag match is true, then you have found the data. If not, you have a miss, correct. So, here is what you have shown at the hit, miss is not shown, right. This is what happens in a direct mapped cache. Now, in a set associative cache, okay, each block has multiple places to which it can go to, right. And the number of places it can go to is essentially the associativity. That means that the cache is divided into a number of blocks and then it is also those blocks are also grouped in terms of number of sets. Each set has let us say 2 or 4 or 8 blocks associated with that. If it is a two way, four way or a eight way set associative, then every memory block is uniquely mapped to one set, but within that set it can go to any one of those blocks. That is really the idea. So, again let us see what it is. Here the same 8 blocks are now divided into four sets and every memory block is uniquely mapped to one of these sets, but then within the set it can be either one of those blocks, right. So, this is what happens, right. Block 0, block 4, block 8, etcetera are mapped to set 0, right. Similarly, block 3, block 7, block 11, etcetera are mapped to set 3, right. So, in this set associative cache it is possible to have for example block 3 and block 11 together in set 3. Whereas in the case of a direct mapped class 3 and 11 cannot be together in the cache, okay. Alright. So, this is similar thing. If you have a set associative cache rate of size again 16 kilobytes, then what you do is that you take the number of cache blocks which is 512, you divide it by 4 if it is a 4-way set associative, then you say how 128 sets. That means that you need to have 7 bits as index. Now, how do you find the blocks in the cache? For a 2-way set associative cache the diagram shows you use the index bits, you index into the cache. There will be two locations that you have because corresponding to the set for a 2-way set associative cache you have two locations you search both these tags, right. One of them is a hit, then you have a hit. If both of them are misses then it is a miss, okay. So, that is how you access the data in a set associative cache, okay. Now, let us try to understand how the caches work from a programmer's perspective, right. We will use a direct mapped cache which is 16 kilobytes with 32 byte cache block size, okay. Do you all know, I think this is going to the example, okay. Now, before we go into that do you all know the three different types of cache misses? So, the cache misses are classified into three types, right. One called the cold misses, right, cold or compulsory misses. Another one called conflict misses and the third one called capacity misses. Have you come across these things? No? Okay. Do we need them? Let us find out, okay. Let me very briefly tell you what these three different types of misses are, right. When you try to bring the block for the first time into the cache that miss has to anyway happen, right, because there is no way by which the cache by default will have that block. So, the first miss that happens is what we call as the cold miss, right. Then the misses that happen because your cache cannot have all the capacity to hold all the addresses, cache does not have the capacity to hold all the addresses. Those types of misses are called the capacity misses, right and remaining misses are what are called conflict misses, okay. So, in other way of explaining is that with regard to the capacity misses supposing let us say you have a 32 kilobyte cache, right with block size as 32, right. Then this will have 1k blocks in this, correct. Now, that 1k blocks can hold only at most 1k of locations, right. If you access, if you are going to access data which is beyond 1k blocks, then irrespective of what you do, it is going to give a miss up to 1k only what is what you can hold and beyond that, right, anything that you try to access even though you may want it to be in the cache, it is not going to be there. For example, I will actually give you some examples of this in the next slide where you will actually try to understand that, right. So, because of my limited capacity any miss that happens is essentially a capacity miss. A conflict miss on the other hand happens in the following way. See, you remember you saw the example of block 3 and block 11 getting mapped to the same set, right, set 3 in our set associative cache, correct. What if I also want to access block 7? That will also get mapped to the same set, correct. Now, if I have a program which tries to alternatively access 3, 7 and 11 repeatedly, then what happens? One of them is going to evict the other, right and you are going to have a miss. That kind of a miss is what is called a conflict miss. See, I have 1024 blocks. If you have allowed each one of these blocks to go somewhere in the cache, I could have had all three of them together, correct. If I had a fully associative cache of 1024 blocks, these three accesses would not have caused a miss whereas, because I have a two-way set associative cache, I have a miss. Do not worry even if I have a four-way set associative cache, I could have had all three of them together, right. So, the misses which happen because my associativity is limited are called conflict misses, right. So, these are the three different types of misses. We will not too much worry about classifying them, but somewhere I might mention, okay. Now, let us move forward. Let us take a real example and then see what happens. Supposing, let us say I want to access an array of 2k elements and this is an array of double. Each element is 8 bytes long, okay. And for the purpose of this example, we will only assume that the array is in my data cache. No other variable is in my data cache, right, right. And this is the code that is going to be executed. Again, I do not really see the reason for understanding the code, but let us see what happens here. So, as I mentioned earlier, we are only going to say that the array A and its elements are what is going to be stored in our cache, right. And so, the way in which the elements are going to be accessed in the loop is that you will first load array A 0, then A 1, A 2, A 3, A 4 and so on. Now, can you tell me that when I do write this program and then when I run it on a machine which has 16 kilobytes of cache, right and assuming that only array A is going to be in my cache, how many hits and how many misses will I have, right. That should be analyzable, okay. At least approximately we should be able to get a picture of this, right. So, I will make it things simpler for you. I will also tell you if I know what is the base address of this, right, then I can divide this in terms of I think these ones and zeros have kind of gone here, right. So, okay. So, this is my offset bits. These are my set index bits, okay. These are my set index bits, okay. So, let us first look at, okay. Now, as I mentioned earlier, let us assume that my cache block size is 32 bytes, right. That means that these 5 bits are going to be my index bits, right and let us say how many bits, okay. So, I have 9 bits as my set index bits. So, that means that this has 512 sets, right and if it is a 16 kilobyte cache, then it is basically a direct mapped cache. So, we are talking about a direct mapped cache of 16 kilobytes, right. So, these are the 9 bits. If the address is this, then the first block A0, right, will be in cache block 256. That is really what it says. If you look at this value 100000, right, that is essentially cache block 256, okay. Now, in each element, sorry, each block of the cache contains 32 bytes. How many elements are these? Each element is 8 bytes long, right. That means 4 elements will be in 1 cache block. So, let us look at this diagram, right. If A0 is in cache block 256, then obviously A1 will also be there, A2 will also be there, A3 will also be there. What about A4? That will be mapped to cache block 257, right. A8 will be 258 and so on, okay, right. Now, suddenly in between I have, okay. So, remember this has 512 blocks, right. Now, when I access A0, since I am accessing it for the first time, right, the data is not going to be available. So, it is going to be a miss. Whereas, when I access A1, A2 and A3 subsequently, right, because A0 was brought in, this entire block was brought in, all of them are going to be hits. Similarly, when I access A4, it is being brought in for the first time. Therefore, it is a miss and all subsequent accesses will be hits. So, you can see that the first miss that happens is what we call as the cold miss or the cold compulsory miss, right. All other things are hit, right. Now, if I start from 256 and if there are 512 blocks, I should have gone up to 511, correct. But then I end up with 255. Why? After 511, where would the next block be? 0, correct. See, remember this is a cache, this is an array of size 2K elements, right, 2K elements each of 8 bytes, correct. So, 1K elements would be how many bytes? 8K bytes, right. So, 8K bytes is starting from 256 all the way up to 512 and the next 8K elements will be from 0 to 255. So, that is really what is happening here, ok. So, somewhere in between when you talk about A of 1025 that will be or A of 1024 that will be in cache block 0, A of 1028 will be in cache block 1 and so on and A of 244 is in cache block 255, ok. In this particular case, the array size is same as your cache block size, correct. So, if you want to access the entire array, you can actually fit it in your cache and as you access all the elements, the entire array is being brought into your cache, ok. So, in this particular case, what is going to happen? Your first access is a miss and subsequent accesses would be shits. So, for every four accesses, you have one miss. Now, supposing let us say you try to access this array repeatedly 10 times, then what would happen the subsequent accesses? All will be hits because after the first access, this value is going to be there, this cold miss will disappear and all the data elements are available, right. So, if you try to do this access 10 times, then you have exactly 2048 divided by 4 which is 512 misses and then 2048 divided by multiplied by 10 that many accesses. So, 512 divided by 2480, 20,480, that is your miss ratio so to say, correct, ok. Good, again this is going into all of these details. So, you can just look at it say that for every four accesses, there is one miss. So, that means there is a 25 percent miss and 75 percent hit, ok. And essentially whatever that we are getting is because of spatial locality, data is available. If you iterate it 10 times, then you also get benefit because of temporal locality, ok. Now, instead of 10, 2048, if let us say if you have 4096 elements, then what happens, right. Then also if you go through this array once, ok. So, that just yeah. So, this is example one with this much, ok. Now, the first time that you go through, you have all compulsory misses, but if you go through it 10 times, then what happens is that you remember that you can only hold 2048 elements in your cache because it is a 16 kilobyte cache. If you have 4K elements in your cache, then after you go to the 2049th element, it will actually rewrite the existing elements. Then if you try to iterate, then the second time when you come, you again you are not going to see your elements in the cache. So, those misses that happen are what are going to be called as capacity misses, ok. That is really what this is explaining. Let us look at another example where you have two arrays, each one of them is 2048, right. So, in this particular case, the order in which you are going to do the accesses is that you are going to access A of 0 followed by B of 0, A of 1 followed by B of 1 and so on, right. Now, I will skip the details of, ok. I will actually do this part, but then skip the other details. Supposing let us say array A is in this location and array B is in this location, right, exactly offset by 2K into 8, ok. Now, you can see that the first address is this and the second address is that, right. Now, what do you see? That both A and B or A of 0 and B of 0 are mapped to the same cache block, right. When both A of 0 and B of 0 are mapped to the same cache block, let us see now what happens when I try to do the sequence of accesses. When I try to do the sequence of accesses, A of 0 will give me first a miss, cold miss, that is ok. B of 0 will give me a cold miss, that is also ok. Then what about A of 1? So, this is 256. Both A of 0 and B of 0 are mapped to the same cache block, 256, correct. Now, we have accessed A of 0 which was a miss. So, A 0, A 1, A 2, A 3 were brought to cache block 0, correct. After that we access B of 0. That means that B 0, B 1, B 2, B 3 were brought into the cache block. But where in the cache block? Same cache block 0, which means your A was replaced, right. Then when you try to access again A of 1, that data will not be there. You will have a miss. This miss is a conflict, right, because this data was replaced because of something else was mapping to the same cache location, right. So, if you look at this accesses, then you can see that after the first two misses of A of 0 and B of 0, A of 1, B of 1, A of 2, B of 2, blah, blah, blah, blah, blah, blah, everything is a miss, right. So, in this case having the cache is no good, right. You have a hit ratio of 0. What can you do to improve this situation? Separate cache for A and B. Who said that? Okay. Yeah, unfortunately we cannot change the processor, right or we cannot change what is inside the processor. This is a very, very simple problem. Let us assume that we are going to do this loop only once and I at least want to get that 75 percent, right, which is for every four accesses, one miss kind of a thing. Is that something that we can do other than changing the processor or the cache? Let us say that we are not hardware engineers, right. We are all elsewhere. We have given a processor and we want to improve the situation because, again remember we are talking about this from the compiler perspective. Why were the misses happening? Sorry? Change sequence of, okay. How will you change it? Okay. Okay, that is one possible way of doing it. Very clever because you have done instruction scheduling. You can do that. But something else, more from a cache optimization point. Yes, definitely a clever answer. Why were the misses happening? Because of the conflict, yeah, go ahead. You are trying to say something. If I do something so that A of 0 and B of 0 go to different cache locations and there is no problem at all, right. Instead of having 2048 elements, if I had 2052 elements, correct, they will be offset at least by one block. That is actually all that I need. Very simple, right. Instead of buying a new processor or a new computer, you just use four extra bytes or four extra elements, problem is solved, correct. All that we wanted is that we do not want these two things to be getting mapped to the same cache location. So, this solution is often called padding. You just pad this element so that, you know, two of them do not necessarily map to the same thing. Compiler by default will do that today. If you want, you can check in your compiler whether it is happening or not happening, right. That means you run a program with 2048 elements and then run another program with, let us say, more number of elements, 2052 or whatever elements and then see whether their execution time significantly differ. They may not. So, all that we need to do is that the compiler should assign addresses such that they do not map to the same location. And if it does, all that you need to do is to pad the first array with four extra elements. The moment you pad the first array with four extra elements, yeah, I will just take the question after I finish this sentence. So, the moment you pad the first array with four extra elements, right, the second array getting shifted at least by one cache block and therefore, both a0 and b0 will be in different locations and therefore, all of these accesses will result in the one miss followed by three hits. That pattern will continue. That is again only for this example. I am sure that you are going to have an example which is different. Go ahead. Right. Right. So, that is entirely a different problem, not a cache problem per se. Even there, if you have these two arrays which are getting mapped to the same location, you will have the cache misses, okay. And again, if you have the cache misses, you bring those b, right, and then you load it. You load it, but you load it with a cost, right. We will talk about these vector processors right after this, right after this, okay. Again, this is a separate optimization that you have to do. That is a separate optimization. These are orthogonal, correct. So, is this solution fine? The simple padding, okay, either add four elements to a or you have a small array of 16 bytes, whatever it is, sorry, 32 bytes so that the one gets shifted. Anything which is more than 32 is all that you need to do, okay. So, essentially, you have to make sure that the base address of b is such that it is not that. You may not directly see these addresses, but you can actually do this padding to make it change, okay. Now, the moment you do this, this is what you are going to get, okay. You are going to get back your 75 percent heat ratio. You could have also done another thing, right. If you are going to have this kind of a thing where you have a and b, instead of having them as two arrays, you can have it as an array of structures, right, where the structure has two elements a, b, right. So, this is what is called structure of arrays versus array of structures, right. So, this is an array of structure idea. So, when you do this, what happens is that each element of the structure has a and b, right, and they will be adjacent elements. So, one cache line will have two structures, four elements, which is two structures, right. Structure 0, a. Structure 0, b. Structure 1, a. Structure 1, b. Next cache line will have structure 2 and structure 3. The next cache line will have structure 4 and so on and so forth, right. When you reorder your data structure, that also helps. So, if you are going to use two arrays which are kind of going to be used together and if they have sizes which are kind of likely to conflict, then you may want to think in terms of array of structures rather than structure of arrays, right. That is another way by which the same problem can be handled, okay. Let us talk about the third important thing which is about dealing with more than one dimension of the array, multiple dimensions of the array, right. Let us now look at this code. I have a nested loop, right, for j, for i and then I have some accesses, right. Now, let us look at the reference sequence. You are basically accessing a 0 0, a 0, right. Yeah, you are accessing a and b alternatively. So, a 0 0, b 0 0 and then you are storing b 0 0, then a 1 0, b 1 0, b 1 0 and so on, correct. Now, assuming that, let us assume that a and b are not conflicting to the same location. Would you get locality here? Would you get the spatial locality that you are seeing earlier? That is an yes or a no? No, because sometimes, you know, some regions yes is this and some regions no is this. So, I do not know. So, would you get spatial locality? Assuming that, let us assume that a of 0 0 maps to cache block 0 and b of 0 0 maps to cache block 256. So, let us say we have put them far apart, right. Let us worry about that. Why would they not give me a spatial locality? a of 0 0 and then a of 1 0, right. So, this depends on what is the order in which the elements are stored. Are we talking about a row major order or a column major order? If it is a C program, it is a row major order. In a row major order, spatial locality is going to be seen from a of 0 0, a of 0 1, a of 0 2 and so on. So, if you access the elements like this, you are going to access a 0 and then a 1 0, which is far away from that and then a 2 0, which is far away from that, okay. I do not exploit spatial locality in that way, but then after I come back, I am going to eventually access a 0 1. Will I at least get the locality then? No, because by that time I would have replaced this with something else, correct. So, that is really what is going to happen. So, let us look at these things here. So, we talked about this row major ordering, which all of you know, right, as opposed to the column major order. So, this is row major order, right, a 0 0, a 0 1, a 0 2 are successive elements, whereas in column major ordering, these will be the successive elements. So, if your storage is in row major order, then you must necessarily do row major ordering to get spatial locality, right. The array is small, then it is okay. Even then you may get the spatial locality, because the entire array stays in your cache. But let us assume that it is a 2k by 2k array, that is like 4m, way, way bigger than any L1 cache or L2 cache, right. So, something is going to replace. By the time you come back and want to look at it, it is not going to be there in the cache, right. So, that is really what is happening here. I have assumed it to be 1024 cross 1024, that is like 8 megabytes, right. 8 megabytes is far too bigger than your L1 or L2 caches, right. So, you and then you have two such arrays, that means 16 megabytes. Definitely it cannot hold in your cache. So, you are going to have capacity misses. So, by the time you come back and access A01, because of capacity misses, those lines would have been thrown. You can actually work out the details and find out, right, okay. So, in this particular case, because you do a load of B00 and then a store, one of them is going to be a hit, the other one, sorry, one of them is going to be a miss, the other one which you access immediately would be a hit. So, of these three accesses, there will be two misses and one hit. That is why you are getting 33 percent. If the store has not been there, you would have got 100 percent misses, okay. So, you remember the earlier program was something like this, right. For j is equal to 0 to 1024 and then i is equal to 0 to 1024 and you are accessing A of ij. That is what was the problem. If I have switched these two loops, then what would have happened? My accesses would have been to A00, A01, A02 and so on. And in that case, I would have had my 75 percent or more kind of a hit ratio. So, again it is the responsibility of the compiler to understand what is the right storage order. And if the loop is not in the storage order, is it possible to interchange the loop so that you can do the benefits of, you can get the benefits of spatial locality. It may not always be possible to do the interchange. May be unrolling. May be unrolling. Will unrolling help in this case? Yes. Unrolling will not help. Unrolling still essentially takes the same order. Unless you unroll it in a different way, okay, which essentially boils down to doing the interchange, correct. If you want to get spatial locality here, you have to do the interchange, right. Are you all with me? Okay. So, let us keep this. Finish this. So, if I interchange the loops and then write it in this order, A of i is equal to 0 to 1024 and j is equal to 0 to 1024, then my access order is same as my storage order. And therefore, I am going to get the spatial locality, okay. Now, okay. Let us just move on to data parallelism. Yeah. This is 2D array. Not coherence wise, coherence is not the point at all. Leave that aside, yeah. Why? Why? In any case, it is actually stored as a one-dimensional thing in memory, correct, right. Is it how it is going to do? If you have declared this as a two-dimensional array and if you give A of i j, it will actually compute i times the number of elements in that multiplied by this plus j times the number of this. Get that as an expression and then index it outside of the A of something. That is how it is supposed to generate code for that, right. It is not like I am going to find out where A of i 0 would be or what is the address of A of i 0 and then to that I am going to add something. Whichever way you do, I think it is only the index address calculation, right. Dynamic memory allocation. Dynamic memory allocation. So, if you are declaring it as a two-dimensional array and then allocating all the space upfront, it is the same, right. If you are doing dynamic allocation, then the problem is different, right. You are essentially allocating an array of one dimension and for each one of them you are allocating an another array, obviously. So, in that case, using the one-dimensional array as a two-dimensional array. Yeah, you are essentially saying that I do not do dynamic allocation or even if I do dynamic allocation, I do all of them together in one shot, right. See, again think of it in the following way. If I have an array which is 1024 cross 1024, why would I want to do dynamic allocation, right? You would not, right. There are, you know, programs and there are variables where you want to declare certain things as, I mean declare something statically and there are certain places where you definitely have to go for a dynamic thing. When you do a dynamic thing, I am assuming do a 1024 cross 1024. It is 1024 and then for each one of them will have different, for example, when I talk about an adjacency list representation, right. There are the cases where you actually do dynamic memory allocation. Why is it that for dynamic allocation to the array, it is so much different than on statically allocated? Okay. Because this different elements of the rows are going to be allocated, I mean, it is going to be allocated by the malloc separately, right. And there are certain analysis that you can do with a statically allocated array. In a dynamically allocated array, you cannot even do many of those analysis, right. So, I would not say why or which one would perform better. It is kind of hard to predict, okay. By saying that I will declare it as a one dimensional array, you are saying that I am going to allocate all of them together in which case it pretty much becomes same as static allocation except that it is done dynamically at runtime. It is a contiguous piece of that many elements whereas this also was a contiguous piece. This was in data segment whereas that will be in heap segment. Other than that, there is no real difference, okay. There may not be, okay. Any other questions? All I wanted to point out here is that do not, I mean, when you have these programs, you should be able to find out which of these accesses are likely to give you more hits and which of these accesses are likely to give less hits. That we should be able to figure out and we cannot say that we would not be able to figure this out because after all we understand caches, we understand how program works and we should be able to figure this out, right. Any other questions? So what happens in a matrix multiplication? That is the eventual question we want to get into, right. All of you remember the matrix multiplication program, right. The program says A of ij is equal to A of ij plus B of ik multiplied by C of kj. So, B array is going to be accessed row wise. C array is going to be accessed column wise, right. So, in one of them you will get benefits of spatial locality. In the other, you will not. And if these arrays are too big, in neither of them you will get temporal locality, correct. So, is that something that we can do to improve that? Yeah, but then when you do the transport you are anyway incurring the cost, right. True, I mean if you transport then you can use that, but then when you do the transport you are going to access C in the column wise way or you are going to write it into the transpose array in the column wise way. You are going to get hit somewhere. You can postpone the problem, but you cannot solve the problem, is not it? But there is an advantage. If you are going to repeatedly use that you will get the benefit of that, true. Okay, so we are going to talk about that little later, right, if time permits. Now everything is if time permits because we have only one more session. So, let us see what happens. Okay, let me quickly talk about this parallelization and SIMD machines or SIMD operations. So, one class of parallel machines are called SIMD machines which stands for single instruction multiple data and in these machines essentially you execute a single operation or a single instruction, but that single instruction operates on multiple data elements. That is why it is called single instruction multiple data elements. Typically when you have a vector and you are trying to do A of i plus B of i across all elements of i then that is like a vector operation or that is like a SIMD operation, right. Today you can see SIMD machines in many forms. Earlier days there used to be vector processors which used to exploit SIMD. There also used to be array processors which used to exploit SIMD, but these were like the 60s and the 70s and the 80s, but today's processors have instructions like the MMX instructions or the AVX instructions. These are what we call as the wide word instruction, right. So, for example I have a 512 bit register and I have a functional unit which can actually do operations on 512 bits. That means that I can take 1632 bits and perform operations on all of these 16 operands. That is really what it is, right. So this is also a form of SIMD machine, right. So it could be 512 bits or 256 bits or 64 bits and depending on that it is called either MMX or SSE or AVX and so on. Again AVX 256 is there, AVX 512 is there and so on. So there is also one other class of machines which could potentially fit into this SIMD machines. What is that, right? GPUs. Right, because they have what are called SIMD cores in them and those SIMD cores try to execute instructions in parallel, same instructions on multiple data, right. It is actually I mean people classify this as SIMD, single instructions multiple threads, but it actually operates on multiple data values, right. That is also a SIMD machine, right. If you have not heard about graphics processing unit, go to NVIDIA, right, for a visit and they will tell you what it is. Okay, so let us focus on this SSE and AVX instructions, okay. This shows in the timeline how these things have evolved, right. I think way back in the 97 we had this MMX instructions which used to operate on 64 or 128 bits. Now today we talk about similar operations which are happening on 512 bits. So let us see what this is. I have the example in the next slide. So this is really what happens, okay. Let us assume I have a register which is 128 bits long, correct. So I have a set of registers which are 128 bits long and let us say, okay, my operations have kind of shifted, okay. And I have four functional units which can perform operations. And what I can do is that I can take some parts of X naught and the corresponding part of Y naught and I can perform the operation to produce Z naught. Similarly, X 1, Y 1, Z 1, X 2, Y 2, Z 2 and so on. This is essentially what happens and this operation can be add, subtract, multiply or logical operations whatever it is, right. So by giving one instruction I am performing operations instead of performing it on 32 bits, I am performing it on 128 bits. Therefore that single operation essentially results in four parallel operations at the same time. This is essentially what we call as SIMD execution or SSC or AVX or whatever you want to call it, okay. Modern processors including the processors that you have in your laptop and desktop today support some form of AVX instruction. So today afternoon in the lab session you will try to find out whether your compiler can generate code so that it can make use of those processors capability, right. That is something that you will see. Again the reason that we are trying to study is that what should I do as a compiler writer in order for me to generate code for these parallel machines, right. That is really what we are trying to look at. But first thing is that we need to understand what these things are, right, okay. So for example Intel, okay Intel of more recent kind let us say if you are looking at Skylake kind of a processor would have a 512 bit register which means that on that you can do 16 single precision floating point operations or 8 double precision floating point operations or 32 16 bit operations and so on and so forth, right. Where are those 16 bits and 8 bits are relevant? All of you should know the keyword, buzzword. Machine learning, deep learning, right. They all work with low precision things so that you can actually do using these 512 bits, right. What 64 parallel operations, 8 bit operations, correct. So all of that is possible. It also has 256 and 128 bit registers and these are called by different names, okay. So there you go, right. If you have a 256 bit register on that you can do 8 single precision floating point operations or 4 double precision operations or 32 8 bit operation, right. These are typically used in machine learning kind of applications, right. So if you have read about GPUs and GPUs capable of having a very high AI flops, AI floating point operation this is really what they are talking about, okay. Now this is all fine. Architecture provides you all these features, right. Programmer do not write code like this. You still have to take the code for i is equal to 1 to 10, a of i is equal to b of i plus c of i. How do I generate code for that, right. That is the problem that we are trying to talk about, okay. So compilers have automatic vectorizations which is what we all enjoy like we say okay I do not have to do any work. I just press the button if it can do it is good. It typically works on simple programs, okay and to certain extent it helps you. So you can see where it works and where it does not or you can do thread. So okay so let me give you the example of automatic vectorization before I go to different other methods of doing this. So again here I have three arrays x, y and z each 256 elements and we are basically trying to perform write some kind of element wise product operations on this, right. y of i is equal to well I should have said z of i but does not matter, okay. So you look at the code over here very complicated. I do not even understand what this is, okay. There are two move instructions which basically move array y and array z into these two registers, okay. And these move instructions would move a set of 16 or 32 words together depending on the size, okay. x is actually 512 bits so it will actually move 16, 32 bit values into that, right. Then you do a multiply. So the v essentially says it is a vector operation. So it will actually do a parallel multiply of this xmm on register with xmm to register. So you will see something like this happening, right, okay. First you move the operand from the memory into these registers and then you perform this parallel multiplication, correct. And then you store these values back into the other array, right. That is again the move operation. So as I mentioned this should have been z not y, okay. So you are essentially moving this xmm 0 back into the z array. So these four operations, each one of these four operations essentially perform equivalent of 16, 32 bit operation, right. And they all can be performed in the same amount of time whether you perform one operation or 16 operation. The time taken is same because they are all going to be done in parallel, right, okay. So this is how the code is going to look like. So afternoon when you compile your code for vectorization do expect to see some really messy code like this, okay. Now you asked a question about vectorization and cache. Is that question still there or not? It is gone, correct. So why do not I ask you the question? Supposing let us say I have a two dimensional array xij, right and I want to multiply x of i0 with y of i0 for different values of i. Then what happens? Assume row major ordering, right. Again we are doing the same thing. If it had been like x of, I mean if I am doing x of i0, i1, i2, i3 with y of i0, i1, i2, i3 you could have easily brought them, correct. Spatial locality would have been there, you would have brought it to these 512 bit registers and you could have performed the operation, everything is fine. That case was no problem. But let us say I want to multiply x00, x10, x20, x40 with y10, y20, y30 and y40. Now these move operations which are actually able to move things together, right is going to struggle because these are in different locations in the memory and therefore in different locations in the cache, right. So, if this move takes let us say t cycles, that move if it is done inefficiently will take n times t, n times t, t times 4 not t plus 4, t times 4 or t times 16 amount of time. But then there are some intelligent operations like gather, right which will actually take values from different locations and somehow try to push these things in a way which is faster but definitely not in t, okay. So, you use gather operations to move those operations into a vector register. Then of course you can perform these operations we multiply d together and then you put a scatter operation which will actually put it in different places in the memory, possible to do that, okay. There is a cost additional cost but it is better than doing it as a scalar, right possible, okay. So, there are lots of things that are happening and what we now need to do is that since architecture is doing all these things as compiler writer we should be able to generate code appropriately. For example, if my architecture supports scatter gather and I have a program which is like this I should rather use the scatter gather instructions to do that efficiently, right. Again more challenges for the compiler writer, okay. So, how do we exploit vectorization from our processors? Either you could use automatic vectorization so GCC or something will have, right the appropriate flag which helps you to do this, okay. But it may work for some simple cases like this but not necessarily for all possible cases, right. You can also do one thing you can also indicate which arrays or vectors where you are likely to perform vector operations and declare them as vector arrays, vector data types and the compiler will take that hint and then try to vectorize it. That is another approach. Again I am not going to go into all the details of how to do that etcetera because that is really not the purpose. You can also write vector intrinsic functions which actually does all of these things, okay. Here I write the bad code in this form and then I give this to the compiler. The compiler uses this to generate vector code. So, you can do one of these three things, okay. But now we will ask the following question. How do I know that this array this loop can be vectorized, right? How do we know that this loop can be vectorized? We have to do certain analysis to figure out that this can be vectorized, right? And that is when you can actually generate code like this. In fact if GCC or LLVM is generating code like this they must be doing that analysis because all I wrote was this. Then it figured out that this can be vectorized and I have to generate code like this, right. So, that compiler was smart enough to do that by doing certain analysis. What are those analysis, right? Again we talked about cache locality and other things and said that okay because you wrote the loop as for j for i it was giving those locality. You have to interchange the loops. How do we figure that out, right? And how do we analyze that and how do we say when we can actually interchange loops? Can we interchange in all occasions? No, you may violate some dependency. So, how do we do those analysis, right? We will try to see some of these things very briefly, right, in the afternoon class, okay. That I am going to do it slightly at a high level. So, I will not go into all the details, but at least try to tell you what is in there and what we need to do. So, I want you to before we conclude this session I want you to take back this following things. Although in the last two days we talked about a lot of mundane things like code generation, register allocation, instruction scheduling, all of this look like some nitty-gritty details, right, may be trying to say one cycle here, one cycle there kind of a thing. It is not about that. It is about the new features which are coming in the processors, the new things that are coming in the processor and being able to generate code automatically for that. That is why compiler design is exciting, right. Today you have GPUs. How do I generate code for GPUs? Can I take your C program and then automatically generate a CUDA code which will do certain things, right? Or if I have a CUDA code can I analyze it and make it better for the GPUs, right? Or if I have a C code can I exploit the AVX capabilities in the CPUs of the processor? Or if my processor has four cores, eight cores, all your processors has four cores and eight cores. How many of your programs are exploiting that? May be nothing, right, because you did not put the proper compiler switch or may be the proper compiler switch is not yet available to automatically parallelize some of them. Can you write one of that, right? So, there are lots of interesting opportunities in compiler design, right, that you can actually do to exploit that. So, do not think of compiler design as oh that lexical analysis parsing blah blah and you know. I mean today nobody really talks about many of these things. It is important for us to understand to know how they are done, but there are lots and lots of interesting challenges and opportunities in compilers, okay. Many problems that you can work on, right and everyday processors are coming up with more and more capability. First of all, I mean let me before I go into that, first of all let me ask you the following question. Why do we even need compilers? Why cannot we just write programs in assembly language? To write large programs is difficult. Even to write an assembly language program, what do you need to know? You need to know about the architecture, machine architecture and other things, correct and then the moment you write an assembly program for one machine, if you want to take it to some other machine, you will have to redo the whole work, right. So, essentially the reason that we have a compiler is that you want to abstract some of those details, correct. So, that the programmer need not have to worry about it. He can still think of A as an array, B as an array doing some addition, for loop, blah, blah, blah, correct. So, everything else is taken care of by the compiler. Similarly, whenever there is a new feature that is coming in the architecture, do you want the programmer to know about it and exploit it? Do you want the programmer to write code like this or do you want the compiler to generate code like this? That is the question, right. So, here there is with explicit arithmetic instructions or data types, here with this with intrinsics where he has to generate a lot more code by knowing things. So, what do we want them to see? What as compiler writers can we enable them to do, right? That is really the question, right. So, again there are lots of interesting problems and challenges that could be solved by writing better compilers. So, keep that in mind when you come back for the afternoon session, that we will stop.