 Okay, so let us start now the session. So one thing is that last session all of you look very sleepy, so look like you are tired of static analysis, right? All of you become very static. So I need to get some dynamism in you. So we will get to, we will look at something called dynamic analysis. So not much but at least I will give you introduction what does it mean, what to do with it. So the analogy was that remember we said that, so we said that there are two ways of, even as a human there are multiple ways of inspecting a program and figuring out what is going on. So what were the different ways we said we could really find, maybe let us say find a bug in a program. One is we can actually stare at the code and then say okay what is going on with it. That was static analysis because you are just looking at the program text and try to figure out what is going on. It was a very similar case with our analysis that we were doing till now. We were just analyzing the program text which was maybe compiled down to bit code or whatever it is and then it was, we analyzed the bit code. But whatever it is, it is the form of the program text, right? We were not doing anything with the, we were not trying to understand the executions of the program. So the other way of analyzing a program is to simply run the program a lot of inputs, collect what happens on those different inputs and they say oh looks like the program is doing this, right? Like for example, let us say you want to figure out what a certain program is doing. What you can do is you can essentially stare at the text and say oh two integers plus a plus b okay it is adding two numbers. The other option is just run the program on different inputs. You give 1 and 2, it gives 3. You give 4 and 5, it gives 9, right? And then say oh I tested it on quite some few programs. Every time it gives the addition of two numbers. So must be the program which adds two numbers. What is the problem in each of these two cases? So let us say static analysis and dynamic analysis. So what is the pros and the cons do you think? Static analysis will be? Ensure the correctness. Excellent, excellent. That is the main important point, right? For a static analysis we are trying to analyze all possible executions and then say what might have happened, what might happen, right? However, in dynamic analysis we simply run the program on few inputs and that is all I get to see. So let us say even this addition of two numbers maybe if the program was something like this where it says if a is greater than b then return like a plus b else return a minus b. So let us say all the inputs you sampled with one of them were a greater than b. So you never even handle the other side. You never even went to the other side. So you never got an output where you got the other result. So you never got to know that the program has a different behavior because you never run the program on an input which could have exhibited that other behavior. However, in a static analysis you would have done some sort of like MFP analysis where this would be one basic log, this would be one basic log and you will take a join of these two basic logs. So no matter what you would have anyway seen tried to capture the different behaviors. Very good. So if static analysis is so good why should you even have dynamic analysis? But I still have to do an analysis. So what do we do in that case? You are on the exactly the right path. No, but I can still take anything and do some sort of static analysis on it. Debugging I can do static analysis also. I can try to see what are the different like for the buffer overflow example. We were trying to do debugging in static analysis. We were trying to find out the range sets of the index and whichever if the index can overflow then I would say that well looks like there could be a bug. No, but that you can do in static analysis also. So static analysis will tell you the you can do a pointer analysis as a static analysis and it will tell you the what are the locations it can point to. What is that bit of uncertainty? Why is that there is a certain bit of uncertainty? So what it is summarizing things for all inputs. That is what we want. I want to know the summary overall inputs. That is anyway what I want to capture. Those things I can always discount in static analysis also. I can say don't even look at paths which are going here or coming from here. I can do the discounting. So it's a matter of modeling the program properly, modeling those constructs properly. Just think about what all we learned last time. So we are doing an MFP analysis. So what cannot be nice in that setting? So we are doing this. We did this data analysis. So what was the property of the MFP analysis? So what is the ultimate thing? The ultimate thing is I want the MOP solution no matter I do a static analysis or dynamic analysis. In dynamic analysis also if I am able to capture all paths and I can summarize the results across all paths, I will be most happy. That is like awesome life. So problem is your inputs can be, number of inputs can be infinite. It may not even be bounded. That is what is happening in dynamic analysis. Some parts are not sampled. Some parts are sampled because that input is seen. Some parts are not sampled because input is not seen. And as I do a meet of all paths only on the paths which are sampled. So that is the idea. The idea is that instead of looking at all executions, I do a few executions and I take a meet over, meet or join over whatever results I have seen there. So the problem is that there are certain paths which are missed out. I have not seen them in execution of those paths. So it is not really the MOP solution. So if I do this meets of join path like this, then let us say I have, this is my solution dynamic. This is my solution static and this is my solution actual. Can I give, can you give me a relation among them? So what is happening in static analysis? So what happened with the MOP solution? We end up computing the MOP solution. So the MOP solution and MOP solution, how are they related? So it is an, so the MOP is over approximated as the MOP solution. So essentially then what is it? So S star is over approximated by S static, solution static. What happened in case of dynamic analysis? Some paths are missed out. This S static is this optimal solution is actually the MOP solution, is the solution I would have liked to have. But I did not get all the paths. I just got a few sample paths and I took a meet over those sample paths, meet or join over those sample paths. So then what is the solution then? So it is smaller than this. So this is exactly the crux of the situation. So I would have liked to reach a star, but either I have to go the static way and reach an over approximation or I have to reach, go the dynamic way and get an under approximation. So if I was doing, so like if you try to use those results in a compiler that can be dangerous because I can miss out on definitions if I am reaching definition in a dynamic way. So I might think oh this does not reach and I can do an optimization which can really spoil things. So dynamic analysis generally, I will come to that okay because I agree this is a compiler school so I should always connect to compilers. But dynamic analysis has already many other applications other than compilers like for instance in program debugging like somebody also already said. The good part about that is that if you see a fact in a dynamic analysis that fact is surely there because I saw an execution that is why I included that factor. If I say some dynamic some reaching definition is there in this set I do not know what all reaching definition I missed. If I missed anything or not that I don't know, but whatever I included surely that is there. But in case of static analysis it can happen that it came due to some over approximation. Not in the reaching definition case because here the solutions converge to the A star but in other analysis like constant propagation for instance. So constant propagation if I do the dynamic way if it says that somewhere a value 5 is seen you are sure that that value 5 is seen there. It may be that some other value 8 is also possible that is a different matter but that value 5 must have been there at least through some execution. So essentially one is the curse of over approximation the other is that of under approximation. So none of them is bad or good it is very difficult to make a pick. So it is always good to know both and depending on the situation you use whatever you want to use. Sir in the case of dynamic analysis if you look at it out of the context of the compiler, so wouldn't it be used for it and through an invariant property or something like that? But an invariant will be has to hold across all program executions. No it is possible and it is done. So one way it is done is program invariant an interesting property of program invariant is let me just include the whole class. So there is this notion of program invariant. So these are actually the tools that you use to prove the correctness of programs. So for example a program invariant there simply you can just simply think them of some predicates. You can just say that the program invariant at a certain point is i is greater than 0 which means that no matter what path I come to this place the value of i will always be greater than 0. So that is an invariant at the program point. Why is it interesting? Because let us say I am doing a division of I am doing a divided by b. I am computing some x equals a divided by b and I can prove that at all through all paths I can prove that b is greater than 25. Let us say I can find this invariant. Why is it interesting? So I have this statement which is x is equal to a divided by b. Why is it interesting? Yes I cannot do a divide by 0 because I know b is always greater than 25. So I can prove I can give you with guarantee that there is no way that this guy can produce a divide by 0 error. Now the problem is that there are certain invariants I mean most often invariants have to be guessed they cannot be computed like especially loop invariants. So they you basically can guess an invariant and you can check if that is invariant or not. But that guessing has to be done like through a clever guy. Some guy has to come and scratch his head and look at the code and say oh looks like this is invariant. And then it can be checked it is invariant or not and you can throw it away even if it is not invariant. I am not getting into details but a dynamic analysis is good way of guessing that invariant. So if you sample a few program paths all of them you observe that b is greater than 25. Propose that invariant, propose it okay let the invariant b is greater than 25. Then you can anyway formally check you can actually statically check if it is correct or not. So there are many situations where such analysis becomes really interesting because of that. So the main difference again just to summarize is that whenever I get a behavior in static analysis because it is an over approximation I get something called false positives which means that it may say that a certain faulty behavior is possible though that behavior is not possible because it did an over approximation. However in case of dynamic analysis I can essentially miss behaviors right because this guy is only sampling on a few paths and it may not see the faulty behavior which it would have seen if it had been able to see all paths. Fine okay good so now the question is how do we do dynamic analysis? How can we go about doing dynamic analysis? So let us say the dynamic analysis I want to do or let me just answer one question there somebody asked me I will forget about it. So why do you think it will be interesting for a compiler to do any form of dynamic analysis? Because the facts I produce from this I may not be able to use because they may not be safe. So if I directly use them in an optimization I can produce wrong results. So what can we do? So why can a compiler be interested in some sort of dynamic analysis? So can you give me some interest in dynamic analysis? Something that you can do by just executing the program multiple times. Oh my god it is so long I mean slowly much simpler things. No but then I might have missed even for that particular platform I might have missed some inputs so that I be greater than 25 may not even hold for even for that execution even for that environment. But even those inputs you may not be able to sample the complete all of them right even that set might be very large if not unbounded. So conditional jumps are nothing but what is used to implement your if statements they are very common things. No but you have already done the analysis and then you have already compiled the code right so the code is already compiled. Now even if you see that jump there is not much you can do you have already spoiled whatever had to be spoiled right so the dinner is over then you realize that so that is not going to help me now. Yeah that is okay. The input from a user is a different channel altogether of course we can take annotation from user which can help us but here we are saying we just sample from program executions what can we do with that. So one very interesting very important dynamic analysis is referred to as profiling program profiling rather control flow profiling. So control flow profiling is actually used to let us say count how many times a program statement is executed right I just count how many times a program statement is executed that is it big deal why should I be interested in doing that that is maybe similar to what Ujjal wanted to say probably. So once I know that histogram if I know that how many times a instruction is executed what can I do with it. No it is executable time does not mean that the state change will be the same no cache is like if it was making the same transition even if it is the i plus plus like if the value of i is 2 it becomes 3 but if the value of i is 45 it becomes 46 so caching 2 to 3 is not going to help me much. It caches the instruction but instruction caching doesn't work like that right so there is a prefetcher and so you can do some sort of prefetching additional but that is one way but how will it is a compiler manage memory maybe but more simply I mean that is a good point right I can find out how many times a function is executed if I see a function is executed a lot of times I can inline that function so what is inlining a function basically copying its whole code inside it what does it save yeah so it's a lot of code to actually make a function call you have to push the parameter onto the stack push the return address make the jump right even coming back you have to destroy the frame so there is so much to do right so you can save all that if you inline the code but you would not like to do it for the whole program because then the code becomes too much and the same function may be used a lot of times and duplication of code a lot of problems so maybe for a few functions which you know are small and use a lot often I can do this business the other thing is let's say for example I have I know that I have an if-then-else statement and I know that this statement is executed more often than this statement like sorry the if branch is the then branch is executed more often than the edge branch let's say you know that so then there are certain optimizations you can do to sort of speed speed up executions along this path right try to always do things along this path it might happen that you miss out something here right so maybe the cost of this path increases actually right the other path maybe the cost increases which as I said yesterday is not allowed by our traditional optimization our traditional optimization will not allow that if the execution cost of any path increases it will say I cannot do that optimization right but there are these special optimizations referred to as pgo's or profile guided optimizations which can do that which are allowed to do that which can say that okay if I have a heavily heavily executed path I can push more code there I can sorry I can reduce code there or make that path lighter right even if I do not like for example what can you do let's say for example I have this particular program and I know that this particular path maybe let's use a blue let's say this blue path is very heavily executed and let me just try to say what statements are there you Let us say x and z are already initialized. Let us say I have this computation. Let us say I have this computation. So, what can I do here? Can I do something if I know that this blue path is heavily executed? How? Now, I mean, I am specifically giving you an example. So, on this example, what can you do? I cannot remove z. Yeah. I can do what? No. So, essentially the thing is that this particular maybe this is not a good example. I can actually do a traditional optimization here. No, I want a specific answer and I will have that answer. I do not want any other answer. Okay. Let us say this. Let us say this is the example. So, what can I do? Where do I pre-compute it? No, but I still have to compute it. How does it help me? There is no redundancy, right? A plus B is not computed multiple times. It is still, even if you computed it temporarily, you still have to do that computation. No, but you do not know. Let us say this is the whole program. Let us say this is the whole program. Okay, I give a more. I will keep on changing it till I get the answer I want. What about this particular case? Yeah, so I can. So, let us think of constant propagation. So, this will not allow constant propagation. Why not? Because I am getting different values of x along different paths. So, I will get a not constant and I will get a non-constant here and I cannot do constant propagation. But now, if you allow me to do constant propagation, modular profile information, then if I know that this path is heavily executed, then I can actually could put constant propagate it 5 here. Yeah, it is unsafe. No, but that does not help. Not constant, constant are at analysis time. At runtime, these entities do not exist. At runtime, these are actual program states where these guys have values and you are running only one execution then. Something becomes non-constant because there can be coming through different executions. It may be a program input, but when you are executing the program, you know the inputs. They are already values and you can actually know which path you are taking. So, actually taking exactly one program path. Yes, so essentially what you can do is, so these are referred to this class of, you can do something called speculation. So, you speculate that because this guy is 5, now it is constant propagate z and this whole thing you can say output as 5. What you can do is, you can put a check here. Did I end up taking the same path? If your check fails, so this is like putting an extra information, extra code here, a check code here. If that check code fails, it will return control back to the whole, back to the entry point and re-execute the whole thing with the actual values, the actual code. Yes, that is why the new path will become, the other path will become slow because it has to restart from a separate execution and do it and come back. But if it does not happen a lot often, then I am good. So, maybe instead of 50, it is let us say these many times. You see through profile information that is there is a lot of times and this only happens 5 times. Do it. I mean 1 in a whatever number of zeros times, you will fail and you have to re-execute. You have to incur more cost than you would have got by not doing this optimization. But then it is okay if it happens very few times. And this is a very, so these are very aggressive class of optimizations, but they are used a lot in like both architectures and compilers. So, even there are things like value speculation where you just speculate the value of a variable. You say looks like the variable is going to have this value. value and see. So, there are a lot of very interesting things happen with such profile information. So, let us try to build a dynamic analysis, think of a dynamic analysis which will do profiling. So, I want to do the simplest kind of profiling which is I want to find out a histogram of basic blocks. I want to find out how many times is a basic block traversed in a given execution. So, now over multiple execution I can sort of accumulate that values and then I have a nice profile across multiple iterations that what is the probability how many times a basic block will be executed. So, I want for every basic block I want to count how many times was that basic block hit in a given execution. How can I build that as a dynamic analysis? So, this is control flow profiling because we are trying to understand something about the control flow. So, how can I build that analysis? Very good as simple as that and how do I maintain the tracker? But how will you dump that? No, I mean you are right completely right, but I just wanted to like ask you I mean more details about the implementation. How will you actually do this implementation if you had to? What will you how will you maintain that information? So, there is a variable corresponding to each basic block. How will you maintain that which variable is corresponding to which basic block? What is the frequency of each basic block? How will you maintain that information? Vector of very good very good vector of basic blocks. Excellent exactly that nothing more than that right. So, you give each basic block an identifier right. For each basic block you give let us say this is my basic block 1, this is 2, this is 3, this is 4, this is 5, this is 6, this is 7. Then you take an array of 7 size 7, 7 integers and whenever a basic block is hit you just have an instruction which is count basic block number or basic block id plus plus right. So, every basic block knows what is the counter it has. So, the first one will be count 1 plus plus, this guy will be count 2 plus plus, count 3 plus plus, this will be put in at as an instruction right inside the code the LLVM code. So, you will use LLVM or any other compiler to insert this instruction in the code. Whenever that code will get executed it will increment that counter. At the end of the day you dump this array it will contain the histogram of all basic blocks the count of all basic blocks right. Works for us any questions on this? to our actual assignment which was that we wanted to figure out if there is a buffer overflow in our arrays. So, I want to write a dynamic analysis which will trigger an error whenever there is a buffer overflow on my stack. Do you understand what is a buffer overflow? Before that who did you guys look at that program tell me what happened in that program what was why you are behaving so weird that motivation dot C could you run it? It showed weird results for your machine also. Oh thank God. Did you study the code and try to figure out what was going wrong there? Why does it change the value of C? Excellent. So, I think the class almost got the idea right. So, it was exactly that business what you said. So, essentially what was happening is there was this variable this array sitting here and the before the array the variable C was stored right and essentially what this guy did was whenever it had to it ended up the computation was such that it ended up computing array minus 1. So, whenever it so C there is no check on the bounds of an array it does not check if you are crossing the bound of an array. So, it simply takes the base address and adds the index to it whatever is the index value multiplied with the size of the base type right that is exactly the computation that C does and dereferences this value. So, in this case it looked at the base address was array it did a minus 1 it reached C and it overrode that value and as you said in GCC it ends up putting I here in Clang it ends up putting C here or the other way around whichever is the yes yes. So, because in Clang C gets stored here in GCC I get stored here. So, the memory layout which means that which variable will come where depends on the compiler. The compiler does not guarantee that I will store things in a certain way it is completely up to the compiler to swap things around as it wants right. So, the example showed you two things one is that buffer overflow is dangerous and these errors are extremely difficult to catch because the bug was not in array the bug was in the error was in the value of C which has nothing to do with array at all. The array initialization what happened never got even used anywhere and whatever was used to compute the final value was C and you see a wrong value in C and you try to debug the call chain of the dependencies of C and you will not find anything there because nothing is happening anywhere right. So, buffer flow flows are very difficult to debug because it is interference between like different memory addresses it is not that that something wrong is happening to that variable that I can easily detect that is one thing. Second thing is that do not trust the memory layout of a compiler it is not that a compiler will layout memory in a certain order never trust that never write your code assuming that right. So, in certain cases for like very seasoned programmers often use such assumptions but then they know what can happen where but otherwise do not make such assumptions because a compiler is free to put allocate memory any way it wants. In fact, there are memory optimizations which actually say key how should I allocate memory which will give me the best performance right. So, the compiler the language gives the compiler full freedom to allocate memory in any way it wants right. So, the memory layout is completely up to the compiler do not trust that and it differs so that is why it was differing across compilers GCC and Clang were both showing different results and it is none of their faults it is the fault was with the programmer the programmer gave wrong answer did the wrong computation. So, now what I want to do is I want to detect such overflows buffer overflows and let us say I assume that it is an overflow by only one more word not more than one word. So, overflow can only happen for I will I can end up accessing a minus 1 but I will not access a minus 2 like I cannot make that bigger blender like a small off by 1 errors are very common errors right lot of times I mean instead of less than you put a less than equal to right such things often happen right I mean so these off by 1 errors are very common. So, let us say if I do create a problem I will create a problem by only one word but I will not create a problem of more than one word. So, let us just simplify our case by taking this assumption. So, under this assumption how can I write a dynamic analysis which can detect memory overflows buffer overflows any ideas. How would I know what are valid addresses that like keeping track of water value addresses needs almost as much space as you are already using you see what I am saying right for so you have to keep some sort of shadow memory. So, for every memory here you have to remember that is it valid or not valid excellent idea excellent idea because I have this assumption that I will probably at most overflow by one word what I can do is instead of laying out things one after the other whenever I am laying out one word instead of laying out one word I will put a buffer here and put a buffer here. Buffer means another extra word here and an extra word here. Oh yeah you have to do it you are doing extra computation but this is not that much as like putting a like completely duplicating your full memory you have to use extra memory because you are doing extra computation right even here we had this extra memory right we had to maintain this table. Do you see this so we put a buffer a word before and after every so before the array also we will put one array one word before the array and one word after the array maybe we can just do it for arrays just to save memory let us say other variables I do not care overflows can only happen on arrays I will just do it for arrays so before every array access I put a word after every array access also I put a word. Now what I want to get a runtime error when I hit that. Which part the hash part but so you have to save very good idea so you have to save save something in this hash part. Excellent I can put some weird bit pattern let us say 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 some weird thing I write there so all these guys I write the same business right I write the same bit string in all these locations right so there is a 1 by 2 to the power 32 chance that I will hit that accidentally but I will take the chance it is a large chance but still I will take it right so I will find so I will store all of them with this and then how do I what extra code do I add tricatch to give me more simpler answer so because I will go to instrument it you have tricatch I need to put it in LNB bytecode right. So whatever you tell you have to implement it in LNB bytecode. Which value where so what exactly how will I exactly implement this but A of I may not be the right thing because A of I you will not get A of I in LNB and moreover there can be other problems there can be a P is equal to A plus I and you say star P star P is equal to 5 then what it may not look so neat. Before any dereference we will check the. Yes before every dereference you check so every time you get a dereference says some star P so you check if star P is equal to equal to that special address that special character the special the bit string that I put then flag error. Else you do the star P and continue whatever you are doing. Does it make sense completely clear. You should not write those programs you should not write those programs which uses A minus 1 to get updated please do not write those programs. I mean other variable which is stored at A plus 1. No first of all how do you know what variable gets stored at A minus 1. As I said it is completely up to the compiler to place anything at A minus 1. A minus 1 is an address that you should not use you are not allowed to use that. That is the first thing second thing is even even programmatically it is a bad practice to like so the memories each memory symbol that use every variable that use is supposed to be not dependent on anything else. Like address wise like x y z are three different variables you should not like if there is an array of 10 bytes and then there is a integer C. So I should never use like some location some operation on A to update C. I should not do that right. I have C why not use C you want to set C is one set say C is one. Why do you want to say A minus 1 as one right. So no no so this is only for the analysis in your actual code when you actually run the code or when you actually deploy the code it will not have this instrumentation because this is expensive right before every access every memory access I am doing a if check I am putting a check. So that is lot of code I would not ship that product with so much code. It is only to do the analysis it is only to do the analysis I use this extra space and this extra instructions to say that let us try to let us see if I can trigger that fault. Yes yes exactly so these problems can be on the stack this can be on the heap solution is are similar right if you have it on the heap what do you do whenever you do a malloc instead of using the systems malloc you use your own version of malloc you have your version of malloc which puts this extra code extra locations before and after the malloc block right and your own free which will instead of saying whenever it says free a particular address it will say free minus my address like whatever some are supposed to be so the user wanted this much buffer but you give it this much buffer this extra thing is yours and when the and but to the user you will return this address user will not even know that there is nothing there right and whenever the user says free of this you will go and say free of this so you will have your modified versions of the malloc and free calls and then you will you will for every memory access you will you will check it if it is it could be a error in this location it is one of these locations that can be faulty and if yes you trigger fault so all these memory checkers like address sanitizer or well grind mostly the technique is this they have more sophisticated algorithms they are which are much faster give you more information but the high level idea is sort of this right so now can we get our hands dirty can we try building such an analyzer in LLVM so what does it take now let us come to what theory again now let us come to how do we build it in LLVM let us say I want to do it on the stack right so what do I need to do let us plan it I want to only do it on the stack I do not want to do it on heap so how will I do that in in LLVM bit code LLVM IR yes so I have to do what insert aloka instructions so like I have to look at the aloka instructions there and after each aloka instruction or before each aloka instruction I add my own aloka instruction right so this will give me extra space to for this extra variables that is one phase this is insert aloka then not store instruction so the dereference is done with something called this GEP instruction gate element pointer instruction right so there is this gate element pointer instruction you have to catch all such GEP instructions right for every such instruction you have to check if this is equal to that pattern whatever pattern you have set here so there is aloka instruction and you also have to maybe that is what you are saying store a special pattern into this aloka instructions in these locations yeah you can sure sure sure but my implementation I did a shortcut so I am telling you what I did I did not want you to see that so yeah you are right so maybe I can do it only for arrays I can just do it before and after an array but that requires me to find the type of the array and I should do that yeah that's true but I just essentially did it for every aloka instruction just find if it's aloka put another aloka in between but you're right I mean we can maybe we get only do it for arrays it saves a space and whenever I have a GEP instruction you look at this and you make the check you check if the address at that location held at that location has is that special bit pattern that you stored here that you put in the store instruction if it is then trigger the error if not you are good yes yes so now you have the tool you will run that pass that will do all the instrumentation required you run the program on it it will trigger the fault if there is any and then I'll show you the code how we can get it done and then maybe you can um so we'll see how can we use the rest of the time so then we can spend a little time in you like tinkering around with that code and try to see if you can do something