 Okay, so let's see what we are going to do. My name is Govind Rajan. I am from Indian Institute of Science, Bangalore. My research work is primarily in computer architecture and compilers. And I do things which are pretty much in the backend of the compiler, what we call as the backend of the compiler. And you would have seen a lot of things about the front end of the compiler in the last 7, 8 days, correct? So what all have been taught in the front end for you? You have to keep this class interactive, right? That way it will be useful and I get to know more from you and then you can also learn something from me. So you have been taught about the front end of the compiler like the lexical analysis and the semantic analysis and so on. Have you also been taught about data flow analysis, intermediate code representation? Okay, so what we are going to see is probably more towards the backend of the compiler. So let's see what we are going to see. But before we see that backend of the compiler, so let me just tell you what is the overall plan for the next 3 days, right? So before we see many of the things which are relating to code generation and machine dependent optimizations, I would give you a little bit background on machine architecture. Most of you would have done a course on computer organization or computer architecture. I will try to kind of revisit or review some of these concepts which are needed from the purpose of the compiler because after all we are generating code for a specific machine architecture and all the optimization that we are going to be talking about are optimizations which are machine dependent. So therefore we need to have a good idea of what this is. So the first lecture is going to be more on the machine architecture. Subsequently I will talk about code generation, right? How to generate actually the machine code. But we will not really talk about it as generating machine code. We will talk about it as generating assembly instructions for a specific machine. From assembly instructions to machine code, it would be done by the assembler, right? How many of you have heard about an assembler? Have you written a part of the assembler? Have you people? No. No. Okay. That is actually a simpler task, much simpler task compared to writing a compiler, okay? So we will not really see how to write an assembler or how to do that part. But let us assume that that can be done in a easy manner, okay? So what we are going to focus from the second lecture onwards, right, till about today evening or maybe some part of it even tomorrow is all about code generation. Then after that as a part of code generation in fact we will also talk about how to do instruction selection because there are multiple ways in which a given intermediate code can be, a code for an intermediate code can be generated. So we will see what is the best way to do it, how to do instruction selection for that, what instructions are more appropriate than what other instructions and so on. Then we are going to spend again another two or three lectures on what is called register allocation. So this is an important phase in the later part of the compiler where in the machine code some instructions would use registers. If it is a RISC machine then all instructions would use registers. But if it is a RISC machine then they can use registers or memory operands. So we will have to see when they use register operands what registers are assigned to operands, how are they assigned and what is the register allocation policy and so on. So we will spend time on that. Then subsequently we will also talk about instruction scheduling which is essentially reordering of machine instructions. In fact in the compiler this instruction scheduling and register allocation phase are done the other way around. First typically instruction scheduling is done then register allocation is done but for the lectures we will actually see it the other way around. Of course it is possible to do register allocation before instruction scheduling. There are some pros and cons of that. We will also talk about that and we also talk about what is the interaction between register allocation and instruction scheduling. Lastly we will talk about certain machine specific optimizations, right? Something like optimizing for memory hierarchy or optimizing for vector parallelism that is available in your architecture and so on, right? This will be towards the end of this program, maybe towards the last one or one and half lectures on that, okay. So that is really the plan for the next three days, right? And what I want to emphasize here is that please feel free to ask questions, right? The idea is not so much to cover the material, it is more to make sure that you understand some parts of it. That is very important. So if there is any difficulty or trouble please feel free to stop me and then ask questions. I may not be able to answer all the questions but at least we can try to sort it out one way or the other, okay. So that is really the idea. Are you all comfortable? Okay, did you have a good sleep yesterday night? Right, okay, good. So feel free to interact with me and discuss. So this is the overall plan for the three days. What we are going to start off is start off with machine architecture, right? As I mentioned earlier this is the part of the machine that we want to understand so that we can generate code for that. I am sure all of you would have done what I am going to teach in the next 45 minutes to one hour, okay. But I am going to reemphasize or go through that one more time so that the terminology that we are using is kind of well understood, right? And there is no confusion on what we are talking about, okay. So as far as the machine architecture part is concerned I am going to do it in two modules, right? One module I will do it today but this is the overall overview of both the modules. I start off with some simple introduction. Then I will talk about instruction set architecture because for generating code you need to understand what is the instruction set architecture. That is your interface to the architecture. So you need to understand that. So let us talk about that. Then subsequently we will very briefly talk about processor data path and pipelining because I want to introduce the notion of hazards in a pipeline and how compiler can help to overcome or reduce the impact of these hazards. So we will talk about that briefly so that that concept is also understood, okay. And then subsequently I will talk about instruction level parallelism because when you talk about instruction scheduling, instruction scheduling is essentially a technique to expose instruction level parallelism to the architecture. So in order for us to understand that we will very briefly talk about instruction level parallelism processors, right? And I will stop with that for today, okay, as far as the machine architecture module is concerned. The next two topics I will probably take it up on Friday because they relate to optimization that we are going to discuss on Friday. So I will probably put a pause after we finish instruction level parallelism and then do memory hierarchy, right, on Friday. Again we want to understand how the cache architecture, right, works, how locality in cache architecture helps you to improve performance because sometimes you need to generate efficient code, not sometimes you need to generate efficient code from the compiler which understands the cache architecture and generates appropriate code. So what are the optimizations that are possible? We will talk about that and in order for us to talk about that we need to understand the memory hierarchy well. So there is a very quick primer on memory hierarchy that we will do later on Friday and we will also talk very briefly about data parallelism so that we can understand how to generate code for exploiting data parallelism, right. So this is the overall overview of this module on machine architecture. We are going to cover the first five topics today and then put a pause on this, go to code generation, register allocation and then instruction scheduling and then after that we will talk about memory hierarchy and data parallelism and then come back to optimization that the compiler can do for exploiting those things. Is that plan okay with everybody? All of you are fine with that? Okay so the next 45 minutes or so we are going to go into the details of machine architecture. Bear with me if you all understand this well. I am sure that you all do that. You are all, you must have all completed six semesters of your courses, right. Everybody has completed six semesters. Everybody has completed the BTEC? No, right. Everybody is in the third year, right. Some people from Nvidia also, okay. All right. So how many of you are from Nvidia? Be careful when I talk about Nvidia process. Okay. I am just kidding. I am just kidding. Okay. So three of you are from Nvidia. How many of you have completed your BTEC? Oh, do not worry. I mean, of course, you can go on to this module. Okay. So let us see. Okay. How many of you understand this diagram? I am sure that all of you should, right. You have no doubt on that, right. But again, we will just quickly go through the various aspects of basic computer system. What you see on the left-hand side here is what we call as the central processing unit, right. And the central processing unit has an arithmetic and logic unit which is responsible for doing all the computation, right, arithmetic and logic operations. The control unit is responsible for controlling all the parts of the central processing unit. You also have something called registers. Registers are temporary storage locations within the processor, right. And of course, you have memory and the processor accesses the memory through this bus. There are also input-output devices which we do not really worry about in this lecture on compilers. What else is there on the CPU which I have not listed here? Yeah. Feel free to talk loudly, right. There is nothing right, nothing wrong about. Sorry? Power unit. Power unit, okay. Power is somewhere outside. It is not as a part of the CPU, right. Power comes into the CPU. What is the CPU? You are talking about the whole CPU. Oh, no, within the CPU, within the CPU, central processing unit. So the central processing unit is the processor, right. So this is the x86 processor or whatever processor that you have. That is the one that you are talking about, right. What about caches? Where are they? Right. They also have to be inside that, okay. Yeah, sorry, one too many, okay. So you see the caches there. And what is this MMU which is the memory management unit? What is the memory management unit used for? Logical address to physical address, address translation purposes, right. Again from a compiler perspective, this may not be very relevant, okay. But these three components are very relevant, the arithmetic and logic unit, the registers and the cache, okay. Again, cache, we are going to see it as a part of the memory hierarchy because you typically have an L1, L2 or L1, L2, L3 caches before you could go to the memory. So the code that you generate should essentially make use of these L1, L2, L3 caches in a very effective way, right. And of course you see these registers. Where are we going to talk about that in the compiler course? Register allocation, right. Okay, so many of you have done the computer organization course. So you at least have a rough idea of how the caches work, correct. Who manages the caches? You as the programmer or compiler, operating system. Somebody said hardware, yeah, right. How many of you think it is operating system? Okay, no, right. The caches are not managed by the operating system in the sense that operating system is not the one which intervenes and puts the data in the cache, right. It is the hardware that given an address, it automatically fetches the data and puts it into the cache, okay. Again we will talk a little bit more about this when we talk about the memory hierarchy, right. The reason I ask this question and I am trying to go over this is essentially to make sure that there is no misunderstanding of some of these things. So do not feel bad, okay. Those wrong answers in fact clear out the doubts of some of the other people also. So nothing wrong about it, okay. It is better that we clear our understanding, okay. So this cache, essentially the data movement from memory to cache is controlled by the hardware. Yes sir. Sir, how does some part of the area of cache and therefore the memory can be created for the cache? Sure, sure. Sometimes when certain things are being flashed, it instructs the hardware to clear some parts of the cache. All I am talking about is that who manages the data transfer between the memory to the cache, right, when a program is executing. It is primarily the hardware, okay. What about the data transfer from memory or cache to the registers? Who manages it? Is it explicit in your program? Is it implicit in your program? See for example, transfer from memory to cache is implicit in your program. You as a programmer do not necessarily say this stays in the cache or this does not stay in the cache to a large extent, correct. You do not necessarily say move this data from the memory to the cache. You do not do that, right. Whereas what happens with regard to the registers? It has to be explicit. It has to be a part of your program. Well, it is not a part of your high level program, but it is a part of your lower level assembly level program or machine code, correct. That means that the compiler is the one which is responsible for doing this data transfer. So this part is going to be orchestrated by the compiler. Movement of data from the memory or caches to the register is being orchestrated by the compiler, okay. All right. So again when you talk about registers, there are two types of registers. Integer registers for storing integer values, floating point registers for storing floating point values. In addition to that, there are also some special purpose registers, right. And you as a programmer or a program may not have, may not use these things as general purpose registers. That is why these are called special purpose registers. An example of this is the program counter, right. Program counter is the one which points to the next instruction that has to be executed, right. And similarly instruction register is the one which holds the instruction which is being executed. There could also be other registers like stack point, frame point and so on and so forth. Or those registers again depending on the architecture could take one of those general purpose registers and specifically use it for that particular purpose, okay. So essentially we have a set of registers, some of which may have a specific usage, some of which is general purpose. When you say it is general purpose, what it means is that you can load any temporary value to one of those registers and you can access it from. That is really what it is, right. How is this loading going to be done? Compiler will generate a code which will make sure that the data is being loaded into the registers and is being accessed, okay. All right. So why do we talk about registers, caches and memory? These are all storage places. What is the difference between them? Access, access time, correct, right. So registers are very very fast, right. They are sub cycle access time, right. So the cache again depending on the level it could be from one cycle to ten cycles to twenty cycles, processor cycles. Memory could be hundreds of cycles, right. So that is a memory hierarchy. Again we will talk more about this when we talk about the memory hierarchy, okay. So is this understanding of basic computer systems good enough for us? Yeah. What is the function of instruction register? Instruction register I will explain to you in the next slide is the one which is going to hold the instruction which is currently being executed, okay. And you need to hold that instruction because you need to understand the different fields of the instruction and accordingly perform the different operations or the processor has to perform the different operations, right. So it holds the current instruction, all right. Okay. So let us very briefly talk about main memory and there are certain concepts of main memory that we want to understand so that when we talk about either the instruction set architecture or generating code this understanding is very important or useful. Main memory is the one which holds instruction and data, correct. always access the main memory to access your instruction or data and this main memory is actually is organized as a sequence of locations. Each location is capable of storing some amount of information. Typically each location is capable of storing one byte of information that is 8 bits, right. And the memory itself can be accessed or referenced by an address which is what we call as the memory address, right. So when you talk about a physical address somebody talked about conversion from logical address to physical address that address that you are talking about refers to a specific location in memory and that location may contain one byte of information, right. If each location contains one byte of information then we call this memory as byte addressable, okay. And this is the minimum amount of memory that you can access or modify, right. So if a location consists of 8 bits, right, then you can take out all the 8 bits, put back all the 8 bits. You cannot just say, okay, I want to toggle one bit. Well if you want to toggle one bit you take out the entire thing, toggle that one bit and write back that entire 8 bits. That is really how you have to do it, okay. That is the minimum amount, that is the minimum value, right. It is like what is the smallest change that you can get. Today the smallest change you can get is probably what, 1 rupee, 2 rupees, right. So that is the minimum denomination that you are talking about. So you can think of byte as the minimum denomination, right, smallest amount of value that you can fetch or write back, okay. If your very location consists of only 1 byte, how do I talk about integers which are probably 4 bytes long or floating point numbers, right, single precision is 4 byte, double precision is 8 byte and so on, right. So that you can essentially talk about it as a sequence of location, right. You can say a sequence of 4 locations is 1 word, right and that 1 word could be an integer. A sequence of 8 locations is 2 words, right and that 2 words could be a float, sorry, double precision floating point number, right or you can talk about a sequence of 2 locations, 2 bytes. What is that? You have come across that in C, short integer, short, right. You have seen short, right and 1 byte is character, okay. So, everything falls in place, right. So, think about it that way, right. So, the memory is organized as a byte addressable memory. Each location of the memory has an address by which it is referenced to, correct and every location has 8 bits, alright and a sequence of 2, 4 or 8 essentially is a byte addressable memory or 8 essentially forms a short word, I mean 4 bytes is word and 8 bytes is double word, okay. That is really what it is. Okay, let us see an example of this in the next slide, okay. So, you can now think of this memory as this, okay. Each location consists of 1 byte of information. You can see 2 hexadecimal digits there, right. That is 8 bits, okay. And here I have taken some address starting from 400 onwards, right. What is stored in the memory is only the data. Address is never stored there, correct. That is something that we have to be very clear about. Address is not stored anywhere. You use the address, you decode the address to go to that location, correct. So, when I say 400, the byte in 400 is 1A. The byte which is stored in 400, location 400 is 1A, right. Okay, now if I want to access an integer at location 400, as I mentioned earlier it should be a sequence of consecutive locations and by convention we will say consecutive means increasing address, correct. 400, 401, 402 and 402, right. That means we are talking about an integer which is this, correct. Now, again we have to have one more convention to say of these 4 bytes, which one is the most significant byte and which one is the least significant byte, right. We are all used to reading from left to right. So, we will say that I like this to be my most significant byte, okay. But computers do not care, right. They are both left handed and right handed. I mean they can be both left handed and right handed if you want it to be, okay. So, if you say it is a big ndn byte ordering convention, right, the end is at the higher address, okay. So, 1a is the most significant byte, okay, c8, b2 and then 46 is the least significant byte, okay. This is what we call as the big ndn convention. The little ndn convention is the other way around, okay. 46 is the most significant byte and of course, b2, c8 and then 1a is the least significant byte, okay. So, each computer is going to follow one or the other, right. Do you know which one your computer follows, big ndn? Can you find that out? Can you write a program, C program to find that out? Just think of it. Can I write a C program? Do not write it, okay. No, no, do not, not that fast, okay. Is it possible to write it? What is the idea? Read the mass each one as a character and then do that. C allows you to do that. So, it should be possible to figure out whether your computer follows big ndn or little ndn. Very simple, right, okay, fine. You will not really see it. The processor is going to give you in a way that it is from the, if it gives you as 1a, correct, then 0001, the first 0 as the most significant bit in that particular byte, okay, and then a which is 1010, the last 0 as the least significant bit. How exactly it is stored in the memory you are not really going to see. So, it does not really matter. We will not talk about it, right, okay. Convincing? Okay. We need to also see a little bit more details about words alignment and so on. Let us talk about that, okay. So, typically we access an integer or a float and things like that which are 4 bytes long, right. In the computer what we are going to say is that anything which is like 4 bytes long we are going to call that as a word, right. And, okay, did I talk about, let me just go back a couple of slides here, then see whether I did talk about, yeah, okay, I did not talk about it, okay. So, we talked about what is the minimum amount of data that you can access from the memory which is what we call as byte addressable, okay. Then, in a single fetch how much of data can you access? That is what is called the word length, right. So, even though each location may consist of only 1 byte, a computer in a single fetch can possibly get up to 4 bytes together. That is what is typically called the word length, right, the maximum amount of data which can be fetched in a single location, right. And then because of the way in which the memory is organized, right, and the accesses happen, etcetera, etcetera, right, typically these 4 bytes that you try to access, right, will as 4 consecutive bytes only if it is starting from an address which is divisible by 4, right. By that what I mean is if you say fetch me 4 bytes starting from 400, right, it will fetch these 4 bytes, okay. But if you say fetch me 4 bytes starting from 402 which is an address which is not divisible by 4, right, depending on how the processor is, most of the process today can actually access this 4 bytes of information starting from 402. But what it is going to do is that whenever it makes an access, the 4 bytes that come together will always be these 4 bytes or those 4 bytes, okay, never these 4 bytes, can you understand, correct. That means that if you want to access 4 bytes of information from 402, the processor says okay I want to access 4 bytes of information, but I will split this as 2 accesses, right, one access with these 4 bytes and another access with those 4 bytes and then I rearrange them to give you the 4 bytes that you want, okay. So that is the importance of word alignment, right. So that is captured in the next slide, let us talk about that, okay. A variable, an integer variable which is 4 bytes long is said to be aligned, word aligned if it starts from an address which is divisible by 4 because if it is divisible by 4 that word can be accessed in a single fetch. If it is not divisible by 4, it will still fetch it and give it to you, but it might take multiple accesses. What is the problem with multiple accesses? More time, right, it is going to take more time to get that data. Therefore, you want your data to be word aligned, okay. Is there anything that the compiler needs to worry about word alignment? Immediately that is what the question is, this entire lecture is on compilers. So anything and everything that we talk about should be somewhat related to compiler. Otherwise you can say that I am wasting my time doing something else. Sorry? It will require more registers. No, we will come to that register part little later. This is from an allocation point of view. So whenever you allocate a word, right, the compiler has to make sure that this allocation is in an address which starts from an address which is divisible by 4, okay. We talked about allocation of variables, right. So all of you are familiar with C program, correct. So where are your local variables stored? Stack, right, local variables. What about your global variables? Data, somebody said data, who said data? Yeah, I said data segment, right, data segment, okay. Heap, what is stored in the heap? Dynamically allocated variables, okay, right. So let us particularly look at the data segment and the stack, okay, and these two are largely controlled by the compiler, right. So when you declare, let us say, an array of integers, let us say, right, in an integer array a 100, right, that means that there are 100 elements in the array, each element is 4 byte long, correct. If it starts this array in an address which is not divisible by 4, then every one of that right, is going to be, when it is accessed, is going to incur additional cost. So the compiler would be clever enough to start this array, right, from a location which is an address which is divisible by 4, right. But if I do everything divisible by 4, there should not be a problem at all, right. But in between I could have a variable which is short or character, right, and that could throw off what comes later, right. So whenever it says, okay, I am going to declare a large array or I am going to declare a set of integers, the compiler should necessarily make sure that they start from an address which is aligned. So it will actually do certain things so that it gets aligned, right. So that is the importance of that, okay. When you do dynamic allocation, you ask for certain amount of memory location and then again depending on what it is, the compiler would say that, okay, ask this as 4 consecutive bytes or whatever, okay. All right. So we talked about the word size which is the maximum amount of data, yes. Sir, you mentioned that it would always try to find out addresses which are starting from which are equal to the address. So wouldn't there be memory which is the first? Absolutely, okay. So let us try that you try to do the following thing, right. You write a program in which either in your local variable or in your global variable, you declare one short integer followed by one array of integers, again a short integer followed by an array of integers, short integer followed by an array of integers. Between every short integer and the array of integers, it will waste 2 bytes, okay. Do we really care about it? Probably not, right. Because you have 2 power 32 or 2 power 40 locations, right. Two locations does not really matter. Two locations in 2 billion locations does not really matter, right. The efficiency of access is important, right. Good, good, good question, right. Okay. So word alignment essentially refers to words which are aligned to the quad word boundaries, okay. That is very quickly what we want to talk about basic computer systems and organization because this understanding is important for all of us. Sometimes you know all of these things but would not have studied it in the formal course, right. So I thought that I would at least repeat some of those things, okay. Let us see. The next thing that we are going to talk about is instruction set architecture, okay. So instruction set architecture is essentially the view of the computer to the programmer or the view of the hardware to the programmer or the compiler, right. And you can talk about two different kinds of instruction set architecture, one called the CISC architecture and the other called the RISC architecture. RISC stands for complex instruction set computer and RISC stands for reduced instruction set computer, right. In a complex instruction set computer as the name suggests, right, it can host complex operations, right. Typically it will host operations which will involve operands from memory and registers or memory and memory depending on how complex the instruction is. Whereas in a reduced instruction set architecture, all arithmetic operations, arithmetic and logic operations can only have register operands, right. Only load and store instructions can have memory operands. There is certain advantage of designing a computer or designing a processor as a reduced instruction set processor, right. And in this reduced instruction set processor, instructions, okay, like add instruction or subtract instruction, etcetera, would only use register operands. That means it will only operate on values which are already available in the register. And therefore in this architecture, it is again very important that the register allocation is done very well, right. In the reduced instruction set architecture, one operand can be memory. This is typically what it is. Although in olden day architecture, even both operands can be in memory. These days complex instruction set architecture support only one instruction in memory, okay. The other instruction can be in or should be in the register and you have fewer registers in the complex instruction set architecture. You have more registers in the reduced instruction set architecture but fewer instructions in the complex instruction set architecture. And therefore there also it is very important that the registers are judiciously used. So register allocation is essential and important in both of these architectures, okay. But here we are going to see more about the instruction set architecture and instruction set architecture essentially deals with, right. As I mentioned earlier, it is a description of the machine from the view of the programmer or the compiler, right. If I am a programmer and I do not want to know anything about the hardware, right and if I have to program in assembly language, then minimally I need to know about the instruction set architecture. If I am a compiler writer, right but I do not want to really understand how the processor works but still generate code, I still need to understand the instruction set architecture. It essentially includes the set of all instructions which are supported in the processor, right. And in each of these instructions, how the operands are being specified, that is another important part which is what is called the addressing mode, okay. And of course there is something called instruction encoding which is only required if you are going to take these instructions in the assembly language and going to convert it into machine code. This instruction format also helps you to understand how instruction execution happens, okay. That is essentially what it is. So instruction set architecture essentially comprises of the set of all instructions that are supported by the processor, the addressing modes that are supported by the processor and also how the instructions are being encoded, right. So this is the separation or the so called interface between the hardware and the software or the hardware and the compiler, right. If the compiler does not want to know anything more about hardware, minimally it should know about these things without which it cannot necessarily generate the instructions, okay, generate the code, yeah. Why there are less instructions? Why there are less instructions? Typically CISC instruction set architecture has a lot of complex control and other things. So the space that is available and they want to utilize for registers is actually limited. And second thing is that again when I say limited you do not think of one having tens of registers and another having millions of registers, no, right. In a CISC architecture you typically have anywhere between 8 to 16 registers or maybe 32 registers. In a RISC architecture you will have 32 to 64 or maybe 128. We are talking about a factor of 2 or 4. But that is essentially the difference and this is again depending on the available silicon space and also based on the requirement. Okay, interestingly when we talk about code generation, right, many of the old books in compilers do talk about code generation for CISC machines. So we are going to talk about code generation really for CISC machines. But the concept is general. It can also be extended for RISC machines. But all the examples that we are going to see are code generated for CISC machines, okay. But if you think that someday somebody is going to build a CISC machine with, today actually CISC machines have up to 32 registers. But if you want 64 or 128 registers, it is not that it is not doable, right. It is not significantly difficult. It is doable but maybe it does not have that much utility. That is the reason why they do not do it, okay, right. Okay, let us move forward. Okay, so as far as the set of instructions are concerned, we can divide them in terms of arithmetic and logic operations. We can divide them in terms of data transfer operations where you try to move the data from memory to register or from register to memory or you talk about control operations. Control operations are essentially jump instructions, branch instructions, conditional branch instructions or function call instructions and so on and so forth, right. Of course you have other kinds of instructions like ALT ALT, right. Okay, so let us see specifically an example of this instruction set little later when we talk about the instruction set for the RISC-V architecture. But before that let us just talk a little bit about operand addressing modes because all instructions are going to have operands. Operands are the ones in which these operations are going to be performed. So how do you specify these operands as a part of your instruction, right. So that is essentially what is addressing mode, okay, addressing mode specifies how to specify these operands. You have both source operands and destination operand, right. There could be one or two source operands and typically there is a single destination operand, okay. So as I mentioned earlier addressing mode is essentially how do you specify the operand and the operand itself can be in a register or it could be in the memory, right. The first one is we are going to see little bit more detail of when it is in the memory how do I specify the operand. If it is in the register of course I can specify the register number that itself is in R. There is something which is in between which actually typically falls in this in memory kind of a thing. For example you could have small constants as a part of your instruction, right. Add R1 with 1, right or subtract 4 from R2. Those are all small constants, right. That constant is called immediate constant and that is a part of the instruction and since it is a part of the instruction, instruction is in the memory, it is actually in memory. It is a part of the instruction but it is in memory. So we will also talk about those things as we talk about operands, okay. Now the last thing is also very important. We are also very important from the perspective of code generation, right. Some machines use a 2 address format, some machines use a 3 address format, okay. Typically RISC machines use what is called a 3 address format. Here you can see examples, right. For example when I say add R1, R2, R3, right again the meaning of this instruction is that add the contents of R2 with R3 and put the result in R1. So I specify the destination and the two source operands explicitly in my program or in my instruction, okay. Whereas in a 2 address format, okay, I only specify two operands, okay. One of them is the source as well as the destination. Which one is the source and destination is by convention. Typically we will assume that when I specify subtract R2, R1 it is R2 minus R1 going back into R2. Sometimes you can also say R1 minus R2 going into R1. It does not matter. It is a convention and depends on the processor which way it is being written, right. But the main difference here is that here you explicitly specify all the three operands whereas here you specify only two operands. One of them is the source as well as the destination, right. Sys instructions are typically of this form. Risk instructions are typically of this form, okay. And in a Sys machine you could also have memory operands as a part of your instruction or as a part of your arithmetic operation. So for example here this says subtract the contents of the memory location A from R1 and then put the result back into R1, right. That is what it says. Again when you talk about a specific processor we will specify whether this is the destination or this is the destination, okay. That is a convention of that particular processor. We can always follow that. We have to follow that consistently, right. And then when you have operands like subtract which are not commutative it is also important which is the first operand and which is the second operand, right. So when I talk about the instruction set architecture I will use the risk instruction set architecture. I will typically talk about this and there we will say that R1 is my destination register, R2 is my first source operand and R3 is my second source operand. When I talk about this construction I will use either one of these formats, okay and typically say that the register is my destination operand, okay. Any questions? Good so far. Okay. So let us talk about the addressing modes, right. There are several addressing modes. Again a processor may support a subset of these addressing modes, okay. Now let us see some of these addressing modes. The first one is the register direct addressing mode. In the register direct addressing mode the operand is in the register, okay. So here is the example. You say add R1, R2, R3. The contents of R2 is added with the contents of R3 and is being stored in R1, okay. The two source operands and the destination operands, okay. So here is an example. It actually goes through. So what should go into R1? So if you assume that these are decimal numbers then it should be 59. I hope it goes there, okay. Sounds good, right. But of course all of this is going to be stored in hexadecimal, okay or binary. Okay. Let us talk about the next addressing mode which is what is called the immediate addressing mode. In the immediate addressing mode the operand is a small constant and is typically available as a part of the instruction. Either you write it just like a number like this or you put a hash sign and then write the number. Again depending on the convention that is followed in the particular assembly language, right. And the idea here is that you add the contents of R2 with the value 1 and put the result back in R1, okay. Let us look at the register indirect addressing mode. In the register indirect addressing mode the operand is in the memory and the address of the operand is in the register, correct, okay. So here is an example, right. So we say R1 hash 32, R2 hash 100, okay. And you have the values in various memory locations starting from 196 onwards, okay. Here you are performing an operation. Of course this is not typically what you see in a RISC machine because RISC machine cannot have arithmetic instructions with memory operands. But just for the sake of example let us see what this means. This essentially means taking the, oh it already went, okay. So taking the contents pointed by R2 which is here, okay. Let us say that in the 4 bytes you have a value 10 and you want to add it with R1 which is 32 and put the result back in R1. So 32 plus 10 which is 42, correct. So the location that is being accessed is 100 which is pointed by R2. Where is this addressing mode useful in C programming? Typically pointers but all memory locations are going to be accessed in this mode in some sense or the other, right, okay. Let us talk about another addressing mode which is a slight variant of the register indirect addressing mode which is what is called the displacement addressing mode or the base displacement addressing mode. This mode is very similar to the register indirect addressing mode except that you also have, let us see, my animation is slightly out of phase, okay, there you go, right. So except that it is an indirect addressing mode but there is a small offset which is also there. So in this case the memory location which is being accessed is the one which is pointed by this but offset by the thumb out. That means that you have to add the contents of R2 with this offset value and whatever is the location that is the location from which you will fetch. So if R2 is 100 and the offset is 4, 100 plus 4 is 104 and the contents of that location is 35, that value is going to be taken with whatever value that was there previously which is 32, right, 32 plus 35 is 67, okay. That is really what is being put there, okay. So what is that this offset value can be both positive or negative. It could be or it need not have to be. If I am accessing a character, if I am accessing something else, right, depending on that it could be anywhere, right. Do not always assume that you are going to access integers all the time. You have characters, strings and may various other things in your program. String is basically array of characters but yes, right, short integers which are two bytes long so. So the offset can be any value positive or negative but it is typically a small constant. In some machines like the MIPS architecture it is 16 bits long, 16 bits to complement representation. In the RISC it is actually a little less than that. You will see what it, RISC 5, okay. Before I move forward, can anybody recognize where this will be useful, offset plus register in your C program? Array, array is one but there is something else also, structures, right. Structures of various fields, right. Each field can be offset by certain amount. That is the place where this would be more useful. Arrays typically you do this register indirect and then keep incrementing it, okay. There is a special addressing mode which is called the auto increment addressing mode which yes over here which is typically used for accessing arrays if that addressing mode is supported. Otherwise it is the indirect mode and then you increment it or you decrement it, okay. The other addressing modes are something like absolute addressing mode in which the address of that location is actually specified as a part of the instruction. This typically happens only in CISC architecture. In CISC architecture you can actually have instructions whose length is variable. So some instructions are 2 bytes long, some instructions are 4 bytes long, some are 8 bytes long, some are 12 bytes long and so on and so forth and they can afford to have even the address of the memory location as a part of the instruction. So this is called absolute addressing mode, right. Then you could actually have a combination of registers, right, R3 plus R4. That is also possible that stores the address of the location. The added value, right, is the address of the location from where the operand is going to be fetched. These are all more and more complex addressing modes. Typically a CISC machine would support, right, the register direct addressing mode, the immediate addressing mode and the base displacement addressing mode, only 3 addressing modes. Whereas a CISC architecture might support this, this and maybe all of this, okay. That is what a CISC architecture may support, right. And if these modes are supported then when you are doing code generation you have to be aware of this and you have to generate an appropriate code. And when I say appropriate code I mean efficient code, right. That is important, okay. We will see what efficiency is. Efficiency is in terms of the generated code being able to execute in fewer number of cycles, okay. That is really what we want to look at. Any questions on the addressing modes? What are all supported by risk? Risk is a generic category, right. So you should not be asking the question. In that way you should ask what are the instructions supported by let us say my power 9 processor or what are the instruction sets that are supported by let us say the MIPS or 10,000 processor, okay. Answer is very simple. Go and check in the website. Go and check in the internet you will be able to figure that out. Typically all risk architecture support register direct displacement, base displacement addressing mode and immediate addressing only three, okay. That is what MIPS support. Risk five you will see what it supports, okay. But in general if you want to know for a processor like power 9 processor you should go and check the manual of power 9. That will tell you, right. Say if tomorrow you have to generate code for power 9 all you need to do is to go and then get the instruction manual for power 9. Look at what are the addressing modes? What are the instruction sets? Blah blah blah and then understand that generate code for that architecture. What about x86 architecture, right, which is what we are all familiar with, right. It will support it is a complex instruction set architecture, right. It will support some of these addressing modes as well. You should again look into the instruction manual and then figure out what all it supports, right. It is fairly complicated and hopefully we do not have to look at that but let us see. But nothing scary about it. Okay there is also another interesting addressing mode called PC relative. Can anybody tell me what this PC relative is? Absolute. It is a part of the program or code where the data is available preferably following the instruction, okay, right. An immediate constant the data is available as a part of the instruction, correct. So some 16 bits of the instruction is the operand, right. The case of a PC relative addressing mode I can say that the operand is in the next four locations that is PC plus 4, right. That is possible. Again only CISC machine support these kinds of addressing modes. Now let us look at the RISC-V instruction set architecture, okay. RISC-V instruction set architecture first of all it has 32, there are multiple versions of the RISC-V instruction set architecture. We are going to talk about the integer instruction set version of that, okay. It is what is called 32I or something like that. So this has 32, 32 bit general purpose registers R0 to R31 and R0 always has a value 0. It means that no value can be written into it. If you want to get 0 as a value then you can read R0. It will give you the value 0, right. Many of the RISC architectures have a hardwired 0 register for various purposes, okay. They may not have a MOV instruction, MOV from one register to another but then if you do ADD with this R0 register that is like a MOV, right. ADD R3, R2, R1, R2 is being copied into R2, right. That is the reason that is one way by which this 0 register is being used, okay. Also for comparison purposes and so on. The addressing mode supported by it are only these three, this PC relative is only for branches, okay. So immediate, register direct and base displacement, okay. This is a RISC instruction set. So all arithmetic instruction should only have register operands or immediate operands, R register and immediate operands. And load store instruction should have the base displacement. We will see examples of these instructions as we go by. Then for branches it is PC relative, okay. That is whenever I do a jump or a branch or conditional branch it is with respect to the current program counter wire. So what we are going to see is what is called the RV32I instruction set. There are different versions of this. By the way what is RISC-Pi? How many of you know about RISC-Pi? One. What is it? It is a open architecture, right, okay. And it has multiple versions that are defined, okay. So we will talk about the R32I version, okay. Of course this is the 32-bit version. The corresponding 64-bit version is also there. And there are also many other variants as well, okay. So in RV32I each register is 32 bits long. That is what we saw. It is a load store architecture. That means only, I mean load and store are the only instructions which can have memory operands, okay. And again we use the same terminology. 32-bit means a word. 16 bits is a half word or a short. 8 bits is a byte, okay. Because why do we need these things? Because your instructions when they are moving data you have to specify on how much of data it is getting operated on, okay. That is the reason. We will see examples of this as we go by, okay. As I mentioned earlier displacement and immediate, okay, in this architecture can have varying size. That means depending on the instruction format it can have a 10-bit or 12-bits or whatever it is. You will see examples of this as we go, okay. So the load and store instructions which are what we call as the data transfer instructions, okay. They typically, sorry, they typically have a format something like this. It is either a load byte or a load byte unsigned, load half word, load half word unsigned or load unsigned immediate or a load word, one of this. So when you say a load word it is typically 4 bytes. It is always 4 bytes. If you say a load half word it is 2 bytes. And the instruction has a format. This x2 is a register. x3 is also a register. The address is available in x3 with an offset of 4. So x3 plus 4 is the address of the memory location that is being accessed and put into x2, okay. Similarly for the store instructions, right, in this case x4 minus 8 is the address of the location to which the value available in x2 is being copied into, okay. Similarly you can think of what the load byte or load half byte, half word would be, okay. It is either 2 bytes, 1 byte or 2 bytes, okay. Arithmetic instructions, okay. These are the arithmetic instructions. Arithmetic instructions operate on register. That means the entire 32 bits, okay. And you can have add, add unsigned, add immediate, add immediate with unsigned and so on. Similarly for subtract logical operations and so on and so forth. I will not go into all of these operations, okay. But again you can see that when you write add x1, x2, x3, contents of x2 and x3 are added and put into x1. Similarly if you can do a logical ORing or you can do a shift, okay, and you can specify the shift amount and which register is being shifted and where is the destination register, right. These are comparison operations, comparison operations compared to operands, okay. And then depending on that set the destination register either to 1 or a 0, right. Any questions? What is the difference between unsigned and normal, okay. Very good. So you are a C programmer, right. Do you use unsigned anywhere? Certain variables can only have positive values, right. They are declared as unsigned and they are unsigned, right. And when you do arithmetic in unsigned, there are certain things that you follow, right. As simple as that. You need to support unsigned. All addresses are unsigned, correct. All addresses have to be unsigned. Oh, okay. Let us say that, yeah, let us look at. See you see this unsigned only for bytes and half words, right. There is no unsigned for this fellow, right. So when you take a byte and then load it into a register, right. So 1 byte 8 bit of information is being loaded in, right. And if you say it is an ordinary load, then that most significant bit in that byte represents a sign and the number is sign extended to your 32 bit register, right. If it is an unsigned, then it is 0 extended in your register, got it. How many of you understood that? Yeah. Any trouble in that? Okay. So the thing is that when you load a value, let us say an 8 bit value into a 32 bit register, the 8 bits are copied as it is. There is no problem. Is this number positive or negative? Because you are talking about an 8 bit quantity to define whether the number is positive or negative, you should look at the most significant bit in that 8 bits, correct. If that is 1, right, then it is a negative number. Now if you copy it into a 32 bit register, only copying those 8 bits and keeping the remaining 24 bits as 0, then what happens to the number? It has become positive. Is that what you wanted? Probably not, right. So if you want all of the leading bits to be 0s, then you should load it as unsigned. If you want all of those things to be 0 or 1 depending on whether the original number was positive or negative, then you should load it as byte, okay. That is the difference, right. Comfortable? Move forward. Okay. Again, I have listed some of them and I have given some examples, but if you go through that you will find out. For example, this is a shift write instruction. So the contents of x2 is being shifted right, okay, sorry, I am sorry, right is that way, shifted right, okay, and 4 bits are thrown out and in that place, 0s are being introduced, okay. That is essentially what is being specified in here. You can see that you know bit 4 to 31 are kept, right, and 4 bits are being added, blah, blah, blah. Right, that this is a shift, right, okay, fine. Let us move on to the next set of instructions which are the control transfer instructions, right. Control transfer instructions are required to implement jumps, okay, conditional jumps, error jumps, okay, procedure calls and so on, okay. So typically these are branch equal, branch not equal, branch less than, greater than, blah, blah, blah, and the meaning of this is that for example, if I write branch less than x2, x3 immediate, then if x2 is less than x3, then I have to branch. At that point in time, a program counter has to be my program counter value plus this immediate value. But typically this program counter value plus 4 plus immediate value. Why is it? Because every time as soon as the instruction is fetched, the program counter is going to be incremented to the next instruction. The next instruction is always 4 by, each instruction is 4 by its long. So the next instruction will be 4 locations away from the current location. Therefore it is PC plus 4 which is the next instruction plus that immediate constant value which is being added that tells you the new program counter value. And that value is taken only if this condition is true. If the condition is not true, it will only be PC plus 4. That is a conditional branch instruction. This instruction is a jump and link instruction. This is meant for, sorry somebody said something, yeah, function calls, right. So RISC-V supports two jump and link instruction, one called jump and link, other one called jump and link register. They have slightly different semantics. So look at them carefully. If you are using the jump and link instruction, you are going to jump to the location which is PC plus immediate. That is where the called function is, right. The called function is in PC plus immediate location. That is where you are going to jump to. But before you jump, you save the address of the next instruction. What is that address? It is a return address, right. After the function call, you are supposed to come back there. That you save it in this register, okay. So you save the return address in this particular register and you jump to PC plus 4 plus immediate. Sorry, actually here it says PC plus immediate to that location, right. That is really very good. If it is a jump and link register instruction, right, then it is actually X3 plus immediate, okay, where X3 is one of your registers. That is your location to which you are going to jump to. That means that the function which is being called could be far away and its address is available in X3 register, okay, plus an offset which is an immediate constant, okay. That is really what it is. And of course, as before PC plus 4 is which is the return address is saved in X2 register, okay. This part of it is essentially to say that the least significant bit is reset to 0, okay. It is because in RISC-V, each instruction can be minimally 2 bytes, but most of the time it is 4 bytes, okay. So it always sets the least significant bit to 0. Does that make sense? Why? If each instruction is 4 bytes long, what do you expect the last 2 bits to be? Each instruction is 4 bytes long. Each instruction is a word, right. So what would be PC, PC plus 4, PC plus 8, PC plus 16, etc. What would the last 2 bits of that address be? It is word aligned. Word aligned means divisible by 4, which means last 2 bits should be 0, 0, correct. Yes, you can quickly write down what is the binary representation for 4, 8, 12, 16, 20 and so on, right. What would they be? The last 2 bits will always be 0, 0, correct. Similarly if it is 16 bytes, sorry 2 bytes or 16 bits long, the last bit alone has to be 0 and that is why essentially it is resetting the last bit. Okay, yeah. Sorry. PC plus 4 has the address of the next instruction to be executed, right. PC plus 4, I am not able to hear you, could you talk louder? Okay, PC stores the address of the next instruction to be fetched, okay. So as soon as it fetches, it will get incremented. The return address should be PC plus 4, okay. Well, actually this happens in the sense that after this happens only the PC value is getting modified, that is the reason, okay. So, the idea is the same, okay. Now we talked about these instructions having these operands and other things. When you write it as an assembly instruction, you write something like this, right. But when this goes as a machine code and how the machine understands this instruction, that essentially is what we call as the instruction formatting, right. So, RISC-V for example supports these various formats. We will not spend time trying to understand each of these formats, but let us just quickly look at in the R type format, okay. There are three registers, RS1 is the first source, RS2 which is the second source and RD which is the third source. Each source or each register operand is 5 bits long. Why because? Because it has 32 registers. So you have to specify each one of those registers, you need 32, okay. So everywhere where you have a register operand, it is 5 bits long, okay. In the I type, I stands for immediate, you can see that there is one source operand, one destination operand and 12 bits of immediate constant, right. This is the small offset that we talked about. 12 bits is 4 kilobytes, 4k locations, right. Again in the case of S type, there is an immediate operand which is again 12 bits, but only thing is it is stored in two places, split, okay, for whatever reason. Here you can see that the immediate is 13 bits in the B type operation, right. And again you can see that some parts of the bits 1 to 4 is stored here, blah blah is stored here and blah blah is stored there. You can see that immediate 0 is not there anywhere, right. So what is it going to be? When it is not there, it is implicitly 0. So it will add that 0 to it, okay. Here in this case, U type instruction has, right, you can see that it has 20 bits of immediate constant and it does not specify the first 11 bits or first 12 bits rather, right. 0 to 11, it does not specify. That will be taken from the current program counter or that will be taken from some place. Again it depends on which instruction uses and what exactly is being done. So this is how the instructions are encoded, right. And the assembly language instruction when it is converted into machine code, it is going to be converted using one of these types depending on what that instruction is and each one of those fields are going to be appropriately specified, right. So we do not worry about this part in our code generation because we assume that the assembler is going to take care of, right. But still kind of quickly we call this structure in order for us to understand how instruction execution happens, okay. That is the next thing that we are going to see, okay. I have a very quick example of a C code and a corresponding assembly code for that, okay. Just to say that it is not very difficult to read the assembly code, right. So what does this code do? GCD, right. So essentially finds a bigger number, subtracts the smaller number from it, okay and keeps doing that, right. And in the end whatever is the remaining thing, non-zero number that is CGCD, correct. So let us look at the code, assembly code. It is only as big as the C code in this particular case. In other cases of course it could be much bigger but at least this one is something that we can easily understand. Let us look at what happens. Let us assume that initially A is in register X1, B is in register X2 and T is in register X3. That means that you must have done some instructions to move the contents of A into X1, contents of B into X2 and so on, right. Let us not worry about that. Let us look at the interesting part, right. You look at if X1 is 0 that means that you are done, right. One of the two operands is 0, the other one has the GCD, greatest common divisor, right. So this is the branch instruction which essentially does that. If X1 is 0, you go to done. Otherwise you come here, right. You find out which of these two numbers is bigger, right. If A is bigger, you continue in this stream. If B is bigger, you jump here, okay. So you are essentially comparing X1, X2. If X1 is greater than X2, right, sorry, it is less than, sorry. If X1 is less than X2, then you go to this label, okay. So when this code is being assembled, right, this label part is going to be replaced by an offset which corresponds to the distance between these two instructions, right. So typically there are two instructions in between, right. So it is PC plus 4 plus, sorry, PC plus 8. So we already know that the offset is added to PC plus 4. So this offset has to be only 4 if you want to get PC plus 8. So the offset is being computed, correct. So if X1 is less than X2, then you have to jump. That means that you have to add this offset to the current PC plus 4 value and that means that new PC will be here. If X1 is greater than or equal to X2, then you perform this step, right. Subtract X1 from X2 and put the value back in X1 and then you go back. You continue like that. If X1 is less than X2, you jump here and then this essentially, right, and we do not have a move instruction. Instead what we do? You add X2 to X0 and put it in X1. That is essentially moving X2 to X1. Remember that X0 is a hardwired 0 register, right. If you write something to X1, that is equivalent to doing nothing, right. Even if you write a value 20 to X1, it will still remain as 0, right. So that is a dummy operation, okay. After this, we jump, okay. So is this code easy enough to understand, right? Not very difficult, right. Not scary. Well, you will see scary assembly code as you go by, but if you take one step at a time, you can actually understand things, okay. Let us move forward. The next thing that we want to see is processor data path and pipelining. How many of the things that we have discussed so far, you have already seen and you would have preferred me not doing it here. How many of you have seen whatever we have discussed so far in some form or the other? All of you have. How many of you are very clear about it even before this class? Okay. You people are being very nice, extremely nice to me. Thank you, okay. So if that has kind of cleared your understanding, I am happy about it, okay. But if you think that I have wasted your time, please do let me know so that at least tomorrow I can, you know, skip certain slides and then move forward. That is the reason why I keep asking that question. Not that I want to see you to be very nice to me, okay. That is really not the purpose, right. It is for your benefit. So I should deliver what you need, not what I know, right. Okay, all right. Let us quickly see what about processor, data path and other things. So we are going to talk about pipelined execution and in pipelined execution, you will see that the instruction execution is divided into five phases, instruction fetch, decode, right, execute memory and write back. Why do we do pipelined execution? It is because we want to improve the throughput and the stages which can do while one stage is doing decode, the other stage can do instruction fetch of the next instruction and another stage can do the execute of the previous instruction and so on and so forth, okay. Let us just quickly look at how the processor pipeline would look like and what it does in each one of these stages, right. In the instruction fetch phase, right, you are essentially fetching the instruction. As we mentioned earlier, PC cast the next instruction which has to be fetched. So use the contents of PC, access the memory and fetch the instruction. The instruction that is going to be fetched is going to be put into one of these buffers which is there in this set of buffers which is called the instruction register, right. So if you fetch the memory location pointed by the program counter, that is actually the next instruction to be executed. It could be an add instruction or it could be a jump instruction or it could be a subtract instruction or whatever it is, that instruction that you have fetched is being put into the instruction register and you are going to execute that instruction. But at the same time, you take the program counter value incremented by 4, right, and this might eventually go back into the program counter or something else can go into the program counter, right. So this is a multiplexer. The multiplexer is either going to take PC plus 4 or something else and then put that value back into the program counter, right. That something plus, sorry, that something could be for branch instructions or control transfer instructions where the program counter is getting affected, right. So we saw that in the case of a branch instruction, PC plus 4 is added with something, right. So that might happen, okay. Let us skip that. Let us go to this side. So typically in an instruction fetch, these two activities happen. That means that you fetch the instruction, you increment the program counter, right. That incremented value of program counter is also available in a small register over here, internal register over here in the buffer and we will call that as the next program counter, NPC, okay. All right, okay. Let us see. Then depending on what your instruction is, you use certain fields of that instruction and then use them to read your first source operand, second source operand and so on, right. And thus source operands are going to be read and put it into some buffer over there. If your instruction has an immediate constant, that immediate constant, some fields of your instruction, which is again taken from your instruction register is going to be taken and then sign extended, okay. Now during the execution phase, you are going to take these as operands and perform the operation. So if it is an arithmetic and logic instruction, then you perform either register 1 plus register 2 or you perform register 1 plus immediate constant, that kind of an operation. So depending on what your instruction is, you perform that operation with the appropriate operands which are available as small registers, internal registers in these buffers, right. So once again let me repeat. In the instruction decode and operand fetch phase, the instruction is already available in the instruction register. From there you find out what is the first source operand. You can actually fetch the contents of the first source operand, right, that particular register, put it into another internal register over here. If you have a second register, second source operand, you can fetch that register, put it as an internal operand here. If you do not have a second source operand but an immediate constant that is sign extended, put as a register over here, internal register over here. So the operands for your operation are now available in these internal registers in this buffer, correct. And those operands are going to be used and you perform the necessary operations over there. This is in case of arithmetic and logic operations. If you have a load store instruction, for example, in a load store instruction, remember that your operand is in memory. And for that you need to compute the address of that memory location. Address of that memory location is specified in the displacement addressing mode. And in the displacement addressing mode you need to take the value of a register and add the offset value to it. The value of the register is available as one of the source operand. The offset is available as a constant here. You add them up, that gives you the contents, the address of the memory location, not the content, the address of that memory location, right. If it is a branch instruction, conditional branch instruction, correct, you are going to check whether a is greater than b or a is less than b, okay. That needs to be done in arithmetic and logic unit but for the time being let us not worry how exactly it is being done. Additionally if it is a conditional branch instruction, you need to calculate what should be the target location to which you have to jump to, that is PC plus immediate constant. So the value of PC, next PC which is available over here and the value of the immediate constant which is available over there can be added and that value can be passed on over there and that can again be stored into your PC if the condition is true, correct. So now you see how that PC plus 4 plus that immediate constant thing is working. PC plus 4 is computed here, that immediate constant is being added over here and that value is being put back in there, right. That is why we always say it is PC plus 4 plus immediate constant, right. So in the case of a branch instruction, this is where the target location is being computed. In the case of a load store instruction, this is where the address of that memory location is being computed. In the case of an arithmetic instruction, this is where the arithmetic operation is being performed. That is why this is called the execute state, correct. If it is an arithmetic operation, then at the end of this stage you have the value which is available. That value is actually going to be written into the destination location. The destination register you know what it is from the instruction register. So you know the place where you have to write and you know the value that you want to write, you can write it down, right. So if it is an arithmetic instruction add R1, R2, R3, I have fetched the contents of R2, R3 here, I have added them up and that value is coming, I want to go and write it back into R1. That is how instruction execution takes place, right. If it is a load instruction, I have calculated the address of the memory location that can now be given to the memory. I can fetch the contents of that memory location that has to go into the destination register. So fetching the memory happens in the memory stage, writing that value into the destination register happens in the write back page, okay. Question yeah. Can you, people have to talk a little louder, sorry. Opcode, where does the opcode go, right. That is a very good question. So let us see, what does this say? Decode, what do we understand by decode? One is identifying the different parts of that instruction and doing whatever that is necessary but then what do you do with the opcode part? Opcode part essentially tells you what operation to be performed. Actually the control unit takes that and then figures out what needs to be done in each one of these stages. Remember everywhere you could see this kind of multiplexers, correct. You need to know whether you should take this operand or this operand, right. That depends on whether it is an R instruction or an I instruction, right. You need to know whether you need to perform an add or subtract or multiply or whatever. That depends on your opcode, correct. And if it is a branch instruction then this has to be an add because you want to compute the target location. If it is a load instruction you want to compute the effective address. So you want to again do an add. So in other words that opcode is taken by the control unit and that control unit sends the necessary signals to all of these things at the appropriate point in time so that the appropriate action can take place. That is why we did not talk about the opcode part, right. But now you see that the RS1 part, the RS2 part, the RD part, the immediate part, everything is being taken at some place or the other, right. Everything is being used, right. Good, we have almost covered. So if it is a branch instruction this computes the target location where it has to go. This part of it computes whether the branch condition is true or false. If it is true then you will take the target location and that is what you are going to set it into the program counter. If it is false you are going to take PC plus 4 and you are going to set that into the target, okay. So roughly this is what that happens during instruction execution or this is how instruction execution takes place in this processor. Each stage of the processor is pipelined that means that when the ith instruction is in this stage, i plus 1th instruction will be in this stage, i plus second instruction will be in this stage, i minus 1th instruction will be in this stage and i minus second instruction will be in that stage and so on, okay. That is really what happened. There are a lot of complications. I have not gone into the details of this. In fact processor data path and pipelined execution we take about 3 to 4 classes to complete the discussion whereas we are describing it in less than 15 minutes, okay. So obviously we have not talked about all the complications but this gives you an idea of how exactly instruction execution is taking, okay. So next thing we are going to talk about is pipelined execution, right. In pipelined execution as I mentioned earlier each phase, right, is going to operate on a different instruction in a given point in time, right. For example when the ith instruction, i1th instruction is going there, this next instruction, instruction, right, is going to do the fetch when this instruction is doing the decode, right. And then in the next cycle you take the next instruction, third instruction and that is going to go into the fetch phase and so on and so forth, right. So at a given point in time when the ith instruction is in the write back phase, i plus 1th instruction would be in the memory phase, i plus second instruction will be in the execute phase and so on and so forth. This is a very idealistic view, right. This may not be exactly how it happens in practice but this is something that happens in a very ideal situation. So now if you observe in the end what do you see? At the end of each cycle you see one instruction complete, right. If you did not have the pipelined execution, right, where will the next instruction start? The next instruction can only start here after 5 cycles and then the next instruction can only start after 5 cycles. So in a non-pipelined execution you take 5 cycles because of the 5 stages to complete this instruction. Whereas in the pipelined execution you still take 5 cycles to execute but your throughput in such a way that every cycle you get a new instruction, right. That is really the advantage that you have. Okay, execution time is still 5 cycles but throughput is improved to 1 instruction per cycle. Yeah. So can you elaborate that question? Maybe you will give me the answer also. All instruction, the entire instruction execution for all instructions are pipelined, okay. So if your question is that if I am doing a load instruction and then I am doing an add instruction and I am doing let us say a conditional branch instruction, do all of them go through all of these stages, right. They all go through all of these stages, some instructions will have some activities to perform in some stage, some instructions may not have. For example, you take the memory stage. The memory stage is useful only for load and store instruction because it is accessing the memory, right. All other instructions and all other instruction types do nothing during the memory stage. That means that they simply wait for one cycle, go to the next stage. I mean, is not it a waste of time? Yes. But then I have simplified my logic so that everybody follows the same path, right. Let us say you walk through a food counter, right, the different kinds of foods that are being kept there. You follow the previous person, who follows the previous person and so on and so forth. Everybody goes through each one of these stages. You may not like all the items that are there. You may skip some. pipeline to it, correct. Because after each step, one person comes out of the queue. If I hold everybody in the queue and then say until the first person finishes taking all the food items, the next person cannot go, then you are going to wait for a very long time and you are going to be hungry, correct. So I allow you to go in a pipeline way. Each step one person goes through one of those items. Whether he takes it or not or whether she takes it or not, does not matter. They go through that. They go through that. Like that every instruction goes through all the stages, right. I think that is probably what you wanted to ask, right. That is exactly what happened. Okay. There are lots of things that I have simplified and I have not really talked about because they may not be directly relevant for our discussion, right. I can talk about it for, believe me, I can talk about it for two weeks. Let us not do that over here. Okay. Now, given that all processors today pipeline their execution, right, what are the problems that come up with and how do we handle that. That is really what we want to do. We are going to take a break at 11, right. Okay. So we have another 15 minutes or so. Let us see what this is. So the picture that I have projected in the earlier slide that everybody goes through each stage and things move very smoothly is a very ideal situation. There are several reasons why this may not always happen. It is because of things like structural hazard or things because of dependencies, data and control dependencies. So let us see those examples. The case of a structural hazard, it happens because more than one unit want to access the same resource at the same time, more than one instruction want to have access to the same resource at the same time and that resource can only service one person at a given time. In which case you will have a hazard. I will give you examples of that. The other one is that whenever you have a dependency between two instructions, this is more important for us. Whenever you have a dependency between two instructions, the second instruction cannot be executed until the first instruction has finished producing that value. So this is an important one and if this happens then we have to stall the pipeline. The third thing is control hazards. When you have branch instructions, you have to jump to a different location and then start fetching from that. Typically, you will fetch instruction, the next instruction, the next instruction and so on. But then if I have a branch less than or branch greater than instruction, the next instruction I have to fetch might be at a far off place and I do not even know where that far off place is. Need not have to be very very far off. Even if it is few instructions away, I still do not know what that address is. So I cannot do instruction fetch until I figure out whether the branch has to be taken or not taken. So these are the situations which cause hazards and because of these hazards what happened is that you will have stalls in the pipeline. We will explain that in the next few slides. Structural hazard, let us look at a specific instruction, a specific example. Let us say the ith instruction is a load word instruction. So what is it going to do? It is going to fetch the contents of R2 plus 8 and load it into R3. When is it going to actually do the computation of R2 plus 8 in the execution stage? When is it going to fetch the memory location? In the memory. For now we are making a very simplified assumption that any operation that we specify in any of these phases can be completed in one cycle. Particularly when I say that the contents of the memory location R2 plus 8 has to be fetched from memory, I am saying that it is going to be done in one cycle. It may be an unreasonable expectation because I said memory is several hundred cycles access time. So how is it going to be accessed in one cycle? We do not know. But for the time being let us assume that it is going to be accessed in one cycle. Maybe the value is available in your cache. Therefore you can access it in one cycle. So we are going to assume that memory access is going to take place and is going to take one cycle. What did we say about instruction fetch? The memory location pointed by the PC program counter is fetched and that is the instruction that you want to execute. Again we said that that is going to happen in the instruction fetch phase. We are fetching a memory location. We are saying that it is going to take one cycle. Again maybe we will assume that it is in the instruction cache and we are able to access it in one cycle. No problem. Right? But even if I assume all of these things, what is going to happen? Let us see. So I have instruction i, i plus 1, i plus 2 and when instruction i plus 3 has to be fetched, it has to be fetched from the memory. At the same time my ith instruction which is a load instruction is also trying to fetch something from the memory. Here you are trying to fetch some data value from the memory. Here you are trying to fetch a instruction from the memory. But if the memory has only one port and can only service one request, then what happens? Then you have a problem. One of them have to wait. Always the later person has to wait. Right? So this kind of a problem is what is called the structural hazard. So if there is a structural hazard, then you say okay you wait for a second, let that person finish and then go. And then what happens? This instruction is going to start in the next cycle. So if you see the completion, if this completes at time t, at time t plus 1 nothing comes out. Whereas at time t plus 2, the next instruction comes out. So this is what we are going to call as a stall or a bubble. So when you open the tap, water comes, sometimes air comes in between. That is the bubble. Right? That is exactly what it is. The pipeline is executing. In the ideal situation it would have got one instruction coming out every cycle. But because of the structural hazard, you now have a bubble. That means that nothing is coming out during that cycle. Right? This instruction comes out at t, at t plus 1 nothing comes out, at t plus 2 this instruction comes out. Okay? That is structural hazard. Okay? There are architectural solutions or hardware solutions for taking care of structural hazards. We will not worry about it. This is not an architecture lecture. So we can move forward. Let us talk about data hazard. This is very important for us. Okay? Let us say my ith instruction is this. Add the contents of r1 and r2. Put it in r3. My i plus 1th instruction is subtract r8 from r3 and put the result in r4. Now let us see what happens. Right? Interesting? This instruction produces a value which is going to be consumed by this instruction. And these people are executing in a pipelined fashion. If they are not executing in a pipelined fashion there is no problem because at the end of this I allow this instruction to go. The value will be available in r3 and I can take that. No problem. Non-pipelined executions there is no issue. Right? But in a pipelined execution. Right? This instruction goes through the execute stage. Memory stage where it does not probably do anything. it is going to write the result value in the r3 register. That is what we saw. Correct? Only in the write back stage. And let us say that is going to happen in time t equal to 5. Whereas this instruction which is the next instruction would have started instruction fetch in time 2 and would have done decode and operand fetch in time 3. And if it goes and looks in register r3 would that value be available? Right? Because the value is going to be written in r3 only during this cycle. Only at cycle 5. If you try to read it in cycle 3 some previous value is what you will get. This is a true dependency what is called read after write dependency. Okay? Read after write. Right? Read has to happen after this write has happened. Okay? Read after write. And because of this read after write dependency can we proceed with this instruction? No. If you proceed with this instruction we will produce garbage. Right? So, at this stage when you figure out that this is going to read r3 which is actually the destination register for the previous instruction. You have to put a hold stop on this instruction. Right? So, this instruction is going to be stalled. It is going to be stalled from the id stage onwards. Right? So, again all I said have been written over there. I did not do the animation but you can see that. So, when this instruction is stopped okay at cycle 3 in the id stage. Right? It is going to create a bubble. Let us look at the animation. So here what you need to do is that you need to wait until this value is being written in this location. So, I will stall this instruction even in the next cycle and if my hardware is intelligent enough that this writing value takes place during the first half of this stage and I can possibly read it in the second half of the stage then this instruction can do the read in cycle 5. If that is not the case I can do it in cycle 6. So, it is either 2 stall cycles or 3 stall cycles depending on how aggressive my hardware is. But essentially there is a stall. Right? Because of the dependency there is a stall and that stall causes these bubbles. Understand? Okay. That is something that we are going to talk about subsequently. So the time being what we will do is that I have to service these people as and when they come in the order in which they come. Then if this instruction stalls all previous instructions also sorry all subsequent instructions also stop because somebody is in front of the queue he is not moving. You cannot knock him out and then move forward. Correct? So, everybody kind of stops. That is the assumption that we are going to make. Okay? So, if you see here so if there is a subtract instruction that gets stalled by 2 cycles that subtract instruction is dependent on that. If there are subsequent instruction they also get stalled whether they are dependent on this or not they get stalled because this previous instruction has stalled. Right? So, in all of these situations what happens is that whenever you have these bubbles or stalls. Right? They cause your throughput to go down. From the ideal one instruction per cycle you will go down. That is the way by which you can calculate if every 5th instruction has one stall cycle. Right? Instead of getting a throughput of one instruction every cycle you will only have 4 instructions in 5 cycles. That is 0.8. Right? Your IPC comes down instructions per cycle comes down. Okay? Let us look at what is control has. Okay? So, maybe this is so maybe it is a good time to take a break because there are more things that we need to see here. So, let us stop here.