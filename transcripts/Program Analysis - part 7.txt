 So, now we will start with, so now essentially we know some structure to the space of solutions that are algorithm traverses right. How what all does it touch, what how does it move we know something, but we still do not know that why does it why should it terminate or why should it why should it compute the solution that we are looking to get those things we have not we do not know yet right. So, let us try to now argue mathematically and see why does it really work because now I can because now I have figured out what is exactly happening on this lattice on this mathematical structure I do not really need to worry too much about the implementation or the algorithm that we used right. So, let us see so there is this big big big big theorem known as a Naster-Tasca theorem which essentially gives some very interesting results for such computations. So, it says that let D the set D with the less than equal to operator be a complete lattice and let f going from D to D be a monotonic function on this set D with on this particular lattice and let P be the set of all fixed points of f then first thing is the set of fixed points is non-empty which means that f under this condition which surely have at least one fixed point. So, this tells me something about the existence of a fixed point right. So, if you have a complete lattice if you have a monotonic function then my there will be at least one fixed point in this lattice second is P with less than equal to forms a complete lattice sorry this the second very interesting point is that ok. So, it first says that there surely exists a fixed point second thing, but it may have more than one fixed point right. So, then it says that it also tells us something about the structure of this set of fixed points. So, P is the set of fixed points it also tells us that the set of fixed points P forms a complete lattice under the same ordering relation really interesting result do you understand what is going on. So, I have this big set in that set it says that ok there is going to be a set of fixed points and this set is going to be non-empty this set P is going to be not. So, this big set is a set D and this small set P is going to be non-empty not just that this set P itself is going to be a complete lattice under the same ordering relation. So, we have a mini lattice inside this bigger lattice the third thing is that the least fixed point of f coincides with the greatest lower bound of the set of post fixed points and the greatest fixed points of f coincides with the lub of the prefix points of f. So, it means that now let us say my. So, the diagram would look something like this. So, these are my post fixed points and these are my prefix points. So, it says that if I take my prefix points and if I. So, if I. So, these are my these are my prefix points right where x is less than equal to f of x. So, when I apply f I go up the lattice right and for these if I take a join. So, I find their join for this subset let us say I find the point right then if I take the join of only the fixed points I would end up getting the same point. Do you understand what is going on? So, it says that you look at all the prefix points this is my set of all prefix points if you take their join it essentially will be it will coincide with taking the join of only the fixed points. Similarly, is the case by duality similarly is the case for the meet with the post fixed points. So, now let us go back to computing least fixed points. So, first is that. So, essentially what has this mainly established for us it has first. So, these two things are really interesting first it actually told us that like if I am able to get my data flow facts to work with the complete lattice then I am sure I will be able to and an order preserving function and my transfer function is an order preserving function in that case I am surely going to get one solution to my set of equations there are at least going to be at least one solution to this set it will not be the case of no solutions. Remember what would be same what was a lattice and what was a complete lattice. Right. So, even for any subset of elements either finite or infinite has a LUB and a GLB if that condition is satisfied then it is a complete lattice. So, because my set D may not be a finite lattice I may sometimes have to even talk about taking meets or joins over infinite subsets. So, now let us start looking at I will again introduce some definition. So, a chain is going to be totally ordered that is by definition. Now there is a notion of a finite chain what is a finite chain if the set of elements in that chain are finite then I would refer to as a finite chain. Now there is a notion of something called an ascending chain. So, what is an ascending chain? So, till here when we were talking about chains we were talking about a set. Here we are seeing a sequence of elements. So, a sequence of elements forms an ascending chain if I can put those elements in a total ordered form. So, a 1 is less than a 2 then less than a 3 a n then this sequence not the set this sequence a 1 a 2 a 3 a 4 sequence means the order is important in this listing. So, this sequence is referred to as an ascending chain. Similarly, I can define a descending chain what is a descending chain it should be able to put it the opposite order and then there is this big definition of something known as if something ascending chain or descending chain eventually stabilizes. So, we say a chain eventually stabilizes. So, I say an ascending chain let us say a 1 a 2 a n eventually stabilizes if there exists some k such that. So, this may even not be finite. So, this may go on forever. I can have infinite infinite sequence, but I say that this set will the sequence would eventually stabilize if there exists some k such that a 1 is less than equal to a 2 is less than or equal to a 3 is less than or equal to a 4 goes on till a k and all values after k are the same. So, it says that it keeps on increasing, but there is some point after which it will just stay there. Right any questions on this? So, now think about the case that if I had this data flow again remember that algorithm and forget that we had this we had this flag which will mark the end of termination and we simply collect the sets the values that I get at each program point. I just sample I just collect keep on collecting the values of this program point they will form a first they will form an ascending chain because every time the value will be something more than what I got previously and after some point when they will keep on seeing the same value again and again and again and again. So, that computation produces a chain an ascending chain which eventually stabilizes. So, now this condition is referred to as ACC or ascending chain condition. So, ascending chain condition says that if you have a lattice such that all ascending chains eventually stabilize. So, it means that if you have some lattice no matter which ascending chain I pick up that is surely going to stabilize. So, what does this tell us? So, if I have a lattice if my lattice D on which I am going to compute my data flow solution if that D is a lattice which eventually stabilizes what can I say about the computation? The computation will terminate because whenever I see so that flap that I was there was to check about this eventual stabilization. So, if it ever sees that I get two things in the sequence which are the same the loop will break and will come out. So, now there is this something known as the clean iteration. So, essentially the algorithm that we now essentially have to compute the least fixed point is that you start with the bottom element. So, once so I start with the bottom elements then I keep on applying F to it while A is not equal to F I keep on going and I keep on reassigning like F A into A. So, this will always so if my F is a monotonic function then the sequence of A's that I will get here will always form an ascending chain and if that ascending chain eventually stabilizes and if the lattice on which my if this values A are picked up from a set D under some operation such that D satisfies the ACC condition then this loop will sometime on the other break because it whenever it eventually stabilizes will have A is equal to F A and I will be able to compute the least fixed point. So, clean iteration says that if you start with your bottom element and keep on applying a monotonic function you will always hit the least fixed point. Sorry come again no need not be no need not be why is monotonicity important otherwise I will not have an ascending chain. So, ascending chain condition says that given an ascending chain so essentially let us go back maybe we. So, essentially what was the thing that this notion of an ascending chain was a definition right this was a definition we said that a sequence of values from D such that the values can be arranged in this form is referred to as an ascending chain this was just a definition any change the values are in total order yes a chain is a set and ascending chain is a sequence they are not actually the same thing yes yes yes yes so we have broken down the definition into multiple definition so that you can follow a logistic logical order that is all. So, there is a set and I sort that set that sorted set is the ascending chain right so if I sort it in the increasing order that is an ascending chain if I sort it the decreasing order that becomes a descending chain for the same chain right and then I would I will pick an ascending chain and I will ask if that ascending chain eventually stabilizes if the ascending chain eventually stabilizes then I would say so that means that there is some finite constant k like so after a finite number of like visiting a finite number of elements even in that infinite sequence I will be able to get values which start repeating that is one way of saying this actually right the height remains finite so in that case there is no other way to go no other place to go so it has to this thing but right this particular definition does not even assume that this particular definition does not assume anything at all this guy this guy just says so this is still a definition right these guys are why it happens in the R lattice is a different matter but right now this is just a definition right so I am just trying to define what does eventual stabilization mean right so it I will say that an ascending chain eventually stabilizes if after a after visiting of a finite number of elements in that sequence rest of the elements are just repetitions that is what I if that happens then I will say that this particular ascending chain eventually stabilizes I am just defining it why it happens for all actors is a different matter but right now we are just defining it and then we define the notion of ascending chain condition this is also a definition right so I am saying that if there is a lattice such that all ascending chains no matter which ascending chain I pick from the lattice if that surely stabilizes then I would say that the lattice satisfies the ascending chain condition again a definition there is again I am not really relating it to what happens to our lattice these are simply definitions right now okay so right if 3 gives 2 f 4 is 3 no so just the function is not enough you have to define the function on what lattice what is the lattice lattice is the set of integers on less than equal to okay okay no no you have to say it for all elements in I okay okay okay so so like people said right like plus with 2 that is going to be a monotonic function maybe I am missing a lot of things you start with so let us draw the lattice so we have 2 then we have 3 and then you have 4 okay now what and how do they move a 3 is 2 and f 4 is like this okay so now on this function this is not that this does not form an no so essentially there is an ascending chain which is 2 3 4 is an ascending chain okay so now what for all points that is the important part no so what does it mean it should be that if 2 is less than 3 it implies that f 2 should be less than equal to f 3 right so is that happening for your f 2 is not even defined so the f 2 is 3 no your f 2 is it is 1 no you wanted it to be 1 so does it satisfy this you check so if 2 is less than 3 f 2 is less than f 3 okay you are getting one point lesser so you are doing a minus 1 basically how fine yes no but remember where did I start start off with you start with bottom element that is important so very good point really good point took me some time to understand what you are saying so yes if you start with arbitrary points it does not guarantee anything at all think about your case like data flow analysis I set this sets to arbitrary values and I start my computation do you think you will reach the final solution right so the initialization is as important right that is why we every algorithm we mentioned initialization initialization in fact yesterday for a long time we did not even say how to initialize yeah no he is saying it is there okay he is saying it is a set of everything right so if you start with arbitrary points I do not know what happens nobody knows what happens it only says that if you start with the bottom element I can tell you it will lead each the least fixed point okay so when is it guaranteed to terminate so if I have a if I have a lattice which satisfies the ACC condition that particular with a monotonic function then that should surely terminate right because and then the question is that where does it terminate so that will always terminate at the LFP the least fixed point so in general so essentially this is the like this is the final result that on a complete lattice which satisfies ACC with monotonic cancer functions the LFP or the MFP whichever we were looking at the bottom semi-lattice or the upper semi-lattice the maximum fixed point or the least fixed point whatever you want essentially it this exists and is computable because I just gave you an algorithm to compute it the clean iteration right so that is why this whole business works I am don't get confused with MFP I should not have written MFP I mean we are working with joins and so we are we will talk about least fixed points not about maximum fixed points so in general the whole structure looks like something like this so essentially it says that if I start with my bottom element and I keep on applying my function on that bottom element I will keep on getting values which are greater than that right so this is the place where I am in the prefix point region right every time I get values which will be larger and larger to this thing just consider this set that I got by applying this again and again and again so if I take a join of those guys just by the definition of taking a join this is going to be greater than this this is going to be less than or equal to the least fixed point of F right so that is how the computation looks like right and then you will have a lot of fixed point setting here I don't know what happens to those but you could have done the computation in the other way with a different function so I should not have put F here this is some other function G right so you could have done a computation starting from the other direction you could have started from top and you could have just applied G multiple times on top kept on climbing down and if you take a meet of those guys then that will be lesser than this and then you would reach the GFP and that would have been the way to reach the greatest fixed point right so it says if you start from the top and applying a function of a certain kind you will be able to keep on you will be able to hit the least fixed point you can do the same business in the opposite direction to reach the greatest fixed point understand that once you hit a fixed point or hit a fixed point you cannot move across fixed points there is no way to write I mean like you apply it again you will still remain there but the cool part is that this particular guys the least fixed point the the least fixed point the greatest fixed point and the set of fixed points in between they themselves form a complete lattice which one yeah so you can view it anyway you want you can you can view it anyway you want so you can view it as starting from the bottom and applying the like growing like keep on applying it keep on applying F to it till you reach the lowest fixed point or you can think it the other way around you can flip the lattices around and you can start from the top and apply some function G till you hit the greatest fixed point whatever we drew was this particular thing so we started with bottom and we kept on getting larger and larger sets by our ordering iteration till we hit the best solution yeah yeah from the top but maybe from with a different function how but why would you not want the other fixed point that is the first question what why are you why should you be interested in the other fixed points no but why do you want that lattice it just shows that it just tells you that those lattices those points do form a mini lattice but why would you be interested in hitting that mini lattice so at least in our data flow scenario we do not have any interest in finding those mini lattices no that is not done empirically that is not done empirically that is not done for example that is done mathematically so there is a mathematical proof that this will happen so I do not want to do the proof here but there is a very interesting proof in fact you go back and look at this particular proof it essentially says that we can actually prove that if there are two fixed points they will always have this ordering and you can always find the top element there but in our scenario we are not interested in getting the other fixed points for us it does not really matter right and all it matters for us is only this part right going all the way from the bottom till the till we reach the LFP we don't even care anything else so for us even a similarity structure is good enough what happens the other direction what happens for the meets I am not even interested about that because we are never applying meet anywhere we are only applying joints right so whatever structure is there for meets I am not even using it so it does not matter right yeah so this is much more interesting with the proofs but like you spend two classes doing the Naster Rasky theorem and then because more interesting but I'll keep it in interest of time but I will really ask you to go and see that why this really works at least the first couple of points are not difficult to show they are not so difficult proofs now the question is about precision so again remember that what did we really want to compute what we really wanted to compute was the MOP solution the meet over all path solution or the join over all path solution if you will right so essentially what we do we would have wanted is that we we try to find out all paths from the program so how will this MOP solution look like essentially what we're trying to how will you now you know the lattice and the structure and all that so mathematically can you write down what does the MOP solution look like how would the so let us say I have some graph I don't know why you always end up making the same graph okay let me not okay so let's say this is a this is B this is C and I want to compute the MOP solution here the meet over all path solution here for the join over all path solution here so how do I how will I apply this solution so let's say this guy has a transfer function which is f1 this guy has a transfer function which is f2 this guy has a transfer function which is f3 how do I compute this particular answer how what is my MOP is going to be at this particular point so let's say I start with something let's say I start with some value here it's some initialization of this guy which could be some value let's say alpha so the entry point to this entry block let's say the dataflow fact is alpha I know that so I have set it to that and I want to find out the meet over all path solution at this particular point how do I how do I do this what is the function going to be very good very good very good so I will first apply alpha on f1 then I will start going around this path right now so I will apply f1 alpha on f1 then I will apply f2 on f1 then join of right then join of f3 of f1 of alpha this is what we want to compute what do we actually compute the MFP solution or LFP solution whatever we want what are actually we computing actually that is why I drew the other diagram again it is not interesting so I have to extend this so let me just change this so I want a I want b c I want b e f g and I start with the alpha now you have to write more so this is what I was trying to avoid so then then what is my MFP solution MOP solution now though you guys tell me now this is f a f b f c f d fe f f bad but let us say at the beginning of this beginning of g so this is the program point where I am interested to find the MOP solution so what is my MOP solution here yeah so long chain of f a then f b then f d then fe of alpha join of yeah f f so if f a f c f d let's say ff of alpha then something then other two parts what about the MFP path MFP solution what does the MFP solution look like fe applied to so that will be f d of join of which two things f b f a of of alpha join f c f a of alpha join same for the other direction can you see the difference between these two equations not completely but somewhat yes yes yes we are applying less functions so we wanted to compute the MOP but somebody told us don't do MOP do MFP and we are we since yesterday I am teaching you MFP now why should this why should it work to compute the MFP when we wanted the MOP solution so essentially we want the MOP right all of you agree what we want why is it the MOP want because any of these paths is a possible execution path right and whatever property we are we get should we should be something that should be possible in some execution right so I am taking a meet over all possible executions that are possible right but we are computing something which looks very different why what is going on very good very good so can you see when will these two be equal yes yes if the join is distributed over your function f then both of them will they not look the same right so if you have distributed frameworks then MOP solution and MFP solution coincide was reaching so which of them is more efficient MFP is much more efficient because see how less we are times we are applying the function but the theory says that if my framework is distributive if my function distributes over my joint operator then I have my MOP solution and my MFP solution will coincide awesome news question is for reaching definitions is it the case can you look at the transfer function and tell me is that transfer function distributive over join you know the lattice you know the transfer function can you can you quickly try out and see if it really looks to be the case how do you do that hey this side decide new that we are doing no we have two elements we are computing the join of it that is all forget the control flow graph now right now just talk about the distinct you know the lattice you know you have the subset lattice of definitions and you have the transfer function which is in minus skill union gen now for this particular function on that lattice tell me what happens now there is no predecessor successor nothing is there so is this function if the form of this function distributive over this lattice is the question how will you do it so what is the stupidity how do I distribute it so what are you trying to prove what is the theory we are trying to prove yes yes so if I have if I take FA union join FB this is going to be the same as right so is it hard to show just write your in and out equations put it in this form and see what you get okay so so I will give it for homework this is not this is actually trivial there is not much truth I mean just a matter of like doing it like properly now let us look at this particular equation let us look at the MOP solution can you tell me something about the relation between these two forms between the MOP and MFP solutions what are we doing a see that your F's are monotonic functions right so can you use that information to say something about this among these two guys which of them will be higher so is there a relation between these two guys so I get some values of this is what if this is a distributive framework instead let us say I compute alpha as FA union FB and I compute beta as F of A union B sorry A join B right if you join FB or A join B right so now the question is that can I can I tell something about alpha and beta some relation between alpha and beta so what do you know you know that A join B is bigger than both A and B right by the definition of join okay what else do you know you know that because it is a monotonic function you know that A is less than FA B is less than FB and her very good okay so which is less than equal to which one so like you are right maybe that you are just reasoning in slightly different order I guess so this is going to be the case right right so if this is the case if this is the case then can you establish a relation between MOP and FNFP so what is the relationship between MOP and FNFP which one is which one the upper one looks like MOP right the lower one looks like MFP where we first you take the join and then apply the function right so in that case your MOP solution less than equal to the MFP solution so when what does less than equal to mean if this solution is this solution then what do we say this over approximates right so essentially what is going on right now okay so then essentially what is going on is the MFP solution over approximates the MOP solution so I have left a lot of exercise for you these are like sort of simple discrete math assignments there is nothing more to it right so try these out at home like you will really be convinced you will be able to convince yourself that what is going on they are just a matter of putting things together there is nothing more to it so then essentially what you can say is the MFP solution will always over approximate the MOP solution what does it mean what does that really mean guys what happened you guys did not have coffee that tea was not good is it yes so it will always compute a safe solution so it may be the case that you do not get the best solution but you will surely get the safe solution you will surely get a safe solution so now let us summarize this whole business that we so essentially the idea is that but why not compute MOP right for even analysis like constant propagation computing the MOP solution is undecidable forget even high complexity right so there is even no not even a hope of computing the MOP solution right so the only way to get a solution is to get the MFP solution so now the whole business looks like this key if it is a monotone framework what does a monotone framework mean that you have you you satisfy ACC on a on a monotonic function right so if you have a monotone framework in that case your MFP solution over approximates your MOP solution this is the first learning the second learning is that if you have a better structure if you have a distributive framework what is the distributive framework when your function is distributive the function distributes over joins in that case your MOP solution is equal to the MFP solution now let us go back to our analysis so people should wake up now people have slept with the the mathematical part of it okay let us go back to our analysis in our analysis let us look at reaching definitions reaching definitions was it distributive has somebody able to prove that so take my word for it till you prove it to yourself like to me if you are not like if it does not turn out to be the case but assuming that reaching definitions it is a distributive framework because of that I actually even if I compute my MFP solution I actually end up getting the MOP solution I still get the optimal solution still get the best solution what about constant propagation is it distributive why is no why no why no give me an example where it is not distributive so when it is not distributed things are easy now give me a counter example you are done give me a counter example which shows it is not distributive hey we saw it saw an example yesterday remember the example can you reproduce that example right right so let us do that so it was x equals 2 y equals 3 it was x equals 3 y equals 2 and we want to get a solution here so if I try to get the if I take that if I take the apply the functions and then take a meet so what happens if we and there we have z equals x plus y so if I apply the functions then what happens then I get so I get here 2 comma 3 and here I end up getting 3 comma 2 but I am applying I am getting the MOP solution so I am not allowed to merge them yet so I will merge them here wherever I decide the solution right so now I will get it and I will get 2 comma 3 comma 5 where did I go really far okay so what do we get with the MOP solution so what is the MOP solution then then I take a merge so I apply the function still further that becomes 2 3 and 5 join 2 3 2 and 5 that is my MOP solution and that will be what that will be not constant top not constant and 5 that is my MOP solution what is my MOP solution or where I do the other way round the other side of distributivity what will I do there first I will take the join as soon as I take a join between 2 and 3 and join of 3 comma 2 and then I apply my f right which is my z equals x plus y right so this immediately becomes not constant not constant apply f on it which again gives me not constant not constant so that it is constant propagation is not distributive because constant propagation is not distributive so I cannot and getting the MOP solution is undecidable so there is no other option I compute the MFP solution which is still an over approximation of my MOP solution so the constant propagation we do not get the best solution we get something which is over approximation of that yeah no these are completely different things altogether separability and distributivity are different thing altogether they do not so distributivity is on the structure of the function f right now the structure the function f could be could be non separable where it is using things from other things that is a different matter but they are they are different aspects I do not have readymade examples but you should be able to construct cases where both the things can so it is safe for monotonic frameworks but for distributive frameworks it is right so final summary is this so if I have a monotone framework consisting of a complete that satisfies ACC and a set of monotone functions that contain an identity element and is closed under function composition then I will essentially be able to if I compute the MFP solution I will be able to get a safe solution and if I have a distributive framework with the correct spelling and a monotone framework sorry and the functions are also distributive in that case I get solutions we I will be able to get the optimal solution the MFP solution so our reaching definitions the lattice was a complete lattice it satisfies ACC so every finite lattice is always a complete lattice right because because there are no infinite sets anyway so you do that extra condition is not does not really matter right so the transfer functions are both monotonic and distributive so it will always end up computing the MFP solution so essentially what we have now what with after all this big business what did we get we got a nice unified framework a unified mathematically sound framework and what it gives is it gives me a nice checklist right even if you want don't want to do all the proofs and assume the people who did it before you were smart enough to have done the smooth proof properly you can just trust them and use but you still get a nice checklist which says that now you do not have to do the termination proofs the soundness proofs for every analysis that you design separately you would not have to argue about that remember how did we start doing the analysis that termination proof for the reaching definition case we said oh there is a set of definitions but this set of definition only increase I can see this transfer function is doing that then it but it is I see the total number of definition is bounded so I was analyzing on the on that particular analysis think about a more complicated analysis doing that might be harder instead I can simply use my checklist I can simply say okay does my does is it a complete lattice does it satisfy ACC is it a monotone framework is a distributive framework so these are small questions to answer on a given given analysis right and once you've answered that you exactly know what your framework is supposed to do it is it immediately guarantees that your framework would terminate the analysis would terminate and it guarantees the sort of solution you will get it will get a sound solution or you get exact solution whatever you get so you do not have to do all the proofs already so Nostra taskie cleaning they have been able to solve they have been kind enough to do all the proof for us right now we can simply trust them and we can just check that our analysis that we have designed satisfies those conditions or not right if those satisfy the conditions and those questions are similar to what I asked you right is constant propagation transfer function monotonic is it distributive and it's a matter of and it as I said this is a discrete math question right sit back and see what what happens once you have that you are you can actually say that fine my analysis is going to have these properties right so you do not have to right so this is the whole thing about the idea of striving for this unified framework that we sort of started yesterday maybe almost day before yesterday right so this is this whole business about dataflow analysis which establishes this very nice sound mathematical framework in which you can build your analysis and you get guarantees that how your analysis would work so how much time do we have we have some time so let me just think what to do okay I just give you bits and spurts of few things I don't know how much after lunch if you have get I'll have time to do that if that we'll see how it goes okay so one thing I wanted to talk about is there is a more generic framework to analysis something known as abstract interpretation so it sort of subsumes dataflow analysis right so essentially it again is a big theory of over abstractions so essentially it says that how can I build simpler analysis which can scale well but still give me certain guarantees right and the interval analysis that I talked about yesterday I did not tell you I cheated a bit and that is actually taught more in terms of the abstract interpretation then in the data analysis framework so data analysis is more in the compiler domain so for optimizations and all people end up using data analysis but abstract interpretation is a more general theory which is used even in the verification community this is one thing second thing I wanted to talk about is so I just want to tell you that something like this exists and and you can ask Google God to tell you about more about this right so search for it and try to do things okay so the other thing I want to talk about is there is this notion of so whenever we are doing an analysis we are essentially saying that a program is extremely complex right if we try to model everything in the program it becomes very difficult to model it so essentially that is why we have to do these abstractions so what is an abstraction abstraction is basically throwing away details about the program throwing throwing away details which you think are not useful right yesterday you guys did an abstraction where we were building this buffer overflow analysis right what was abstraction the abstraction was that we will only look at integer variables because those are the variables which occur as indices to arrays can create a buffer overflow second is we said that we will only look at operations of the kind V1 is equal to V2 plus minus some constant C because that is the that is what generally happens with array indices generally array indices don't show any other nobody multiplies things that I mean not that you cannot do it but generally you don't do it right so essentially what we did was we essentially said that every operation which looks like this we will retain in our model and every other analysis we will assume that that variable becomes what those statements are there I have to do something with those statements right what if they end up coming here so what do I do I have to do something about it I still have to say what values they can get so what is the worst I can assume the top right the minus infinity to infinity so I assume every other operation pushes the values to minus infinity to infinity or in other words we this is a non-deterministic assignment so it is star means it can just assign it to any value by the way what does the lattice for the interval analysis look like what will the lattice look like and what is the ordering relation so I am sorry what is the set and the ordering relation so what is the bottom element think about it what do you want no so how did we initialize the sets you saw the code yesterday right so actually in this case so every such range like let us say 2 to 5 essentially represents a set of elements right 2 3 4 5 so what should be the corresponding set for the bottom element 0 elements right unfortunately in this range business I cannot write it I can write it but it is very bad I mean I can write minus 5 and like the other side minus 5 and minus 6 negative range which says that of course but this is like very clumsy and not nice no no but you do not 0 to 0 1 to 1 is singleton elements right singleton sets yeah those are constant values the singleton sets but bottom element what do we do so we define a bottom we add an dummy bottom which is basically means that there are zero elements what is the first level then after that singleton sets like 0 to 0 1 to 1 2 to 2 so on 4 to 4 then what next level yes so 0 to 1 1 to 2 then 0 to 2 so on and so how do you take the meat remember meat operation now mean of the 2 and the mean of the left hand sides and the max of the right hand ends that is exactly what is happening see that is how not meat in this case join so the join is happening this way and what is going to be the top element minus infinity to infinity that is also sort of dummy but we put it right so this is the lattice and what is the ordering relation how do I define the ordering relation how do I say something is less than something length of the interval no length of the interval will not work right no that is also not required what about these two guys say 2 to 20 and let us say 12 to 15 right so the range of values contained within one of them should be completely subsumed by the second the right hand side right so like in this case this is less than equal to 2 to 20 right because the right hand end is past this and the left hand end is before this that is the ordering relation okay so like now back to abstractions so essentially the abstraction we did was we said that any so if I have an expression I have a statement which assigns to something like this I will simply retain it everything else I'll say it's it can be just anything at all right so I can do abstraction on the data flow I can see that certain statements can do weird things can just do anything they want similarly I can do abstraction on the control flow you can say that rather than actually looking at the control flow graph and saying that the control can be only be transferred in this manner which requires such a sophisticated analysis can't we not say okay any control can go to anywhere that's an abstraction then I don't need the control flow graph at all so I can throw up in a control flow graph I can say I don't care about what the control flow graph is I take a bag of my statements s1 s2 s3 s4 and I say any statement can just go anywhere and loop itself and go like this and loop itself it can so it's a complete graph on all these guys with even self loops so if I do that so essentially what I've done is I've thrown away my control flow graph I said I don't care about my control flow graph give me a solution on this particular thing this is so people define these two types of analysis and these are this is a very important distinction one is called a flow sensitive analysis where I care about how control can flow the other is flow insensitive analysis where I do not care about how control flows I assume that control flow control can flow in any way any these statements can be arranged in any order for a flow sensitive analysis we compute data flow facts at each program point but a flow insensitive analysis anything can just flow anywhere right so in flow insensitive analysis we create one summary solution for whole function right so now if you think about the landmark of program analysis so we have so we talk about these multiple types of sensitivities we talk about like like do you want to model function calls so correspondingly we have context sensitive or context insensitive analysis if you want to model control flow behavior you have flow sensitive and flow insensitive analysis what we have covered so far is only this flow sensitive analysis we have not even modeled function calls right so we currently only have solutions for every function separately so in the future lectures if the instructor is talking about these things you should be aware of what they are even if you do not know how they are computed so there are algorithms to actually induce like flow sensitivity how would you make a function flow sensitive how will you how will it know where to so let us see an example how what is flow sensitive analysis and what is flow insensitive analysis just to make sure that you are ready for the next sessions so essentially it means that let us say I have function foo and I have a function bar which is called from here somewhere in the function let us say this is foo 1 similarly I have another function foo 2 which also has a call to bar now let us say I want to do an inter procedural analysis as opposed to the intra procedural analysis we are doing till now so we only assume our whole universe is one function and that is all I care about right so now if you want to analyze this function so let us say this guy takes a parameter a so x takes some parameter y now there are two ways of doing this analysis right one is the context sensitive way of doing this analysis is to analyze each call to bar separately right separately figuring figure out that what will happen with bar when it is invoked here and what will happen with bar if it is invoked here right so for each of these calls so these are called call sites for each of these call sites you will have a different solution for the things in bar the other option not just that even the effect that bar has like let us say the value it returns that will be that will depend on that bar was called here not here right similarly the other option is that you do an analysis assuming that bar could have been called anywhere so it is similar to doing this MOP and MFP sort of business one option is that you first merge the behaviors of this function like wherever it is called you merge those values that can occur and then analyze this function the other is first analyze this function separately and then apply the respective function the rest of the function on that those particular pieces right so these are this is a difference as you can understand context sensitive analysis is more expensive than context insensitive analysis but again it can give you much better accuracy right so next you may be exposed to this context sensitivity context I am not sure exactly what is the thing but you should know these terms you should know what is context sensitivity what is context sensitivity what is flow sensitive analysis what is flow insensitive analysis any questions on the topics let me see if the next session we can do a very quickly do a flow insensitive analysis just to tell you how it works why is context sensitive more expensive because whatever we are doing right now what what we are doing is that every program point we are maintaining what happens at that program point and how that particular value propagates further in a context insensitive analysis we don't even have the control flow graph so there is no notion of a program point anymore and because I don't even know so program points are defined only on the control flow graph right so if you do not know an ordering of the statements the behavior the program point does not mean anything at all so now what I do is we compute one summary structure one summary solution which is true at every program point right so let's take a very very simple very this example so let's say I have x equals 1 y equals 2 x equals 5 right so let's say this is my program so I am doing an exact analysis which is trying to track the value of each variable let's try to do a flow sensitive analysis of this so when will I will do a flow sensitive analysis what is the value I'll get here I'll say that both x and y are not initialized what will happen here I will say that x is initialized to a valued 1 here I will say that x is initialized is assigned a value 1 y is assigned a value 2 and here I will say x is assigned a value 1 sorry x is assigned a value 5 and y is assigned a value 2 agreed okay now tell me can I somehow summarize these solutions which will be true irrespective of which order I run these statements and where I ask this question I want a summary solution for this whole thing I can do what no it's not about constant so I can simply say either I can say x is not constant I can say that x can point to a set of values 1 and 5 and y to a set of values 2 okay so what information have we lost here what about we have lost something about program point but maybe I should complicate this example let's I also have z is equal to x plus y so let's put this other thing so then essentially why what I have here is that x is 5 y is 2 and z is 7 right now if I do the flow insensitive part what will happen to z it can take any value of x because I do not care about which order the statements are being done and so by values of z can be either 2 plus 1 3 or it can be 2 plus 5 7 no not defined if you are if you are doing a propagation oh I see I see huh so either you do that you can say not defined garbage values it is possible to have a garbage values otherwise the problem is that garbage will go and sit everywhere otherwise so almost so instead of that let's try to keep an initialization that values will always get initialized okay I am doing a little clumsy analysis but let's say we take an initialization which is x equal to 0 equal to z equal to 0 these are global variables so then we will of course I will get a 0 at all these places also right taken point taken right so now essentially what I ended up having is I have a bad value of z which was not even possible in a flow sensitive analysis right but think about the complexity it's a very simple analysis right so every time so I did not have to take maintain meets like sorry maintain the ins and the outs at every program point take a meet and do something I don't even have to bother doing that I can simply just every time I maintain my set and update it depending on what happens but you have to be very clever about doing the summarization the only thing you can you cannot do a kill basically now anymore right so because the kill because you could do the kill because you knew that something happens after something only then that killed as possible so now the kills will not be available so you have to do what are called weak updates you can only append something to the sets you cannot remove anything from the sets anymore right so this is what a flow insensitive analysis looks like right so it is much simpler simpler in the sense in even in terms of space you can understand what was the space taken by the flow sensitive analysis it was a size of the solution set times the number of basic blocks what is the size of the solution set here just the size of one solution right very cheap the memory footprint is very small right in fact there are certain analysis like pointer analysis which are generally implemented as flow insensitive analysis because the sort of accuracy we get is okay with like so essentially pointer analysis is used to drive other optimizations right so why can pointer analysis create a problem what is a what is a problem with if you get pointers how you are seeing so can you give me an example where it can create a problem which let's say reaching definitions or let's say liveness analysis maybe that is easier so what what can be the problem with liveness analysis right so I let's say I want to try to I want to figure out if some variable Y is live here or not right now let's say I do not find any use of Y anywhere down the line but I see one basic block which uses star of Q what is the start of Q nobody knows what if this star of Q was Q was nothing but ampersand Y it is actually using Y but in disguise as a pointer dereference right so all your analysis that we have been doing you first will have to do a pointer analysis to figure out which location a variable can point to and then modify your analysis accordingly to take care of such scenarios right so pointer analysis is typically implemented as a flow insensitive analysis because the accuracy you get with like the sophisticated flow insensitive analysis is good enough to drive these other optimizations they do not do they do not hurt it too much so it is not too bad so I will come back I will again see the mood of the class if I find it is interesting I will do a flow insensitive points to analysis will be very interesting maybe I will drive it more to examples and not do too much of heavy theory right and if I see that the mood is not good then we will move to something called dynamic analysis.