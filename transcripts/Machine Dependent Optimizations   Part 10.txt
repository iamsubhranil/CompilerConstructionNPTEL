 So, as I mentioned in the last class, now we are going to focus on optimizations for memory hierarchy and parallelization, specifically vectorization that is the one that thing that you are going to focus on. Similar techniques can be done for parallelization as well, but we will not talk about it right in this slide ok. Now the idea is to exploit more performance and then try to do some of these optimizations. One important thing that you are going to see today, I mean important thing that you are going to see in this lecture as opposed to some other thing that we discussed, we will have to come back and ask the question where are these optimizations going to be done or when are these optimizations going to be done. Because some of these optimizations need to be done at a higher level rather than at the machine code or at the intermediate code level. So, we will talk about that towards the end and then say why that is important for this ok. The idea here is again to exploit higher performance and possibly exploit parallelism and locality in loops, that is really why we are talking about these techniques. We already talked about instruction level parallelism in the earlier class, today we will talk more about data level parallelism. Thread level parallelism is something similar, task level parallelism is something I have sliced, but I do not think we will be able to cover that in this lecture. We will also very briefly talk about exploiting data locality ok. So, as I mentioned earlier data level parallelism is exploited by vector machines, M D machines, sub-word parallelism and GPUs we saw that ok. Again I am going to skip this example, because we have already seen this in the previous lecture. If you have a piece of code which is like this right, then for the vectorized machine which we have not seen the code would look something like this if the vector length is 64 right. Again this is still a high level representation of the code, but what it says is that for elements j to j plus 63 or totally 64 elements right the vector operation would be performed. Then again for the next set of vector, then again for the next set of vector and so on. Whereas, if it is an AVX kind of a code, then you are going to do it for 16 or right 8 or whatever number of operations in parallel using this v move and v add v multiply kind of instructions ok. If it is a GPU kind of a situation, then it is going to be in the form of a CUDA kernel well I am not saying that the compiler is going to generate this, but in order for you to exploit the data level parallelism right you may have to write as a programmer this kind of a CUDA code which essentially talks about data level parallelism. So, talking about thread or task level parallelism typically this is exploited in the multi core architecture all other processors are already multi core they have multiple processors in that ok. And this multi processor architecture can be either a shared memory system or a distributed memory or message processing system. In the case of a shared memory system the different cores that you have in the processor they share a common memory right and there is hardware abstraction for sharing this memory. Whereas, in the case of a distributed memory architecture different machines or different processors have their own local memory and they communicate with other processors by means of messages which is send receive messages and they do not share common memory right they have to explicitly send data by means of messages or receive data as messages ok. For the shared memory machine and multi core architecture typically you use open MP kind of a programming model whereas, for the distributed memory machine you use MPI of course, it is possible to write your MPI code and run it on a multi core architecture also right ok. If you have not seen an open MP code this is how an open MP code would look like it essentially has what are called pragmas which are compiler directives right and these compiler directives are being looked at by the compiler the compiler understands that this piece of code is actually can be executed in parallel right and that parallel code essentially says that you have to execute this for n elements and let us say that there are four processors or four processes or in this particular case four threads that are there then each one of the threads can actually execute one fourth of the task right in parallel. So, execution would proceed something like this roughly right the first thread will do for the first one fourth of the data element the second thread would do for the second one fourth and so on and all of them would execute these statements in parallel, but they can kind of execute them at their own speed it is not like a SIMD machine where everybody is executing the same instruction at a given point in time right this code or this processor has to execute this sequence of instructions right for n by four elements it will execute in its own way as fetch decode execute and so on. Whereas, some other processor would execute the next one fourth or the next one fourth and so on and they will all do instruction fetch decode and execute individually that means that they fetch decode instruction separately and because of that they can actually operate in different speeds that is possible you should not be thinking that this is also same as SIMD or SIMD architecture here they are actually executing individual pieces of code by themselves right. But here one thing that we can notice is that in this particular case as I mentioned earlier the programmer gives certain directives to the compiler saying that this piece of code can be executed in parallel and this for loop can be split across four threads or eight threads or whatever number of threads and the task can be executed in parallel why does he say that because right these are independent tasks right when we talk about instruction level parallelism we talked about independent instruction when you talk about thread level or task level parallelism you have to think in terms of these tasks being independent of each other that means that performing the operation ai is equal to ai plus s for the first one fourth of the element is definitely independent from performing for the next one fourth and for the next one fourth and so on. Therefore, they can be done in parallel right and the programmer has made the compiler job easy by specifying that these are parallel that means that compiler need not have to analyze whether they are parallel or not correct the compiler was told look this is parallel go ahead and parallelize it right. So openMP allows you to specify allows the programmer to specify this therefore, that the compiler does not have to do really do anything but supposing you are given this piece of code as a sequential C code can you analyze this and find out that this is a parallel task and I can execute this in parallel or I can put these directives like this so that my openMP program will execute them in parallel that is the job of a compiler right if a sequential code was given only this part of the code was given can we analyze this and then find out that these are independent tasks and they can be executed in parallel right. In general the task is a very hard task right taking completely sequential program and parallelizing this right we have only had limited success but certain part of the code can be analyzed and definitely that can be parallelized right this is possibly an easier task to do this particular example okay. So in this talk what we are going to do is that given a piece of code which is like this how do we generate code for vector machine how do we generate code for let us say a parallel machine or how do we generate right how do we identify whether locality is being exploited or not how do we modify this loop so that locality can be exploit that is essentially what we are going to focus on what is required to understand this how do we change those things or how do we generate code for those things that is really what we are going to focus on and because we are going to focus on that primarily we are going to focus on array variables and loops right because that is where most of the time is spent in the code we have a sequence of code like a is equal to b plus c something something right and not inside a loop then very little time is going to be spent on that part of the code and that may not be really exciting okay. So our focus is going to be on arrays and what we are going to talk very briefly is about array dependence analysis how to identify what kind of dependences exist in program okay and how do we figure out what part of the program are independent and can be executed in parallel okay and then what kind of loop transformations can be done and when these loop transformations are legal to be done so that you can transform the loop from for example we saw in the case of caches that having for j for i loop is a bad thing you want to change it to for i for j right that is that transformation is called loop interchange when can we do loop interchange transform is it legal to do that right those are things that we are going to study right of course all of this is going to be used for parallelizing and vectorizing the code okay. Let us formally define what is data dependence since you are going to be talking about array variables and when I say array variables what do I mean one array can have several elements and you are going to talk about dependences okay so we say that there is a dependence from statement s 1 to statement s 2 if both s 1 and s 2 access the same memory location same array location both of them are accessing a of i or both of them are accessing b of i j right that particular location right and if there is a feasible runtime execution path from s 1 to s 2 that is fine okay and one of these operations is a write and the other one can be either a write or a read this is when we say that there is a dependence right we are not interested in a read to read dependence we are always interested in the read to write write to write or write to read right those are the dependences that we talk right. Now let us look at these dependences earlier we looked at the dependences in the form of instruction now let us look at it in the form of statements right and then talk about dependences between these statements right look at the code on the left hand side right now can you see any raw dependence here right so this is read after write true dependence there is a dependence from s 1 to s 2 on which variable x i j x i j is being written here and the same value is being read here any other dependence that you see there is an output dependence x i j to x i j and therefore, there is an anti dependence okay anything else that you see any other dependences okay let us see what I see right this is fine flow dependence or true dependence anti dependence that is fine output dependence okay right we are only seeing some of the simple dependences I will ask you the question what about from this x i comma j plus 1 to this x i comma j is there a dependence anti dependence this is a read that is a write okay for example, let us take i is equal to 4 and j is equal to 5 so this is going to read the value i is equal to 4 j is equal to 5 we are going to read the value 5 comma 6 right after that you are going to execute the statement i is equal to 4 j is equal to 6 and this will also be 4 comma 6 so is there an anti dependence from here to here what about from here to here from this y i j in statement s 2 to the statement y i comma j minus 1 just use some examples and then figure out right the value produced by y i j is going to be read in the iteration y i comma j plus 1 right so there is a true dependence from s 2 to s 1 right there is a true dependence any other thing I think that is all we have covered all the occurrences of x y right so all of this are dependences earlier we were dealing with scalar variables now we are dealing with arrays non scalar variables so not only you see dependences which are within the same iteration but you would also see dependences which are across iterations just like what we saw in software pipelining correct we saw a similar situation what is being created in the jth iteration is being consumed in the j plus 1th iteration of course here it is a two dimensional loop so we are talking about i j i comma j plus 1 what is consumed in the i jth iteration is written in the i j plus 1th iteration it is also possible that there is a dependence on the i side ok let see whether you see that over here right what is produced by x what is produced by state statement s 2 in the i jth iteration is going to be consumed by statement s 3 in the i plus 1 comma jth iteration right that is after all j iterations for i is over you are going to go i plus 1 and that corresponding j you will have a dependence right so when we talk about array data dependence analysis we are talking about identifying all of them correct now because there is a dependence from this statement to this statement in the jth iteration correct so can i execute statement s 2 sorry statement s 1 s 2 s 3 of i jth iteration in parallel with statement s 1 s 2 s 3 of i j plus 1th iteration can i execute them yes no maybe again i will repeat there is a dependence from statement s 2 to statement s 1 what is produced by statement s 2 in the i jth iteration is going to be used by this in the i comma j plus 1th iteration so the question is can we execute these two iterations in parallel no whenever there is a dependence you cannot unless you put appropriate synchronization otherwise they cannot execute in parallel so these are not independent operations if i look at s 1 s 2 s 3 together as one group of statements and this one group of statement is dependent on that one group of statement so i cannot execute them in parallel right so to do analysis like this is essentially what is what is the framework that we are going to talk in this lecture it is actually fairly i mean somewhat involved mathematics involved in this but we will try to keep it to a minimum level because a i am very tired b softer length session right so we will not go into all of that we will only talk about what is really required so each statement in a loop is executed several times because it is inside a loop right and that is possible to have a dependence from a statement to itself okay we have not seen that example here for example if this statement says c of i plus y of i comma j minus 1 then from i j statement to i j plus 1th iteration there is a dependency so a statement can have a dependency with itself right and then there are so many dependencies not just the number of dependencies that i talked about just look at these two dependencies right now i said that i jth iteration to i j plus 1th iteration if i actually enumerate this for all values of i and j correct it will be n cross n isn't it it is just one dependence that you are talking about but then if you enumerate it for all values of i and j it is n square dependencies that you are talking about right so we want a compact representation for this otherwise i have to say that 3 4th it means iteration 3 comma 4 is dependent on iteration 3 comma 5 or the other way around right or iteration 3 comma 5 is dependent on 3 comma 6 3 comma 7 3 comma 8 whatever it is like that we have to say all of them so we need a compact representation for that so let us see how we are going to do that and the dependencies can be either loop carried or loop independent we will give examples we have already given examples but we will also see more examples of them okay so before we define this let us define two terms one called the iteration space iteration vector and iteration space if you have a n way nested loop that means for i for j for l like that right then that loop is going to be when one iteration of that loop is executing right you have one value for the first let us talk about a two dimensional a two way nested loop i comma j i going from 1 to n j going from 1 to n right then for each value of i comma j i have one iteration for example 3 4 is one iteration 5 7 is another iteration right 23 is another iteration as long as n is greater than 20 all of these are points in the iteration space right that is really what we are talking about right iteration space is the set of all possible iteration vectors iteration vector is essentially the index value index value for a particular iteration right we will also talk about what is called the lexicographic ordering since you are talking about nested loops correct in the nested loop you can say that i jth iteration i j plus 1th iteration which one is earlier which one is later i j is earlier i j plus 1 is later this is the lexicographic ordering correct i jth iteration i plus 1 jth iteration which one is earlier i j is earlier i plus 1 j is later right let us do one more i jth iteration right i minus 1 j plus 1 i minus 1 j plus 1 is earlier the other one is later just like you have a sequence of letters you try to do lexicographic ordering of that right that is really what we are talking about so when you specify the loop the order in which it is going to execute right all of this is essentially the lexicographic order in all of this what we are assuming is that you have a loop nest n and the loop nest is typically written in what is called the canonical form for i is equal to 1 to n for j is equal to 1 to n for k is equal to 1 to n it is not like for i is equal to 1 to n for j is equal to n down to 0 it is not like that because when you do things like that this lexicographic ordering will have a different interpretation so let us assume for the time being everything is in the canonical form that means always going from 1 to n in steps of 1 actually in mathematics this is basically said well let me not go into the definition because i mean few people got an idea of what it is so for example let us look at this loop right i going from 1 to 4 j going from 1 to 6 so what is an iteration vector for this 1 3 4 2 these are all iteration vector what is the iteration space for this how many points are there in the iteration space 24 right from 1 1 to 4 6 right those points are being put in here there are some red arrows blue arrows and dotted arrows let us see what they are correct so as i mentioned earlier where you are always going to talk about these loops in the loop nest in what is called the normalized form or canonical form which is always going from 1 to n or 1 to m they need not have to be same they can be different like what you have but always going from some 1 to k right ok now this is the iteration space you can see that i going from 1 to 4 j going from 1 to 6 what is the iteration order the iteration order or the lexicographic ordering in this is this like this right that is the dotted arrow correct for i is equal to 1 j is going to go from 1 to 6 and then i will increase to 2 j will go from 1 to 6 then i will increase to 3 j will go from 1 to 6 that is the lexicographic ordering right therefore between this which one is earlier which one is later obviously this is earlier this is later right so if i have said yeah let us say 4 2 and then 4 2 and 5 1 which one is earlier 4 2 is earlier ok of course 5 1 is not here because i cannot go to 5 but anyway right ok this blue arrows and red arrows i think they are not really important but anyway i will briefly tell you they talk about the various dependences that are there in this program ok now let us just try to look at one or two of them this arrow here right if this element is x i j it says that x i j is dependent on x i minus 1 j is that such a dependency such a dependency is there for y correct so this dependency is possibly for this dependence between them correct what about this let us look at this again right if this is x i j this is x i j minus 1 got it the red arrows are anti-dependence ok so this is x i j minus 1 j is there any so that must be this one ok there is some loop independent dependent that is not being shown here because loop independent dependence is within this each one of them is one iteration of the loop ok right so typically whenever we talk about iteration space this is what we say iteration space and we also got an idea of what is a lexicographic order right again the same definition of data dependence we will go through this one more time we are going to see one or two more times but let us see that there exists a dependence from s 1 to s 2 in a loop in a loop nest if there exists two iteration vectors i and j such that i is less than j that means that i happens earlier j happens later ok and there is a path from s 1 to s 2 both s 1 and s 2 access the same memory location right that is s 1 at iteration i and s 2 at iteration j they access the same memory location again it is the same point that we talked about right for example here when i talk about 3 2 right y of 3 2 a value is being produced in the 3 2 iteration right that is iteration point that is going to be consumed by s 1 in 3 3 because this index is j minus 1 correct so that is really what we are talking about there are two statements s 1 and s 2 s 1 in iteration i and s 2 in iteration j access the same memory location that is when there is a dependence and in this case we assume that i happens earlier than j then it is a true dependence right i mean if yeah if i is a right i mean if s 1 is a right and s 2 is a read and so ok right there is a dependence here we actually are talking about just a dependence so it it just says that they both access the same memory location and one of them is a right that is all that you need ok ok now again the same points but now we are going to have a representation for this so there are three statements s 1 and s 2 and s 3 there is a two way nested loop correct there is a dependence from s 1 to s 2 because you produce a value x i j which is being consumed correct now let us look at this dependence s 2 to s 1 you produce it in the i jth iteration and you consume it in the i j plus 1 the value produced by this statement s 2 is going to be consumed by s 3 one iteration later in the j dimension and zero iteration later in the i dimension correct that is why it is i comma j plus 1 that is represented as a dependence distance of 0 comma 1 right now let us look at s 2 s 2 produces a value y of i j that is being consumed by s 3 in i minus 1 j that means it is one iteration later in the i th dimension zero iteration later in the j th dimension so that is written as 1 0 whereas this is 0 0 typically 0 0 is something that we do not write it means it is loop independent dependence correct it happens with the same loop s 1 s 2 same loop i jth iteration i jth iteration so when the dependence is in the same loop iteration same iteration that is called loop independent dependence when it is across iterations it is called loop carried dependence and the loop carried dependence can be in the i th dimension or j th dimension depending on that it is either 1 0 or 0 1 it is not even have to be 0 and 1 it can be a large value also i minus 4 i minus 8 i plus 3 right you could also have something like 1 minus 1 what does a 1 minus 1 dependence represent correct so what happens is that the value producing the i jth is going to be consumed in the i plus 1 but yeah sorry i minus 1 sorry yeah i plus 1 and j minus 1th iteration right that is also possible now you can think of this as having any value from 0 to n or n minus 1 plus or minus right okay so this is essentially what we call as the dependence distance take the vector i i mean sorry take the iteration vector i which is actually a two dimensional variable i comma j take the iteration vector here which is again another two dimensional vector i prime j prime whenever they access the same memory location whatever are these vector values you do a vector subtraction that is essentially what is the dependent distance okay mathematically that is how it is defined but intuitively you people now understand what it correct the intuition is more important you can formalize it as you go by okay now sometimes what we do is that we can actually specify this as distance okay in terms of dependence distance vector sometimes you are only interested in knowing whether it is positive or negative okay i i i will come back and then talk about that okay so as i said you want to know whether the value is greater than 0 equal to 0 or less than 0 if it is greater than 0 a bit confusing you put the less than sign right if it is equal to 0 you put the equal sign if it is less than 0 you put the greater than sign okay this kind of a representation is called a dependence direction vector direction essentially says that which way it is okay that is why it is slightly on the reverse direction all right we will we will always talk about it with regard to the numbers and then from there move on again the same graph right i have marked all the dependences i have also marked the directions whenever it is 0 we have put an equal whenever it is positive we have put a less than there is nothing which is negative here therefore you would not see a greater than sign this dependence is an anti-dependence okay that anti-dependence you remember that x of i, j plus 1 going to x of i j that is the dependence that we are that also is 0, 1 so this is the dependence distance representation 0, 1 this is the dependence direction representation direction so this is called the direction vector this is called the distance vector okay we will interchangeably use these things some cases it is only enough to know about the direction some cases it might be required to know about the distances okay so one last example in this case so let us look at this code now there is a triply nested loop so it has to be a 3-tip right a i plus 1 j k is being produced and then a i j k plus 1 is being consumed so what is the distance vector what is produced here in the i j kth iteration is going to be consumed in the i plus 1 j k minus 1 correct so the distance vector is 1 0 minus 1 and the direction vector is this okay all right we will come back and then this first non-zero vector has to be less than and things like that that is why I keep putting this also let us look at another example I have a two-dimensional loop nest and I am accessing a single dimensional array right now what happens here right there is a dependence from the i th iteration to the i plus 1 th iteration right and this iteration is there for all j iterations so in this case right the first distance vector is 1 the second distance vector is dash or star right so as long as you have i and i plus 1 whatever is the value of j does not really matter j is in this case like do not care correct so there is a dependence that is represented as 1, dash or less than, star there are also other complicated things but I will not get into this in this lecture right we want to kind of quickly get to some basic transformations and things like that so does this give you a good picture of what dependence distance vectors are and direction vectors are any confusion no okay good understood that as I mentioned earlier okay again look at the same definition of dependence there is a dependence from s 1 to s 2 if s 1 is an iteration vector i and s 2 in iteration j access the same memory location and if i is equal to j that is the two iteration vectors are same then it means it is a loop independent dependence correct that is the case of 0 0 if i is different from j then it is a loop carried dependence if the iteration vector i is less than the iteration vector j and i happens to be a right then it is a true dependence right if j happens to be a right then it is a war dependence and so on if both of them happen to be rights then it is a war dependence okay we can define that okay this is an important point let me try to spend some time on this so when i is not equal to j and the distance vector has to be positive we use the word positive again let us see we can go back to that example and then talk about it so whenever you talk about a true dependence right well not just for true dependence even for other thing but even for the other dependences you need this but let us first try to talk about it for the true dependence we say that a of i j k or the value produced in the iteration i j k right is going to be consumed by the iteration i i plus 1 j k minus 1 so there is a dependence from that vector to this vector that is what we say but this dependence it starts with a 1 0 and a minus 1 we call this vector right we call this dependence vector as positive right because the first non zero element is positive in this vector the first non zero element is positive right if the first non zero element is negative let us see what happens right okay let us again write the following a of i j k or the value produced in the iteration equal to a of i plus 1 j correct let us assume a two dimensional loop now what does this mean i am talking about some statement s 1 to itself okay now if i am looking at it from here to here right okay now if i am looking at it from here to here all i know is that can you say something about it what can you say about this if it had been i minus 1 you know that the value produced here is being consumed in the next iteration but now here is the case that the value in a i j must be consumed right before the i plus 1 jth iteration writes into it correct so in this case the dependence vector okay is minus 1 comma 0 right but that is if you are looking at it from the left hand side to the right hand side right and that kind of a dependence cannot exist cannot satisfy that it is not a true dependence if you think of this as a true dependence that kind of a dependence cannot exist because that means that i plus 1 jth iteration has to be executed before i jth iteration that is not possible so if you have a legal dependence in the legal dependence the distance vector will always have the first element as first non-zero element right the first non-zero element as positive okay if the first non-zero element is negative right either you have talk you are talking about the dependence in the other direction correct you are talking about the dependence in the other direction i am looking at a dependence from here to here right from this side to this side it would have been 1 0 right so any legal dependence has to be positive and by positive what we mean is that the first non-zero element has to be greater than 0 i mean yeah the first non-zero element has to be positive okay or we can also say in the case of dependence in the case of direction vector the first non-equal sign has to be less than both are same right so is that clear so whenever we talk about a dependence a true dependence that dependence i mean not in the case of true dependence like flow dependence what i meant to say is that whenever you talk about a dependence in a particular direction if it is positive then that is a dependence if it is negative then you are actually reversing the dependence that is really what is happen okay okay now we will talk about loop transformations there are several loop transformations that are possible right initially i will tell you what the loop transformation is and later on you can figure out where it is useful you also have to see whether these loop transformations are legal to be performed anything is legal only if it satisfies all the dependences otherwise it is not legal okay let us look at it first thing is loop unrolling do we know this right so essentially it is duplicating the loop body multiple times okay so unrolling it twice essentially means this right is loop unrolling legal loop unrolling is always legal okay as long as the index variable that is the counts are actually taken care of in the proper way right it is always legal right where do you do loop unrolling we did right we did loop unrolling specifically for instruction scheduling increasing the size of the basic blocks right you do loop unrolling right loop rolling is the opposite of unrolling correct so here is a very complex set of statement but if you look at it everywhere i have z k z k plus 1 k plus 2 k plus 3 x k x k plus 1 k plus 2 k plus 3 so it is a four time unrolled version right or five times unrolled version that can be rolled back into this version right typically this transformation is never done but just for our understanding i put that loop peeling let us look at loop peeling right now in this program or in this piece of code i goes from 1 to n a of i is a of i plus a of 1 right now try to understand analyze the dependence for the statements the first iteration is obviously dependent on itself a of 1 what about the second iteration in the first iteration we would have produced a of 1 the second iteration a of 1 is being consumed in the third iteration a of 1 is being consumed in the fourth iteration a of 1 is being consumed so essentially in this case what we have is that all iterations are dependent on a of 1 so we cannot execute this loop in parallel or we cannot vectorize correct that is a bad thing because if i have a vectorizable a processor like a v x or something available and if i have this loop i cannot vectorize it right can you think of some simple transformations that can be done sorry tell me store a of 1 in some other but then where will you do that a of 1 writing inside that loop right so you do not need to necessarily store it in a separate array just do iteration 1 outside of the loop right that is what we call as peeling remove the first iteration the name means what it is right do not be too surprised right so every iteration depends on iteration 1 so take out iteration 1 right from the loop then what you have is you have this piece of code a of 1 is a of 1 plus a of 1 whereas a of i is a of i plus a of 1 now what happened is that here all of these statements are dependent on this one so once you execute this serially the rest of it all can be executed in parallel so you can vectorize this execution whereas you cannot vectorize this execution this is loop peeling from the beginning of the iteration what about loop peeling from the end of the iteration right if it is dependent on a of m something like that would happen and you have to do ok alright now let us look at another loop transformation called loop fission it is also called loop distribution oh boy so sequence of statements right for i is equal to 1 to n right can I break these things in terms of for i is equal to 1 s 1 for i is equal to 1 s 2 for i is equal to 1 to n s 3 for i is equal to 1 to n s 4 can I do that will that have the same meaning as this one you all understand right I have 4 statements which are put in the loop and I am executing this s 1 s 2 s 3 s 4 s 1 s 2 s 3 s 4 like that I am executing but what I want to do is that I want to execute s 1 s 1 s 1 n times s 2 s 2 s 2 n times ok let us look at what happens right so this produces a value in the ith iteration which is going to be consumed by s 2 in the i plus 1th iteration so if I execute all iterations of b 2 I mean all iterations of s 2 then this will take the old value of c it will not take the new value of c so obviously I am breaking this dependence correct anything else that you can see this also b of i b of i minus 1 correct right so if you have a loop carried dependence then you are going to have a problem right but you can handle it let us see what we can do right so again we will build a dependence graph and then annotate it right and then in the dependence graph we will identify what are called strongly connected components all of you know about strongly connected components right it is a subset of the graph where every node is connected to every other node right and then combine all these strongly connected graphs into a single node right and then do topological sorting right so let us see how that works this is the program ok now this is the dependence ok here the distance vectors are only one dimension therefore I have just represented them by a number again you can see that b of i b of i minus 1 so that dependence is here c of i c of i minus 1 ok there is a dependence from s 1 to s 2 to s 3 that is a loop independent dependence correct so that is also put in here s 3 to s 4 and then from s 4 to itself right so all of these dependences are marked in this graph now which is the strongly connected component here s 2 s 3 correct so combine them into one node right then s 2 s 3 is a single node there is an edge from s 2 s 3 to s 1 there is an edge from s 3 to s 4 and of course there is a loop on s 4 right you have marked all of them right now what do we understand from here if I understand s 2 s 3 together in this form together meaning that there is a single loop for that that is for i is equal to 1 to n s 2 s 3 so it executes s 2 s 3 s 2 s 3 s 2 s 3 like that right and then after that if I execute s 1 n times and after that if I execute s 4 n times or in any order it does not really matter correct I would have met all the dependences right let us see that in the next slide ok correct now does this satisfy all your dependences yes because the c 1 to c c to c dependence is satisfied because this is going to be executed in this way right there was a dependence from b of i to b of i minus 1 which was actually loop carried and it was going backward but now what we have done is that we are going to execute b first and then a later so that is going to go like this that is also taken care of d of course only had a self dependence there was a loop independent dependence from c to d that is also being satisfied now you have this 4 loops which was originally put as a single loop you have distributed them what can you do with them right some loops can be parallelized some loops cannot be parallelized right which one can be parallelized a of i is equal to a of i plus b of i minus 1 can be parallelized right I think that is coming over here ok just went to one more level right it can be parallelized what about this one this cannot be vectorized because there is a self dependency so it cannot be this also cannot be vectorized because there are two statements and one depends on the other and then vice versa so that cannot be parallelized so if you are given a original loop like this by analyzing all the dependences and doing these sequence of steps you can actually reduce it to this right and then you can parallelize some of that so the compiler can do this no longer magic correct so well defined sequence of steps that can be done ok now before we go more into this now tell me where do you think it would be more appropriate to do this kind of transformations or optimizations after we do register allocation instruction scheduling register allocation and other things because after you do register allocation instruction scheduling in what form are you seeing the loop you are seeing the loop as a sequence of statements with some control branches right this structure of the loop is not really seen at that level whereas if you have looked at this loop at a much earlier point in time like the AST or even before that right you could have actually had information about the loops and other things you could have had see for example this array accesses are being looked at as B of I and C of I minus 1 whereas if you are looking at the code after instruction scheduling or register allocation right we discussed at length about this in the afternoon right it is going to be seen as some index you would not know I or J or I minus 1 or J minus 1 nothing will be there that is the last case right so many of this optimization that we talk about in terms of loop transformations they are often done at the early part of the compilation okay so do not think that all optimizations have to be done after the intermediate representation not necessarily and it is also not that all optimizations have to be done only on the machine code or the output of the optimization has to be on the machine code in this case what we have done is we have taken the original loop and then we have rewritten it as another loop so in some sense this is a source to source transformation so some of these optimizations may happen at a higher level it may not be in C code it may be in AST or it may be in some appropriate representation but it happens at a slightly higher level if you go to the level of instructions or three address code or something below that many of these information should have been lost and you may not be able to do that right so you have to now understand a different that something that we have not seen in the last two days right you are saying that oh take the three address code do that or take the machine code do this right now you are also going to do things at the higher end to optimize certainly so again depending on what optimization you want to perform there is an appropriate place and representation on which you have to perform that okay you are doing okay in terms of time let me just go a little bit for that so here is a nested loop making things a little bit more complicated let us see what is happening here again this is the code okay now we will introduce one new point here okay so again I have represented the dependence distances correct so this dependence distance is 1 0 this is 1 0 this is 1 0 and this is 0 1 okay that possibly is this from this C of i j to C of i, j minus 1 in all other cases it is i plus 1 to i okay that you can see i plus 1 to i i plus 1 to i okay let us say that in this particular loop in this nested loop I am only interested in parallelizing the innermost loop or vectorizing the innermost loop typically only innermost loops are vectorized right let us say that we are only interested in vectorizing the innermost loop if I look at this code and look at all the distances dependence distances this I am only interested in vectorizing the innermost loop and let us say the outer loop is going to be executed sequentially correct then I do not have to worry about this dependence because the outer loop is going to be executed sequentially right this dependence is always going to be satisfied this dependence is also going to be satisfied because the outer loop is executed sequentially this self loop is also going to be satisfied so what I need to make sure is that whether this is going to be satisfied or not so let us look at it so when I do this and only consider vectorizing the inner loop the only dependence that I need to care about is the dependence from s1 to s2 and what is the dependence s1 produces C of ij s2 consumes C of i, j-1 now the question is can I do loop fission on the innermost loop I can write I can write this as a for j loop I can write this as a for j loop I can write this as a for j loop as long as I execute this for j loop after this for j loop this dependence is satisfied correct so when you are looking at vectorizing the innermost loop you can actually ignore all dependences which are on the outer loop okay correct all dependences which are on the outer loop can be ignored and now your dependence graph has become like this there are no strongly connected components so nothing needs to be merged right as long as you satisfy this ordering of s1 followed by s2 they can be distributed and they can be executed in any order only thing is s2 has to be executed after s1 I could have first executed s3 then s1 then s2 that is also possible right so I have shown this representation form that is why you do not see an inner for loop right so I have distributed it and I have shown it in the vector representation which essentially means for j is equal to 1 to n C for j is equal to 1 to n this for j is equal to 1 to n okay now each one of this loop can now be vectorized so you can do AVX on that or whatever okay right so again it is possible because we did loop fission on that okay there are additional optimizations that you can do to enable you to do vectorization we will do one or two of them okay yeah let us see this one so let us say statement s1 writes x of i statement s2 uses x of i and x of i plus 1 okay so in this case what happens is that there is a dependence from s1 to s2 true dependence from s1 to s2 which is loop independent there is a var dependence from s2 to s1 which is loop carried correct because this value has to be consumed before the next iteration executes otherwise you will overwrite this right so in this kind of a situation you can neither write I mean you can either distribute or you cannot distribute this loop with s1 as the first statement or s2 as the first statement because there is a dependence from s1 to s2 there is a anti dependence from s2 to s1 correct so this essentially causes a problem and what you need to do is that you kind of need to use another array for x instead of one this thing so let me show you what you do you first copy x of i into an array called t of i right and then do the statement s1 which modifies x of i then do the statement s2 using the copied array right you can see that this has the same semantics of this right you originally you are writing x of i using that value of x of i and using the old value of x of i plus 1 correct the old value of x of i plus 1 is being copied into t of i right now I am using t of i and x of i where is the new value so new value plus old value divided by 2 I am able to do that now I can do fission of this and then I can parallelize all three statements right again the only need to make sure that s naught happens before s2 s1 happens before s2 right that is the dependence that I have to preserve yeah if you want to parallelize that you have to have a blocking mechanism but such blocking mechanisms are expensive to implement okay so oftentimes you say it cannot be parallelized see if you want to do blocking for every iteration it will essentially become sequential right therefore you say it is not possible whereas now by doing this copying I am just saying that I can do them in parallel I do not know whether you got the answer that you are looking for no so let us look at this so this is what we have transformed it correct now I can execute all statements of s naught in parallel nothing prevents me I can copy this entire xra into tra with a shift without a shift also I could have done but with a shift is what I have done correct I could do for all the n elements in parallel right after I finish this right I do s1 again I do all the values of x of i in parallel that means that this code can also be vectorized right after that I do s2 that code can also be vectorized so there is no blocking now the moment I have renamed this variable or copied this variable into another array I broke one of these dependences I broke this anti-dependence in terms of this true dependence and this copied array so now it now you can actually topologically sort this graph s0 s1 s2 topologically sort this graph so there is no problem once you can topologically sort you can actually do loop distribution and the moment you do loop distribution each one of those distributed loops can be vectorized right as long as there is no other dependency which prevents it to vectorize right so it can be vectorized okay one last transformation for vectorization is this one let us look at it here I have a variable t a scalar variable t a scalar variable t which is some a of i plus c of i minus 1 then that scalar variable t is also being used in the next statement again I have the same situation that there is a dependence and there is an anti-dependence correct now again I can break this by making this t of i making this t from a scalar to a array so this is called scalar expansion so instead of having one variable t I have an array of variable the moment I put this as an array of variable only the true dependence remains the anti-dependence has gone because for each iteration I have a new temporary right that is essentially what it is so this can also be parallelized okay now let us come to the familiar things so here is a loop where I say for i is equal to 1 to m for j is equal to 1 to n do blah blah okay now can I interchange the i and j loop here do not worry what it is going to how it is going to help but can I interchange right and write it like this let us look at let us look at the dependence right x of i j is equal to x of i minus 1 j right so the dependence here is 1 0 right now if you interchange the loop put j i the dependence is going to become 0 1 right now the question to ask is that is 0 1 the legal dependence vector what did I say about legal dependence vector the first non-zero element has to be positive is 0 1 illegal vector or not yes therefore it is possible to do interchange on this that is essentially what the previous slide was trying to tell you in mathematics okay I was just trying to give you the intuition behind it so in this case loop interchange is possible so now tell me where loop interchange is not possible so if instead of having x i j is equal to x i minus 1 j if I have x i minus 1 j minus 1 that is not a problem a plus 1 that is very good right if I have here as x i minus 1 j plus 1 then what happens is that I have a 1 minus 1 dependence and when I permute them I have a minus 1 plus 1 and the first non-zero element is negative therefore it is not a legal vector so we are violating some dependences so that is not possible okay now let us also see pictorially what is happening here okay the same example right x i j right i minus 1 j right now see this is how the loop is executing correct the dependence is from x i j to x i minus 1 j that means that for the same j value but the previous i value right what is being produced in i j is consumed in i plus 1 j so the dependence is this blue arrow right this was our original execution order now if I want to execute in the other order am I satisfying the dependences or not is what you need to see right if you do loop interchange this is what it is are you satisfying the dependences or not right so if you are going this way this dependence is satisfied if you are going this way also that dependence is satisfied so there is no problem right of course you cannot be checking it like this this is for our understanding the way to check is look at the dependence vector permute them and then see if the permutation does not result in a illegal vector then it is a possible thing okay so here is your example for a case where interchange is not permissible right as you can see I have okay this is actually an anti-dependence from here to here that is what we are talking about that is why the arrow is red rather than blue so the anti-dependence vector here is 1, minus 1 right that is anti-dependence because we are seeing it from here to here okay it is only an anti-dependence there is no true dependence here all right now if I interchange the loop look at look at the execution order right when I am executing this element I am using the value of this element which I have not even visited that means it is a old value correct so that is why I call it as anti-dependence right now if I do loop interchange which order am I going then what happens now I am using the new value which means that I have violated my anti-dependence so it is not a valid transformation so the anti-dependence here is actually 1, minus 1 which is same as what we discussed earlier and that anti-dependence is a right cannot be satisfied when you do a loop interchange okay there you go right so this is the mathematics behind this transformation can be represented as a matrix and then you can multiply it with the dependence vector and then you see whether the resulting thing is this which I kind of explained as look at it as permutation of the bits and then you will get permutation of the vectors and you will get okay now let us look at this example and then see if I still want to perform a loop interchange can I do that right in this case I do what is called loop interchange and reversal right let us see what that is okay so I have done the J loop and then the I loop I have done the interchange but now what I am doing is that for the J loop I am going from n minus 1 to 1 that means that this is the order that I am so for J is equal to 5 I goes from 1 to 4 and then for J is equal to 4 I goes from 1 to 4 and so on now is that anti-dependence is satisfied yes so now what was happening is that you had that 1 minus 1 vector you did a permutation which resulted in minus 1 comma 1 which is not legal then you did one more transformation which is the loop reversal and loop reversal has this effect of changing the order so that minus 1 1 has now become again 1 comma minus 1 so you were okay with that okay again I do not want to go into all the mathematics because it will take quite a bit of time to do that again I think I am going to skip all of this parallelization because we do not have time for this I was trying to hide this but when it crashed it did not necessarily save this so a lot of things about parallelization I will leave the slides with you you can have a look at them I just want to cover one aspect of locality before we close okay so again as we mentioned why do we want to do loop interchange right loop interchange is typically done when you want to exploit locality right morning we saw that example right where you had this i j and you have this two-dimensional array and the loop was written J i right and you are accessing B of j comma i and you are going right column wise which was bad right improve your locality what you could have done is you can do a loop interchange but you can do a loop interchange only if it is legal now let us look at this example of matrix multiplication right which is another interesting example right so in the matrix multiplication what you have is that you have two inner loops right and you are trying to access let us look at this right why array is being accessed i comma k right and k varies faster that means that you are going to access this in the row major order there is no problem whereas z array is going to be accessed as z k j k array is varying faster than the j array sorry the k index is varying faster than the j index that means that you are going column wise so this is going to have lower locality not only that both of these arrays are going to be repeatedly accessed because you have an outer loop also correct so if you remember matrix multiplication you take each row multiply it with every column correct so first row first column first row second column first row third column first row nth column right so if you go access all the elements of the first row then come back and try to access the elements of the first row again if it is not there in the cache then you have a conflict miss or a capacity miss depending on whatever happens right so that is actually basically you just brought this entire row one after another one cache block after another but then again if you want to access it it is not there for you right that kind of a situation is not what you want to end up it so trying to access it column wise you can at least understand because you go column wise you are actually filling in new block every time so you will go n new blocks before you come back to the same block again whereas when you are going row wise you are actually doing n by 4 or n by 8 depending on how many elements are there it is the same set of things that you are bringing in right so let's see what happens in this example again I have given all the details here I don't want to go through this let's look at these two arrays right so essentially what you are trying to do is that you are trying to multiply y with z that means that right ik is going to multiply it by kj right now if I go through this entire set of elements and then come back here assume that this is an array of 1024 elements right 1024 elements means roughly we are talking about 8k right again another 1024 elements here right that's another 8k 16k your cache block is gone by the time you come back here even if you had all of these things in the cache right by the time you come back here something would have been replaced so you would not see this again right this case of course is pretty bad because you are trying to go through it column wise right instead what you can do is so this is essentially how the accesses proceed right for one row you are trying to access all the columns right so if you want to exploit both spatial locality and temporal locality my slides are slightly out of animation slightly out of order try to do what we are trying to do take a small block of elements right and another block of elements here so maybe this is an 8 cross 8 matrix of this 1024 by 1024 try to do the multiplication of all of these elements with the multiplication of all of these elements right where else are you going to use this you could use this with this and so on similarly when you talk about this you are going to multiply it with this multiply it with this and so on right so now what you are going to do is that you are going to do this multiplication in what is called as a blocked multiplication right and what does that really mean is this following thing right instead of i going from 1 to n in steps of 1 right actually here yeah I have only done tiling for the j loop and the k loop okay the j loop is being tiled and the k loop is also being tiled you can see that the j loop right goes in steps of block size and the k loop also goes in steps of the block size and the internal loops go in steps of 1 so essentially what we are trying to do is that we are trying to take a block of elements trying to multiply with the other block of elements and trying to maximize reuse at least in one dimension that's really what is important okay let's see whether we can see that here so what I need to do is that once I take this I can multiply it with this right I can also multiply it with this I can also multiply it with that and so on right and if I finish all of this then I would have finished using this fully then I can take the next one and do things like right so that depends on the order in which you are trying to do this now if you try to look at this from a loop transformation perspective right what we are trying to do is that the j loop that you have here right you are essentially done what is called a strip mining that means that you have instead of doing for i is equal to 1 to n you are saying that let us do for i is equal to 1 to n in steps of 4 and then within that you have another for loop which says i is equal to whatever current value of i plus 1 like that you are doing so that goes 4 times whereas the outer things goes n by 4 times so together it has now you have a sequence of loops and you are trying to do interchange of them loop interchange of them that's really what's happening okay so with that I think I will stop