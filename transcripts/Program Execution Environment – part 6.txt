 So, everyone got the L1 cache how it works and so on. Next quiz, so we saw yes. Typically whatever happens I mean it depends on how the layout of the processor is done. So, if your cache is present along with the and the MMU is outside then you will have those as virtually indexed. So, whichever cache come on the layout before the MMU comes into picture they will typically be virtually indexed anything which comes after that there is not much benefit of having them as virtually indexed because you would have anyway done the translation. I do not know because it could vary from processors to processors like in the GPUs L2 is still outside the this thing and MMU is also outside. So, it is more of a tradeoff between what processors design wants to do, but conceptually you can think of it like if the address translation is part of some other unit which lies beyond the cache then you get benefit out of having virtually indexed cache. What is the exact specific architecture doing could be their own set of tradeoffs because they have architecture design it just a bunch of tradeoffs involved from here and there. So, they have lot of factors into consideration before deciding that. Now let us look at this thing. So, what I said is you have the kernel space as part of the process address space. Why is this done that way? Why have kernel mapped as part of the user process? Why? What are the benefits? Correct. So, the main advantage is what he is saying. So, whenever a process needs to do a system call it will enter into the kernel space. Now imagine if kernel was not part of the process then it would have its own set of page tables. Now because of that the page table switch needs to happen. Now as soon as page table switch happens TLB needs to be flushed because TLB no longer holds the correct context of the virtual address. So, it is beneficial to have as part of the same process. Now how many of you have heard of a recent security threat called Spectre and Meltdown? So, Meltdown Spectre is slightly different. So, Meltdown was actually a threat which presents in most many processors including x86 ARM and so on where due to some security hole you could actually access kernel space. So, you could access the entire address space without any problems and that was done through a loophole which presents which is typically present in these architectures. And the only solution in that case is something called as kernel page table isolation or KPTI for a short form. And what KPTI does is essentially takes out the kernel space from the user address space and puts it separately. Now consequence of that is what he was saying that every time now you have a system call you are actually switching the page tables. So, this kind of had lot of performance issues and Linux community was not very happy with this hardware bug because they had to lose lot of performance because of this. Now just like we have address identifiers there is also process identifiers which are used for page tables. Some it is not again in all processors some processors have it, but it essentially allows you to avoid TLB flush even when you swap the page tables. So, even when you switch the page tables you do not have to flush the TLB in the same way because what you are storing in TLB is not just the virtual address to physical address mapping, but process identifier plus virtual address mapping to the physical address. So, some processors do have that even in the TLB, but again with those processors the performance was not so good, but we cannot do much. So, as of now most of the Linux kernels the latest versions will have kernel page table isolation applied where the kernel space actually does not live in this. So, this is historical now. The next quiz, so I have this code what this code is trying to do is it is contiguously trying to malloc 768 MB and whenever it runs out of the memory it breaks out of this infinite loop and sorry there is a typo this should have been counter, but and we count how many times we were able to successfully allocate 768 MB. Now let us say I run this program on two machines one which is 32 bit one which is 64 bit what will be the relationship between the count. Does count determined by physical memory which is present or will it be dependent on 32 bit versus 64 bit. But if you do not have that much physical. Now let us say you have a machine which has 32 bit which is 32 bit with 4 GB RAM and you have another machine which is 64 bit with 4 GB RAM what will happen will the counter be same. Why? Why? And then we will switch to the next example where we will look at that. So, essentially remember all the addresses which need to be returned to the program need to be virtual. We saw that earlier. So the pointer which is returned by malloc has to be virtual. So malloc has to at least allocate that much virtual address space at least that much virtual address space needs to be allocated. Now if malloc cannot find this much contiguous virtual address space it will error out. So if it cannot find 768 MB in the heap region it will error out and virtual addresses have to be contiguous because that is how pointer arithmetic works. Now chances of running out of finding contiguous 768 MB blocks in a 64 bit process versus a 32 bit process 64 bit process has much more chunk available of 768 MB just by because you have more address space available you probably have more chances of finding such a thing. So leaving aside the physical resource for a moment if you just look at the virtual address space the 32 bit machine will run out of this allocation sooner than 64 bit machine. Does everyone agree with that? Now let us look at the other example. Here actually sorry I should have changed this. So I have these two codes. The left one is same as the earlier one which I showed. The right one is actually using caloc instead of malloc. What is the difference between the two? So caloc unlike malloc will actually write zeros into the memory. So caloc actually has to physically access that memory to put zeros. Now the way malloc actually works is it only allocates the virtual addresses. It does not actually give you a physical address. So whenever you do malloc, malloc will give you a contiguous virtual address space but all those pages will not have any backup in the RAM. They will not be mapped. As you start accessing data into that that is when it will actually start allocating physical pages for it. And that is what he was saying that it depends on how things are implemented. This scheme is called as copy on write. I do not know if you have heard of this term. So most of the operating systems support something called as copy on write. So what happens is you write some only when you write something is when you get physical space for it. Otherwise you have just inherited whatever was earlier present. You do not have any physical back. So in this case the code which is with caloc will actually run out of memory when you have run out of either address space. So it depends on what you run out of sooner whether you run out of the address space or whether you run out of the physical resource. Does this make sense? Now assign yes. Better implementation of malloc simply means a better way to track the scheme. The overall not how lazily memory is actually allocated. Actually I do not have time to cover memory allocators in detail. But the way memory allocators actually work like if you want to write your own malloc It is not that these things you have to write because these are more of a services provided by the operating systems. So you have to build on top of it. So unless you are writing the operating system kernel itself where you get to choose these policies a memory allocator is simply keeping track of what is free and what is allocated. And then the better so when someone says I have a better memory allocator what they are saying is I have a better implementation which finds free blocks quickly or has some ways to compact things. So it is all abstraction which is created. They are not changing the policies provided by the operating system. Is the distinction clear? So what we talked about was actually a thing in the operating system. Copy on write is decided by operating system not by malloc versus calloc. It is a OS policy because of that behavior of malloc and calloc might appear different. But if you know what happens behind malloc behind calloc then you should be able to reason about it. Now the assignment for you is something like this. You have to write a program which will print addresses of different portions. So write a program which takes which prints address of some function which prints address of some data which prints address of some variable in the stack and which prints address of something in the heap. And verify if they match to something similar to what we had seen here. Where the function addresses should be something like this. So that might be very close to the kernel and so on. Remember to compile with dash M32 otherwise you might see completely random stuff which is not shown in this diagram. So whatever concepts you are trying to learn I think the best way to understand them is to be able to write some program which can demonstrate that concept. Otherwise that will make your knowledge much more grounded and you can observe things. Now one of the things which you can do which is comparatively better than this is using some commands. So let's say I want to actually observe the memory map of how things are. I cannot keep doing this because there might be some things like we know before main there is also something called as underscore start as of now. So we can't exactly get what is the stack layout and so on because there might be some other things which are hidden behind my back which are also occupying space. So you can actually print memory map of a program. So for that you have to find what is the process ID. So there is a command called pgrep. Everyone is aware of command grep. Grep is to search something. So pgrep searches for a process which are running and then I can actually use a command called pmap which shows me the memory map of that process. So let's try that. So let's say I have So let's say I have some proc.c. To ensure I can keep poking the binary I will keep it running. So I will put a scanf so that the process is kind of waiting for something to happen and it doesn't run fast enough and I can't observe it. So now I have this dot is left. So this process is now running. Now let's switch back to other terminal and look for where is a dot out. So it says the PID for this is this one. So I will use that and this is the actual memory map of this. So it says that there are some segments which are loaded at this address which have read and execute permission and these are of 4k. Now this is the interesting part. So you remember we said that you will get secfault only when you cross the segment boundaries. So you can see that so if you are on stack if you access up to 132k bytes you will not get a secfault but if you do something beyond that you will start getting secfault. Similarly it says that there is some 4k segment which is read and write then there are bunch of other things. Then there is libc which is there. With libc you can for now assume it's something which provides all the things which you were using so far printf malloc and other things and this is also loaded and remember this is dot so. So this is shared object which we will see how they work and then there is ld dot so. This is also something which we will see later. If you look at the map which we had so it had stack and memory map region for shared libraries. So this is all the shared library region which I was showing in the memory map and then there is a stack. Does that make sense? You can also observe the same thing using a different command called slash proc. So you could do cat slash proc slash pid slash maps and this also prints similar information. It actually let me dump this to a file so it's readable. So this actually prints the similar information. But it also prints the exact paths on where the things are loaded from. So libc is loaded from this location. Now what is this saying is this segment whichever was formed was loaded from a dot out. Now remember we had said that from the program code segment will be loaded, data segment will be loaded and bss segment will be loaded. So that's where these come from. So what it is saying is from executable these three segments have come, from libc these bunch of things have come, from ld these things have come and this is a stack. Everyone got how we did this? So you should be able to observe this using these kind of commands. Any doubts so far? So just to quickly revise since morning we looked into details of what are static libraries and then we looked into details of how processors are created and how virtual memory is useful in creating all the abstraction which we know of for memory. Now let's switch on to loader on what loader actually does. Now I will actually not get into lot of details of loader because I think dynamic linker is much more interesting topic than this. So what happens is what all loader needs to do? We saw loader has to create the memory map. Loader is essentially responsible for saying allocate some space at this virtual address in the stack. Take some things from executable code segment etc. Copy them at this virtual address in the process and so on. So loader is essentially responsible for getting data from disk and actually loading it into the memory. And then loader will actually start executing the program point which was specified in the elf. Remember in elf I had showed you that there was something called as entry point? That is where once loader is done with its activities it will actually jump. Now what happens in the startup? So there is something called as CRT which stands briefly for C runtime. So what happens is when you are running your C program it's not that loader can magically start running it. For example, everyone knows that main takes some arguments. ARGC, ARGV. So who populates them? Someone needs to populate them and where will they get populated? Stack. Stack of main. So they need to be on stack frame of the main. So all of that setup which is needed to be able to start executing main needs to be done by underscore start. There are few other interesting things which underscore start does which we will see later. But underscore start is essentially responsible for start setting up the state so that main can be executed. Now start itself comes in an object file called as CRT 1.0. So many times you might see this error CRT 1.0 missing or something like that. And what linker does the start which it is said in the execute L file as the entry point it is address of the underscore start function. So what linker says is loader once you are done loading the program please start executing a function called as underscore start. And what does start does? So it does bunch of things before calling main. So it calls lip-c first to initialize the lip-c so that malloc and other things are initialized properly which might have been used. Then it calls init function which initializes some state. Then it sets up at exit handlers. So what happens is when your program exits it is not that it immediately exits. There are some post clean up which happens as part of at exit. So start function will say that once the program is done call these functions call those functions and so on. Then it finally calls main and then it will finally call exit which will actually return to the operating system. Is it clear? So essentially if you just want to conceptually remember this remember underscore start will set up the state for main to start functioning correctly. And there are few other things which happen in init etc which we will do in the last phase of the module. Is it clear? So I will not actually get into details but I will show you picture of what happens actually. So what I showed you was a simplified picture but what is actually happening before main is all of this. So it is not a simple thing although it appears that you just set up state and it works. What is happening is loader is calling start then lip see start main gets called and then there are bunch of other things and at this point main is called but there are bunch of other things which are happening. Now I will not get actually into details of what each of these are doing because this is actually too specific to an implementation. There is no concept behind it. So I would rather skip this. But you can look you can just search for these names and you will start getting some information on web on what these do. No. So on Linux there is no other way. So what happens is when you do exec loader will come into picture which needs to load which you have set and then it will do all of this before actually start executing main of that function. Now let us look at more interesting topic which is dynamic linking. So remember static libraries which we saw in the morning had disadvantage. One of the disadvantage was that the code is kind of present in every executable. So if I have printf code that will be present in the every executable file which is probably not desired. And bug fix in static library requires everyone to update and recompile their code. So and we overcome these by using shared libraries. So a shared library is an object file which can be loaded at runtime. It is not linked statically in your executable. It is loaded at runtime when your application starts. That's when it gets loaded. And it can be loaded at arbitrary memory location. And this process of loading this library is essentially known as dynamic linking because we are dynamically calling in some code which was not earlier present in my executable. So remember yesterday I was trying to tell you that executable is self-sufficient, doesn't have any other dependencies. That was true for static dependencies. If there could still be a dynamic dependency on from the executable to a dynamic library. And this work is essentially done by dynamic linker. And Linux will refer to these libraries as dynamic shared objects or DSOs. And Windows refers to them as DLLs, dynamic link libraries. Now how do you create, just like we saw how we create static library, we should be able to create a shared library. So let's say I have this code. I can do something like this where I can say gcc, temp.c, which is my code. And I specify a flag called dash shared, which is telling that instead of creating a normal relocatable object or executable object, create a shared object. And just like static libraries, the convention even for dynamic libraries is to name them as lib name.so. So can you all write this code and create your own shared library? Are you able to have lib my.so on your disk now? Okay, so that might happen if you don't have 32 bit dynamic linker and 32 bit runtime. Can you do this on the server machine where it is already there? Let's create it here so we can see. Okay, so is everyone able to do that? So let's write some client code also, which will use this. So I have a client code which calls into that library. And I will now compile my client code. So I will say gcc minus m32 client.c. And just like we tried giving the path for the static libraries, let's try the same thing. So I give l. which says find my libraries here. And the library I'm trying to link is my. Okay, one second. And it worked. Sorry. Yes because I am creating executable. So now I have a client.c compiled. Let me try to run it. It didn't run. It's saying while loading libraries libmy.so cannot open shared object file, no such file or direct. So simply being able to create library and specify it at compile time does not allow me to use that. Something else needs to be done. So unlike static libraries, you need to install dynamic libraries. And installation means you have to copy them at some standard location like userlib or something and run a command called ldconfig. So you have to put your shared library at some specific location to be able to use that. The other option is essentially when you are compiling your program, you specify where to search the shared library using dash wl command. So other option we have is we can use dash wl, dash rpath with dot. And now when I run it, it actually worked. So dash rpath says that at compile time, this is the hard coded location where I must look for my shared library. Now if the shared library is not present at this specific path, it is not going to work. So it is restrictive from that point of view because at compile time you are saying look for library which is present only at this specific path. If it is present in any other path, it does not matter. So this is kind of a problematic thing. So the other option is actually being able to use something called as ld library path. So you can say export ld library path and specify the path at runtime and then it will run. Now even if I put libmy.so at some other location, I will just change my ld library path and it will start working. Yeah, no that is okay. You are going into, so the shell instance in which you are running must have ld library path set. So that is a different problem what you are trying to say. Is it clear? So see what happened is static library was part of the executable. So when it was being run, loader did not have to search where to look for function foo because it was part of the executable. But that is strictly different in case of dynamic libraries because dynamic libraries get me are loaded at runtime. So dynamic loader needs to know where is this library present. Where should I find my foo? And it finds it by looking at some libraries which are at standard location. Otherwise if while compiling you had hard coded the path using r path, then it will look for that. Otherwise it will look for ld library path. Now how do you find out what all an executable depends on? So you can do a command called ldd which says list out all the libraries which will be loaded dynamically by this program. So there is something called as Linux gate, then there is something called libmyso, libc and then there is this ldlinux. Now let us try this. So let us say I do not set my ld library path and I try to run ldd. Now it says libmy.so not found. So it was not able to find and that is when I try to run this executable now I will get that error that I could not load what is libmy.so. So if you are any time getting this error, try to look what does ldd show and whether ldd is able to look up these paths. If it is not then you are guaranteed to get this error and then you have to set either ldd library path or change your rl path to point to that. So essentially what loader is going to do, it is going to find all the directories which are listed in etcld.so.conf. So this is the system variable file which can where you can add the default libraries of where to find them and then you can have either r path configuration or ld library path to have the dso's looked up. Now can someone guess why we have r path as well as ld library path? So ld library path what it allows you? It allows you to configure changing the library which is being loaded. So you may want to try with a different library and see running the things. But all of this activity is typically during development. But when you are shipping your software you do not want user to configure ld library path and you do not want accidentally some other libmy.so get into the picture. So for example let us say you had shipped libmy.so which did linked list. Then there is another libmy.so which pretends to do linked list but is actually doing something malicious and you set ld library path to that and application will start running that. So you do not want that to happen. That is why when you are shipping your software along with a dso you will say please use this specific version only which is at this location and typically that might have some privilege for you to change. So you may not be able to change. For example if it is installed in user then you need root permissions to overwrite a file in that. So there is some security. Does that make sense? So ld library path is really useful when you are doing development because you might say that okay I have this older version of the library let me try injecting new and whether it works and so on. But when you are shipping something you will configure your build to have hard coded paths. Now let us examine what all goes into the dynamic shared object. So remember yesterday I had told you that elf is a format which can be used to represent all three things executable, relocatable and dynamic shared. So if you look at kind you will actually see dyn which says it is a dynamic shared object. So if you look at redelf-h of libmy.so it is actually an elf file of type dynamic shared object. If you look at if you look this for a.out it is an elf file which is executable. So elf holds the information that this elf is being described for a dynamic shared object. Then there is entry point. Now entry point for dso is very interesting because entry point could be anything because it is being loaded dynamically. So depending on where is the space in the memory mapped region it will get loaded. Does everyone understand this? Because let us say you are running your program and you have so much data and some usage of stack and so on. Then the dso needs to be in the memory mapped region and there might be multiple dsos which your program depends on. So every dso starting point might be different. So the entry point address here is really from the start of the file. It is nothing more. So what this is saying is 3d0 is where the code starts from the start of the file. And how do we actually fix it? So relocation. So relocation will actually put it at some specific address and things will get changed as part of that. Now if you look at it, so this is the code. What this code is doing is it is accessing this global variable gbl and then some way adding that to parameters. Now if you look at the generated elf for it you would see that there are couple of relocations here. Now these relocations are on the dynamic section. And what they are saying is change address 500 with the address of gbl. What is at 500? We can see there are bunch of zeros. So what this is going to do is dynamic linker when it is loading this library it is going to change these zeros with address of gbl. Similarly here we see there is a relocation on 50a which is this. So this will also change with address of gbl. Similarly this will also change with address of gbl. So every reference of gbl which was earlier, see because the same problem happens. When linker is creating code for this does linker know where the gbl will go? Static linker does not know where gbl will go because the dynamic library may get loaded at different points in different processes. For example address of gbl in p1 might be 100 and address of gbl in p2 might be 500. So you cannot change this to 100 or 500 at link time. Does everyone get this? So the same problem which assembler had in static compilation, the same problem happens when linker is when compiler is trying to create a shared object file. Because it does not know where it will get loaded. Unlike your executable, in executable you knew text section is going to go here and section is here so you can have all of their addresses. And that is why we need relocations which will change this as part of dynamic link. So these will actually get changed by the dynamic linker. So when dynamic linker is done loading the library it would have assigned some address to gbl in that particular process and then it will change the address to that. Yes. Yes. We will see that. We will come to that. So every reference of gbl is kind of changed. this relocation is actually performed at load time. So using shared objects it is causing some additional work to happen as part of the application load. It is not coming for free. So your assignment is to figure out what happens when you do these function calls. So we saw that whenever foo was referring to gbl there were some relocations. Now your task is to write a function in dso and call it and see what kind of relocations get generated. So let us try to do this so we can observe here. So in this shared library I will simply call bar and So I have modified this code which was the earlier example which now has function call. Now let us try to compile this. Now if I look at the relocations there are some relocations which were for patching gbl which were present earlier also and now there is additional relocation for patching bar and it is of other type it is of pc32 which we saw yesterday because when we are doing function call we are not patching the absolute address of the function. We are patching the relative address of the function. Now if you look at the actual code using so and if I look at say underscore sorry foo. So if I look at foo I do see that foo has some relocations. So this call instruction now this fcff you remember we saw yesterday also this is minus 4 so the same thing has come up and it is going to change with address of bar. Similarly gbl wherever gbl is referenced there are relocations to patch the address of gbl. So in dynamic library what is happening is for every reference of the variable we are getting a relocation. So if you call bar twice you should see two relocations because address at both places need to be changed. Similarly gbl is kind of reference three times so its addresses there are three relocations for it and all these relocations need to be resolved at run time when the application starts. So the problem is this is this scheme actually works for supporting shared libraries because what will happen is you will have this shared library and once it is loaded by the loader it will resolve all these relocations once it is done loading it as some specific address. Once it knows the address of gbl and bar it will change the text section. Now the issues with this scheme are essentially first issue is remember we had relocation for every reference. So every time a function is called all those references need to be changed. Does everyone understand this and this needs to happen at load time that means your application will start running slower because initial overhead will be for dynamic linker to come up with the addresses of all these variables and patch them. Everyone understands this overhead which needs to happen. Now the second issue is actually more serious which is what I think that other person was trying to ask is the fundamental reason why we started off with shared libraries is because we do not want to duplicate code. We do not want every program to have duplicate code of common libraries like printf. Now with dso we are able to share it but what will happen when you are actually loading it references need to be changed and the reference the addresses with which the references will be changed will actually be different for different processes because depending on where bar is loaded into the memory. So in p1 process p1 bar might be loaded at address 100. In p2 bar might be loaded at address 500 and you will change the text section of bar and foo to refer to address 100 in process p1 and in process p2 you will change it to address 500. Does everyone get this? So can you really share code of foo? So foo which was calling bar in process p1 foo needs to call address 100 in process p2 foo needs to call address 500 that means the content of the text section is different. Does everyone understand this? And if the content is going to be different can you have one physical copy of that? No, because in p1 you need to refer to a version which is going to call the one with address 100 and in p2 you need the one which will be called with address 500. So you cannot share the code in physical memory which is what we started off as one of the goals which we wanted to have. So that is where the problem comes. So if you have relocations they are going to change things in a way that will make it process specific because the addresses are going to be process specific. So how to solve this? Why not reserve some addresses? So why not say that Lipsy is always loaded at address 100 for everyone that way the addresses will not change. So what are the issues with that approach? If we do that if we say that this DSO takes address 100 to 500 for all processes that way addresses will remain same and we don't even if we change the text section the content will still be same for all the processes. What are issues with such an approach? Correct. So if you reserve some addresses that means even if there is a process which does not need to use that DSO those addresses cannot be reused for something else. So very inefficient use of the address space plus if the library changes so for example today printf requires 5 bytes to store tomorrow it requires 50 bytes then how will you get more addresses because you have said printf should fit within only these addresses and then you create your own library then how will you get address space. So it becomes a problem of who is the authority to give you that these addresses are reserved for you and that needs to be consistent across everyone every process. So this is basically really bad idea we can't do with this. So that's where the position independent code comes into picture. How many of you have heard of this term PIC? So what position independent code implies is it's the code which can be loaded at any address without any modifications. And if we somehow have position independent code what it means is I could load it at any location in different processes and I don't have to change it. That means it truly works. So if we can somehow have position independent code the problem which we have currently at our hand will get solved. Now how do you obviously achieve this? Indirection. Every problem can be solved using additional indirection. So let's look at what assembler did. You remember yesterday we learnt assembler for 5 minutes where we saw that there are 3 types of addresses. There is PC relative address, there is section relative address and there is file relative address. Now the beauty of PC relative address is it is position independent. Does everyone get why? Because no matter where the text section begins the distance between JMP done and done is going to remain same. If I use section relative address that might actually change because I have to add something to get to the data section. So essentially PC relative addresses by design are position independent and no matter where the text section is loaded. So if JMP says JMP to 5 bytes after current instruction then no matter where it is loaded the same instruction is going to work. No matter what is the address of the JMP itself. Does that make sense? So this is the key idea which we will use. So when dynamic linker is linked, when the static linker is merging the objects it knows sizes of various sections. And it knows that when move refers to some data. So when we had a reference for GBL in the text section instead of using address of GBL why can't we use relative address of GBL from the current instruction. Does that make sense? So remember what we were doing earlier is when we had move instruction we were actually using the actual address of GBL variable. So if GBL was loaded at location 100 then we would say move from address 100 to some register. Instead we could say that move from relative address of GBL from the current instruction. And no matter where the base of this starts the code will still keep working as long as they are in the same contiguous region. And remember when we saw the memory map the text section and code data section were all one after the other. And those were laid out by linker statically. Nothing came in between that. So this is the essential idea of what we will do. So we will somehow generate code which will not use absolute address in the instruction but relative address in the instructions. And this is done by additional level of indirection called as GOT global offset table. I hope it is very easy to remember term now. So GOT you can think of as a indirection which exists. So what you do is for every variable which you have you store its address in a table called as global offset table. So global offset table holds address of every variable and it holds absolute address of every variable. So the GOT holds absolute address of every variable and GOT is at fixed distance from the text section. So whenever you have to refer to a variable you first look up where is the GOT entry for that and read the address of that and use that address to actually dereference. Is it clear? The indirection so from code instead of directly accessing the variable you first see what is the address of variable in the GOT and GOT is at fixed position as laid out by linker and then from GOT you get the address. So now one of the things is x86 supports instructions like call and jump which support PC relative addressing. That means the offset which you are encoding needs to be relative to the current instruction but the instructions like move do not really support relative addressing. So whenever I say move from this address hardware is going to interpret that address as absolute address. There is no way that you can say that move from five locations below current location. So there is no way. So how do you obtain PC relative data reference? So if you have a program counter so if I know my current instruction and if I know the absolute address I can compute the difference. So for example in so let us say my current instruction is here my PC is here and then I have bunch of other instructions and then I have my data section. I can obtain the relative address between PC and data by knowing the address of PC absolute address of PC and absolute address of data because I can simply subtract them to get the distance between them. Now how do you get program counter? So you still remember we had a trick yesterday which was the first quiz on how to get address of the current instruction. We get address of current instruction by doing a dummy call and popping from the stack and EBX now holds the current instruction and now we already know how to get absolute address of symbols and we subtract them and we get the PC relative address. Is it clear? Now this is what the code will look like. So whenever you have to access any variable you first have to get address of GOT and address of GOT can be obtained using the trick which I showed in the last slide because GOT is at some fixed position from every instruction. Once you have address of GOT every variable has a entry in GOT. So let's say you say GBL is entry 0, GBL 2 is entry 2, GBL 3 is entry 3 and so on. Then you get address of the variable from GOT and then that's what you use to dereference your code. Is the algorithm clear on how we are going to do it? So anytime you have to refer to a variable you first look up where is the GOT. Once you have GOT you know layout of the GOT that in what order symbols are laid out in GOT and then you use that entry to get the actual address of the symbol. So this is how the code will look. I'm sure when you were looking at this assembly you were seeing something like this x86 PC get THUNC CX. How many of you saw this? That there was something in the text section called THUMCX and THUMBX and so on. So this THUNC code is actually doing the trick which we did. So it is going to do a call and it is going to do a pop and return. So after this instruction CX registers holds address of the current instruction. And then we add to that the offset I mean this address will likely be negative and then we add to that the address of the how far GOT is from the text section which was decided by linker. So at this point ECX holds the base address of GOT. Then we essentially get to the variable which we wanted. So minus C holds the address of whatever variable we were concerned with and loads that address and then we actually load from that address to get the content of the variable. Is this clear? Because this is the key part of how dynamic libraries work. So remember to access any variable you have to first load somehow GOT. GOT you can load by knowing the PC relative address between your text section and data section which was done by linker. Once you have GOT then within GOT you have series of variables addresses and then you load address of whatever variable you want from that into the register and then you actually dereference it. The question is how is the address of variable actually placed into GOT? How does GOT know that GBL is at location 563 or whatever? Because the addresses will still be allocated at runtime. So compiler does not know or linker does not know what will be the actual address to be put in GOT. Relocation. So what we do is we generate a relocation on this entry for example. Since we do not know what is address of var 1 until it is loaded we can simply say var 1 holds some dummy content and there will be a relocation to patch this content with address of var 1. Is that clear? So var 1 in GOT needs to hold absolute address and unless it is loaded we do not know that address. So there will be a relocation to patch this address. So you can actually see this. So there is another type of relocation called R386 globe dat. What this is saying is patch the address of var 1 in GOT. This offset is actually in the GOT. Now I told you relocations are bad because of relocations we were not able to share the text section. So now if we are again going to generate relocation what is the use of all this scheme? Because of relocations which were present earlier we were not able to share the text section. Now we are saying to patch address of variable in GOT we still need a relocation. But as soon as something is patched which is different for different processes you cannot share it. Does everyone get the question on what I am trying to ask? So recollect what was happening earlier. So in earlier case what was happening is MOV instruction had a relocation which says patch the address of GBL in MOV instruction. Because of that the problem was if GBL was at different locations there was a problem because the code section cannot be shared between two processes. Now what we are saying is we did all this complicated thing and we said that MOV will now refer to GOT and GOT will have a relocation because GOT how does it know where is GBL? So are we not losing all the benefit which we created by this indirection? Correct. So one of the things which you have to understand is GOT is not in the text section. GOT is in the data section. So there are no relocations on the text section. Remember text section was only referring to GOT which was at the fixed relative address. And GOT what will relocation do? Relocation will change data in GOT and it will change it differently for different processes. But guess what? Every process needs to have its own data because things are going to be different. Because GBL in process P1 may hold different data than it may hold in process P2. Does everyone get this? So what we did by this is we shifted all the relocations which earlier were present in text section into data section. That way text section can now actually be shared. So when you have Lipsy loaded, what is happening is all the processes are sharing same physical copy of printf. Although printf itself might be loaded at different addresses for different processes. The data required by printf is still duplicated by every process because data needs to be different because processes may call printf differently. Does that make sense? So what this is allowing us to do is to be able to share the code. It's not sharing data. And shared libraries are for sharing code, not sharing data. Is this clear? Because this is I think the fundamental reason why DSOs exist and why there is this complicated mechanism. Now the library which we created earlier did not have this GOT and other things. So how do you create a library which had GOT? So you specify a flag called fpick when creating the shared library. So let's try that. So I had this library already. So instead of compiling it as normal library, I will compile it as pick library. Now what I see is if I look at the obj dump with relocations, you can see foo does not have any relocations. If you look at it, foo does not have any relocations. Unlike earlier case where foo had relocations to patch the address of gbl and bar and this and that. And there is we can see something called as GOT. So there is something called as GOT which is coming. And you can also see there are relocations on GOT. For some reason this machine has become very slow. So if you look at it, now these are actually some of the relocations which will be present on GOT. So it is asking to populate address of gbl in the GOT. Is everyone clear with that? So to create a shared library, we have fpick. So I think we can stop here for now and continue after the lunch.