[{'id': 0, 'seek': 0, 'start': 0.0, 'end': 21.68, 'text': ' So, let us start with data flow analysis. So, now in data flow analysis we will start', 'tokens': [50364, 407, 11, 718, 505, 722, 365, 1412, 3095, 5215, 13, 407, 11, 586, 294, 1412, 3095, 5215, 321, 486, 722, 51448], 'temperature': 0.0, 'avg_logprob': -0.28964682420094806, 'compression_ratio': 1.2318840579710144, 'no_speech_prob': 0.03828030079603195}, {'id': 1, 'seek': 2168, 'start': 21.68, 'end': 42.2, 'text': ' with four important class of optimizations. The first is what is called liveness analysis.', 'tokens': [50364, 365, 1451, 1021, 1508, 295, 5028, 14455, 13, 440, 700, 307, 437, 307, 1219, 375, 553, 442, 5215, 13, 51390], 'temperature': 0.0, 'avg_logprob': -0.22348623690397842, 'compression_ratio': 1.0975609756097562, 'no_speech_prob': 0.6094040274620056}, {'id': 2, 'seek': 4220, 'start': 42.2, 'end': 49.2, 'text': ' The second is...', 'tokens': [50364, 440, 1150, 307, 485, 50714], 'temperature': 0.0, 'avg_logprob': -0.9897500446864537, 'compression_ratio': 0.6666666666666666, 'no_speech_prob': 0.963653564453125}, {'id': 3, 'seek': 7220, 'start': 72.2, 'end': 93.2, 'text': ' The fourth is, we forget I will come to it later. Liveness organ, available organ, reaching', 'tokens': [50364, 440, 6409, 307, 11, 321, 2870, 286, 486, 808, 281, 309, 1780, 13, 8349, 553, 442, 1798, 11, 2435, 1798, 11, 9906, 51414], 'temperature': 0.0, 'avg_logprob': -0.7291136521559495, 'compression_ratio': 1.0963855421686748, 'no_speech_prob': 0.898573637008667}, {'id': 4, 'seek': 9320, 'start': 93.2, 'end': 100.2, 'text': ' the', 'tokens': [50364, 264, 50714], 'temperature': 0.0, 'avg_logprob': -0.3419767828548656, 'compression_ratio': 1.5689655172413792, 'no_speech_prob': 0.12202075868844986}, {'id': 5, 'seek': 9320, 'start': 100.2, 'end': 111.32000000000001, 'text': ' Okay, so let us first try to understand the four analysis. So, liveness analysis says', 'tokens': [50714, 1033, 11, 370, 718, 505, 700, 853, 281, 1223, 264, 1451, 5215, 13, 407, 11, 375, 553, 442, 5215, 1619, 51270], 'temperature': 0.0, 'avg_logprob': -0.3419767828548656, 'compression_ratio': 1.5689655172413792, 'no_speech_prob': 0.12202075868844986}, {'id': 6, 'seek': 9320, 'start': 111.32000000000001, 'end': 118.08, 'text': ' that if I have a program for every statement I would like to figure out. So, essentially', 'tokens': [51270, 300, 498, 286, 362, 257, 1461, 337, 633, 5629, 286, 576, 411, 281, 2573, 484, 13, 407, 11, 4476, 51608], 'temperature': 0.0, 'avg_logprob': -0.3419767828548656, 'compression_ratio': 1.5689655172413792, 'no_speech_prob': 0.12202075868844986}, {'id': 7, 'seek': 9320, 'start': 118.08, 'end': 122.4, 'text': ' it says that now I can talk in terms of control flow graphs because you all understand control', 'tokens': [51608, 309, 1619, 300, 586, 286, 393, 751, 294, 2115, 295, 1969, 3095, 24877, 570, 291, 439, 1223, 1969, 51824], 'temperature': 0.0, 'avg_logprob': -0.3419767828548656, 'compression_ratio': 1.5689655172413792, 'no_speech_prob': 0.12202075868844986}, {'id': 8, 'seek': 12240, 'start': 122.4, 'end': 126.4, 'text': ' flow graphs and for the time being let us assume control flow graphs which have only', 'tokens': [50364, 3095, 24877, 293, 337, 264, 565, 885, 718, 505, 6552, 1969, 3095, 24877, 597, 362, 787, 50564], 'temperature': 0.0, 'avg_logprob': -0.1726521687074141, 'compression_ratio': 1.5954545454545455, 'no_speech_prob': 0.18660221993923187}, {'id': 9, 'seek': 12240, 'start': 126.4, 'end': 132.64000000000001, 'text': ' single statements just to make our life easier. So, now in liveness analysis what I want to', 'tokens': [50564, 2167, 12363, 445, 281, 652, 527, 993, 3571, 13, 407, 11, 586, 294, 375, 553, 442, 5215, 437, 286, 528, 281, 50876], 'temperature': 0.0, 'avg_logprob': -0.1726521687074141, 'compression_ratio': 1.5954545454545455, 'no_speech_prob': 0.18660221993923187}, {'id': 10, 'seek': 12240, 'start': 132.64000000000001, 'end': 140.68, 'text': ' do is I want to figure out if there is an assignment to a variable x from that point', 'tokens': [50876, 360, 307, 286, 528, 281, 2573, 484, 498, 456, 307, 364, 15187, 281, 257, 7006, 2031, 490, 300, 935, 51278], 'temperature': 0.0, 'avg_logprob': -0.1726521687074141, 'compression_ratio': 1.5954545454545455, 'no_speech_prob': 0.18660221993923187}, {'id': 11, 'seek': 12240, 'start': 140.68, 'end': 152.32, 'text': ' onwards is there any future use of x or not. Right, why would I be interested to do that?', 'tokens': [51278, 34230, 307, 456, 604, 2027, 764, 295, 2031, 420, 406, 13, 1779, 11, 983, 576, 286, 312, 3102, 281, 360, 300, 30, 51860], 'temperature': 0.0, 'avg_logprob': -0.1726521687074141, 'compression_ratio': 1.5954545454545455, 'no_speech_prob': 0.18660221993923187}, {'id': 12, 'seek': 15232, 'start': 153.23999999999998, 'end': 159.12, 'text': ' Because if there is no further use of x along any path from x from this assignment to x', 'tokens': [50410, 1436, 498, 456, 307, 572, 3052, 764, 295, 2031, 2051, 604, 3100, 490, 2031, 490, 341, 15187, 281, 2031, 50704], 'temperature': 0.0, 'avg_logprob': -0.24360867797351274, 'compression_ratio': 1.5847953216374269, 'no_speech_prob': 0.0021145406644791365}, {'id': 13, 'seek': 15232, 'start': 159.12, 'end': 168.92, 'text': ' I can simply discard this statement and that is what is called dead code elimination. Right,', 'tokens': [50704, 286, 393, 2935, 31597, 341, 5629, 293, 300, 307, 437, 307, 1219, 3116, 3089, 29224, 13, 1779, 11, 51194], 'temperature': 0.0, 'avg_logprob': -0.24360867797351274, 'compression_ratio': 1.5847953216374269, 'no_speech_prob': 0.0021145406644791365}, {'id': 14, 'seek': 15232, 'start': 168.92, 'end': 175.76, 'text': ' the second thing that we would like to talk about is something called reaching definition.', 'tokens': [51194, 264, 1150, 551, 300, 321, 576, 411, 281, 751, 466, 307, 746, 1219, 9906, 7123, 13, 51536], 'temperature': 0.0, 'avg_logprob': -0.24360867797351274, 'compression_ratio': 1.5847953216374269, 'no_speech_prob': 0.0021145406644791365}, {'id': 15, 'seek': 17576, 'start': 175.76, 'end': 184.07999999999998, 'text': ' So, what does reaching definition say? Reaching definition says that let us say I have a program', 'tokens': [50364, 407, 11, 437, 775, 9906, 7123, 584, 30, 1300, 2834, 7123, 1619, 300, 718, 505, 584, 286, 362, 257, 1461, 50780], 'temperature': 0.0, 'avg_logprob': -0.20332678504612134, 'compression_ratio': 1.2151898734177216, 'no_speech_prob': 0.49702829122543335}, {'id': 16, 'seek': 20576, 'start': 205.76, 'end': 219.35999999999999, 'text': ' So, look at this particular program. Right, so what I might be interested to figure out', 'tokens': [50364, 407, 11, 574, 412, 341, 1729, 1461, 13, 1779, 11, 370, 437, 286, 1062, 312, 3102, 281, 2573, 484, 51044], 'temperature': 0.0, 'avg_logprob': -0.22597347161708733, 'compression_ratio': 1.462809917355372, 'no_speech_prob': 0.04712963104248047}, {'id': 17, 'seek': 20576, 'start': 219.35999999999999, 'end': 227.39999999999998, 'text': ' is that whenever you reach this particular basic block this particular expression x times', 'tokens': [51044, 307, 300, 5699, 291, 2524, 341, 1729, 3875, 3461, 341, 1729, 6114, 2031, 1413, 51446], 'temperature': 0.0, 'avg_logprob': -0.22597347161708733, 'compression_ratio': 1.462809917355372, 'no_speech_prob': 0.04712963104248047}, {'id': 18, 'seek': 22740, 'start': 227.4, 'end': 237.56, 'text': ' y has already been computed along all paths. So, essentially it means that oh sorry this is', 'tokens': [50364, 288, 575, 1217, 668, 40610, 2051, 439, 14518, 13, 407, 11, 4476, 309, 1355, 300, 1954, 2597, 341, 307, 50872], 'temperature': 0.0, 'avg_logprob': -0.24867273413616678, 'compression_ratio': 1.123456790123457, 'no_speech_prob': 0.8891339302062988}, {'id': 19, 'seek': 23756, 'start': 237.68, 'end': 264.56, 'text': ' available expressions. So, this is available expression. So, essentially in this case what', 'tokens': [50370, 2435, 15277, 13, 407, 11, 341, 307, 2435, 6114, 13, 407, 11, 4476, 294, 341, 1389, 437, 51714], 'temperature': 0.0, 'avg_logprob': -0.3846139453706287, 'compression_ratio': 1.2857142857142858, 'no_speech_prob': 0.24996531009674072}, {'id': 20, 'seek': 26456, 'start': 264.56, 'end': 274.6, 'text': ' we say is that this expression a star y is available. Right, what it means is why would', 'tokens': [50364, 321, 584, 307, 300, 341, 6114, 257, 3543, 288, 307, 2435, 13, 1779, 11, 437, 309, 1355, 307, 983, 576, 50866], 'temperature': 0.0, 'avg_logprob': -0.13456701131967397, 'compression_ratio': 1.6204819277108433, 'no_speech_prob': 0.18557725846767426}, {'id': 21, 'seek': 26456, 'start': 274.6, 'end': 284.8, 'text': ' I be interested in figuring out some expression is available or not. Right, so essentially for', 'tokens': [50866, 286, 312, 3102, 294, 15213, 484, 512, 6114, 307, 2435, 420, 406, 13, 1779, 11, 370, 4476, 337, 51376], 'temperature': 0.0, 'avg_logprob': -0.13456701131967397, 'compression_ratio': 1.6204819277108433, 'no_speech_prob': 0.18557725846767426}, {'id': 22, 'seek': 26456, 'start': 284.8, 'end': 291.54, 'text': ' instance let us say you have something like z equals x star y in that case what is the', 'tokens': [51376, 5197, 718, 505, 584, 291, 362, 746, 411, 710, 6915, 2031, 3543, 288, 294, 300, 1389, 437, 307, 264, 51713], 'temperature': 0.0, 'avg_logprob': -0.13456701131967397, 'compression_ratio': 1.6204819277108433, 'no_speech_prob': 0.18557725846767426}, {'id': 23, 'seek': 29154, 'start': 291.54, 'end': 304.14000000000004, 'text': ' optimization you can do? Right, so I can essentially compute I second say h equals x star y h equals', 'tokens': [50364, 19618, 291, 393, 360, 30, 1779, 11, 370, 286, 393, 4476, 14722, 286, 1150, 584, 276, 6915, 2031, 3543, 288, 276, 6915, 50994], 'temperature': 0.0, 'avg_logprob': -0.1640385954003585, 'compression_ratio': 1.7696969696969698, 'no_speech_prob': 0.5605619549751282}, {'id': 24, 'seek': 29154, 'start': 304.14000000000004, 'end': 313.22, 'text': ' x star y b is equal to h a is equal to h and z is equal to h. Right, so now essentially what has', 'tokens': [50994, 2031, 3543, 288, 272, 307, 2681, 281, 276, 257, 307, 2681, 281, 276, 293, 710, 307, 2681, 281, 276, 13, 1779, 11, 370, 586, 4476, 437, 575, 51448], 'temperature': 0.0, 'avg_logprob': -0.1640385954003585, 'compression_ratio': 1.7696969696969698, 'no_speech_prob': 0.5605619549751282}, {'id': 25, 'seek': 29154, 'start': 313.22, 'end': 318.14000000000004, 'text': ' happened is instead like if you look at every path this path x star y would have been computed', 'tokens': [51448, 2011, 307, 2602, 411, 498, 291, 574, 412, 633, 3100, 341, 3100, 2031, 3543, 288, 576, 362, 668, 40610, 51694], 'temperature': 0.0, 'avg_logprob': -0.1640385954003585, 'compression_ratio': 1.7696969696969698, 'no_speech_prob': 0.5605619549751282}, {'id': 26, 'seek': 31814, 'start': 318.14, 'end': 324.26, 'text': ' twice this path x star y would have been computed twice, but now with this optimization I am', 'tokens': [50364, 6091, 341, 3100, 2031, 3543, 288, 576, 362, 668, 40610, 6091, 11, 457, 586, 365, 341, 19618, 286, 669, 50670], 'temperature': 0.0, 'avg_logprob': -0.19278956241294987, 'compression_ratio': 1.5783132530120483, 'no_speech_prob': 0.5358254313468933}, {'id': 27, 'seek': 31814, 'start': 324.26, 'end': 330.41999999999996, 'text': ' computing x star y only once. Right, and that temporary value I am being able to reuse multiple', 'tokens': [50670, 15866, 2031, 3543, 288, 787, 1564, 13, 1779, 11, 293, 300, 13413, 2158, 286, 669, 885, 1075, 281, 26225, 3866, 50978], 'temperature': 0.0, 'avg_logprob': -0.19278956241294987, 'compression_ratio': 1.5783132530120483, 'no_speech_prob': 0.5358254313468933}, {'id': 28, 'seek': 31814, 'start': 330.41999999999996, 'end': 336.94, 'text': ' times. Right, so that is an interesting so that is available expressions.', 'tokens': [50978, 1413, 13, 1779, 11, 370, 300, 307, 364, 1880, 370, 300, 307, 2435, 15277, 13, 51304], 'temperature': 0.0, 'avg_logprob': -0.19278956241294987, 'compression_ratio': 1.5783132530120483, 'no_speech_prob': 0.5358254313468933}, {'id': 29, 'seek': 34814, 'start': 348.14, 'end': 371.14, 'text': ' Right, so very busy expression says what very busy expression says that if you have a computation', 'tokens': [50364, 1779, 11, 370, 588, 5856, 6114, 1619, 437, 588, 5856, 6114, 1619, 300, 498, 291, 362, 257, 24903, 51514], 'temperature': 0.0, 'avg_logprob': -0.2740172472867099, 'compression_ratio': 1.3472222222222223, 'no_speech_prob': 0.08201431483030319}, {'id': 30, 'seek': 37114, 'start': 371.14, 'end': 379.65999999999997, 'text': ' which is so it says that from this point onwards is there a computation which is happening along', 'tokens': [50364, 597, 307, 370, 309, 1619, 300, 490, 341, 935, 34230, 307, 456, 257, 24903, 597, 307, 2737, 2051, 50790], 'temperature': 0.0, 'avg_logprob': -0.13094790570147627, 'compression_ratio': 1.8896103896103895, 'no_speech_prob': 0.4777047336101532}, {'id': 31, 'seek': 37114, 'start': 379.65999999999997, 'end': 388.53999999999996, 'text': ' all paths like if there is a computation of x star y here also x star y here also x star y here also', 'tokens': [50790, 439, 14518, 411, 498, 456, 307, 257, 24903, 295, 2031, 3543, 288, 510, 611, 2031, 3543, 288, 510, 611, 2031, 3543, 288, 510, 611, 51234], 'temperature': 0.0, 'avg_logprob': -0.13094790570147627, 'compression_ratio': 1.8896103896103895, 'no_speech_prob': 0.4777047336101532}, {'id': 32, 'seek': 37114, 'start': 388.53999999999996, 'end': 395.97999999999996, 'text': ' so let us say a is equal to x star y b equals x star y c equals x star y. Then again I can do', 'tokens': [51234, 370, 718, 505, 584, 257, 307, 2681, 281, 2031, 3543, 288, 272, 6915, 2031, 3543, 288, 269, 6915, 2031, 3543, 288, 13, 1396, 797, 286, 393, 360, 51606], 'temperature': 0.0, 'avg_logprob': -0.13094790570147627, 'compression_ratio': 1.8896103896103895, 'no_speech_prob': 0.4777047336101532}, {'id': 33, 'seek': 39598, 'start': 395.98, 'end': 405.74, 'text': ' something I can hoist up this computation of x star y here I can say h equals x star y and I', 'tokens': [50364, 746, 286, 393, 1106, 468, 493, 341, 24903, 295, 2031, 3543, 288, 510, 286, 393, 584, 276, 6915, 2031, 3543, 288, 293, 286, 50852], 'temperature': 0.0, 'avg_logprob': -0.17888660430908204, 'compression_ratio': 1.6198830409356726, 'no_speech_prob': 0.4741256535053253}, {'id': 34, 'seek': 39598, 'start': 405.74, 'end': 416.46000000000004, 'text': ' can just assign them here. Yeah, so I come to those conditions so that that even holds for', 'tokens': [50852, 393, 445, 6269, 552, 510, 13, 865, 11, 370, 286, 808, 281, 729, 4487, 370, 300, 300, 754, 9190, 337, 51388], 'temperature': 0.0, 'avg_logprob': -0.17888660430908204, 'compression_ratio': 1.6198830409356726, 'no_speech_prob': 0.4741256535053253}, {'id': 35, 'seek': 39598, 'start': 416.46000000000004, 'end': 421.70000000000005, 'text': ' even the other cases I have not come to those let us just I try to understand these different', 'tokens': [51388, 754, 264, 661, 3331, 286, 362, 406, 808, 281, 729, 718, 505, 445, 286, 853, 281, 1223, 613, 819, 51650], 'temperature': 0.0, 'avg_logprob': -0.17888660430908204, 'compression_ratio': 1.6198830409356726, 'no_speech_prob': 0.4741256535053253}, {'id': 36, 'seek': 42170, 'start': 421.7, 'end': 426.02, 'text': ' cases but let us let us say you have talked about it so what is the condition under which it can', 'tokens': [50364, 3331, 457, 718, 505, 718, 505, 584, 291, 362, 2825, 466, 309, 370, 437, 307, 264, 4188, 833, 597, 309, 393, 50580], 'temperature': 0.0, 'avg_logprob': -0.20718082221778664, 'compression_ratio': 1.6022099447513811, 'no_speech_prob': 0.05764564871788025}, {'id': 37, 'seek': 42170, 'start': 426.02, 'end': 436.46, 'text': ' happen. So x and y should be unaffected along that path so I should not be I should so these', 'tokens': [50580, 1051, 13, 407, 2031, 293, 288, 820, 312, 2002, 11259, 292, 2051, 300, 3100, 370, 286, 820, 406, 312, 286, 820, 370, 613, 51102], 'temperature': 0.0, 'avg_logprob': -0.20718082221778664, 'compression_ratio': 1.6022099447513811, 'no_speech_prob': 0.05764564871788025}, {'id': 38, 'seek': 42170, 'start': 436.46, 'end': 445.65999999999997, 'text': ' are modification free. Right, so there should be no updates to x or y only if that happens only then', 'tokens': [51102, 366, 26747, 1737, 13, 1779, 11, 370, 456, 820, 312, 572, 9205, 281, 2031, 420, 288, 787, 498, 300, 2314, 787, 550, 51562], 'temperature': 0.0, 'avg_logprob': -0.20718082221778664, 'compression_ratio': 1.6022099447513811, 'no_speech_prob': 0.05764564871788025}, {'id': 39, 'seek': 44566, 'start': 445.74, 'end': 451.46000000000004, 'text': ' I can do this so that also holds for available expressions because there were expressions', 'tokens': [50368, 286, 393, 360, 341, 370, 300, 611, 9190, 337, 2435, 15277, 570, 456, 645, 15277, 50654], 'temperature': 0.0, 'avg_logprob': -0.1813256854102725, 'compression_ratio': 1.658682634730539, 'no_speech_prob': 0.27921953797340393}, {'id': 40, 'seek': 44566, 'start': 451.46000000000004, 'end': 457.22, 'text': ' computed and I can use their values only if again along the path from where I define this', 'tokens': [50654, 40610, 293, 286, 393, 764, 641, 4190, 787, 498, 797, 2051, 264, 3100, 490, 689, 286, 6964, 341, 50942], 'temperature': 0.0, 'avg_logprob': -0.1813256854102725, 'compression_ratio': 1.658682634730539, 'no_speech_prob': 0.27921953797340393}, {'id': 41, 'seek': 44566, 'start': 457.22, 'end': 467.06, 'text': ' a star b to the place where I used it there is no and reaching definition says that if you have a', 'tokens': [50942, 257, 3543, 272, 281, 264, 1081, 689, 286, 1143, 309, 456, 307, 572, 293, 9906, 7123, 1619, 300, 498, 291, 362, 257, 51434], 'temperature': 0.0, 'avg_logprob': -0.1813256854102725, 'compression_ratio': 1.658682634730539, 'no_speech_prob': 0.27921953797340393}, {'id': 42, 'seek': 46706, 'start': 467.06, 'end': 481.38, 'text': ' variable h equals a star b in that case I say like if you have a use later right say x equals h plus', 'tokens': [50364, 7006, 276, 6915, 257, 3543, 272, 294, 300, 1389, 286, 584, 411, 498, 291, 362, 257, 764, 1780, 558, 584, 2031, 6915, 276, 1804, 51080], 'temperature': 0.0, 'avg_logprob': -0.18821626469708871, 'compression_ratio': 1.812121212121212, 'no_speech_prob': 0.21135446429252625}, {'id': 43, 'seek': 46706, 'start': 481.38, 'end': 491.98, 'text': " 1 right so then I would say that the definition so essentially whenever you have so what is let's", 'tokens': [51080, 502, 558, 370, 550, 286, 576, 584, 300, 264, 7123, 370, 4476, 5699, 291, 362, 370, 437, 307, 718, 311, 51610], 'temperature': 0.0, 'avg_logprob': -0.18821626469708871, 'compression_ratio': 1.812121212121212, 'no_speech_prob': 0.21135446429252625}, {'id': 44, 'seek': 46706, 'start': 491.98, 'end': 497.02, 'text': ' me just define what is the definition our definition is basically an assignment so whenever you have', 'tokens': [51610, 385, 445, 6964, 437, 307, 264, 7123, 527, 7123, 307, 1936, 364, 15187, 370, 5699, 291, 362, 51862], 'temperature': 0.0, 'avg_logprob': -0.18821626469708871, 'compression_ratio': 1.812121212121212, 'no_speech_prob': 0.21135446429252625}, {'id': 45, 'seek': 49702, 'start': 497.02, 'end': 512.5799999999999, 'text': ' x equals b plus c this is a definition of a and these are uses of b and c right so this statement', 'tokens': [50364, 2031, 6915, 272, 1804, 269, 341, 307, 257, 7123, 295, 257, 293, 613, 366, 4960, 295, 272, 293, 269, 558, 370, 341, 5629, 51142], 'temperature': 0.0, 'avg_logprob': -0.14242161644829643, 'compression_ratio': 1.7168141592920354, 'no_speech_prob': 0.0014074300415813923}, {'id': 46, 'seek': 49702, 'start': 512.5799999999999, 'end': 521.22, 'text': ' uses b and c so these are the b the values of b and c are being used and the value of a is being', 'tokens': [51142, 4960, 272, 293, 269, 370, 613, 366, 264, 272, 264, 4190, 295, 272, 293, 269, 366, 885, 1143, 293, 264, 2158, 295, 257, 307, 885, 51574], 'temperature': 0.0, 'avg_logprob': -0.14242161644829643, 'compression_ratio': 1.7168141592920354, 'no_speech_prob': 0.0014074300415813923}, {'id': 47, 'seek': 52122, 'start': 521.22, 'end': 529.9, 'text': ' defined so something is being put into that box right so definition and use so if it is possible', 'tokens': [50364, 7642, 370, 746, 307, 885, 829, 666, 300, 2424, 558, 370, 7123, 293, 764, 370, 498, 309, 307, 1944, 50798], 'temperature': 0.0, 'avg_logprob': -0.15113839236172763, 'compression_ratio': 1.7439024390243902, 'no_speech_prob': 0.28609275817871094}, {'id': 48, 'seek': 52122, 'start': 529.9, 'end': 541.1, 'text': ' through any program path a definition is being so our definition can reach a certain use then I', 'tokens': [50798, 807, 604, 1461, 3100, 257, 7123, 307, 885, 370, 527, 7123, 393, 2524, 257, 1629, 764, 550, 286, 51358], 'temperature': 0.0, 'avg_logprob': -0.15113839236172763, 'compression_ratio': 1.7439024390243902, 'no_speech_prob': 0.28609275817871094}, {'id': 49, 'seek': 52122, 'start': 541.1, 'end': 547.38, 'text': " would say that it is a it's a reaching definition problem right so again when can it not be a", 'tokens': [51358, 576, 584, 300, 309, 307, 257, 309, 311, 257, 9906, 7123, 1154, 558, 370, 797, 562, 393, 309, 406, 312, 257, 51672], 'temperature': 0.0, 'avg_logprob': -0.15113839236172763, 'compression_ratio': 1.7439024390243902, 'no_speech_prob': 0.28609275817871094}, {'id': 50, 'seek': 54738, 'start': 547.38, 'end': 556.3, 'text': ' reaching definition thing like for instance I can have so h equals 2 for instance right there', 'tokens': [50364, 9906, 7123, 551, 411, 337, 5197, 286, 393, 362, 370, 276, 6915, 568, 337, 5197, 558, 456, 50810], 'temperature': 0.0, 'avg_logprob': -0.1710879537794325, 'compression_ratio': 1.8343949044585988, 'no_speech_prob': 0.1674017608165741}, {'id': 51, 'seek': 54738, 'start': 556.3, 'end': 564.3, 'text': ' is a update to x right then so this is a definition point of definition this is d1 this is d t right', 'tokens': [50810, 307, 257, 5623, 281, 2031, 558, 550, 370, 341, 307, 257, 7123, 935, 295, 7123, 341, 307, 274, 16, 341, 307, 274, 256, 558, 51210], 'temperature': 0.0, 'avg_logprob': -0.1710879537794325, 'compression_ratio': 1.8343949044585988, 'no_speech_prob': 0.1674017608165741}, {'id': 52, 'seek': 54738, 'start': 564.3, 'end': 574.62, 'text': " so the definition d1 reaches the statement s1 earlier when d2 wasn't there but once I have d2", 'tokens': [51210, 370, 264, 7123, 274, 16, 14235, 264, 5629, 262, 16, 3071, 562, 274, 17, 2067, 380, 456, 457, 1564, 286, 362, 274, 17, 51726], 'temperature': 0.0, 'avg_logprob': -0.1710879537794325, 'compression_ratio': 1.8343949044585988, 'no_speech_prob': 0.1674017608165741}, {'id': 53, 'seek': 57462, 'start': 574.86, 'end': 582.86, 'text': ' then the definition does not reach this location so that the reaching definition problem is that', 'tokens': [50376, 550, 264, 7123, 775, 406, 2524, 341, 4914, 370, 300, 264, 9906, 7123, 1154, 307, 300, 50776], 'temperature': 0.0, 'avg_logprob': -0.17347413301467896, 'compression_ratio': 1.9893048128342246, 'no_speech_prob': 0.02363898791372776}, {'id': 54, 'seek': 57462, 'start': 582.86, 'end': 590.22, 'text': ' for every in the program or every state program we would like to figure out which are the', 'tokens': [50776, 337, 633, 294, 264, 1461, 420, 633, 1785, 1461, 321, 576, 411, 281, 2573, 484, 597, 366, 264, 51144], 'temperature': 0.0, 'avg_logprob': -0.17347413301467896, 'compression_ratio': 1.9893048128342246, 'no_speech_prob': 0.02363898791372776}, {'id': 55, 'seek': 57462, 'start': 590.22, 'end': 596.18, 'text': ' definitions which are reaching but location now there may or may not be a use of that location', 'tokens': [51144, 21988, 597, 366, 9906, 457, 4914, 586, 456, 815, 420, 815, 406, 312, 257, 764, 295, 300, 4914, 51442], 'temperature': 0.0, 'avg_logprob': -0.17347413301467896, 'compression_ratio': 1.9893048128342246, 'no_speech_prob': 0.02363898791372776}, {'id': 56, 'seek': 57462, 'start': 596.18, 'end': 601.1, 'text': " that's a different matter but I would like to compute it for every program point the other", 'tokens': [51442, 300, 311, 257, 819, 1871, 457, 286, 576, 411, 281, 14722, 309, 337, 633, 1461, 935, 264, 661, 51688], 'temperature': 0.0, 'avg_logprob': -0.17347413301467896, 'compression_ratio': 1.9893048128342246, 'no_speech_prob': 0.02363898791372776}, {'id': 57, 'seek': 60110, 'start': 601.1, 'end': 605.26, 'text': " important definition is I'll keep on using it again is this notion of a program point", 'tokens': [50364, 1021, 7123, 307, 286, 603, 1066, 322, 1228, 309, 797, 307, 341, 10710, 295, 257, 1461, 935, 50572], 'temperature': 0.0, 'avg_logprob': -0.12629144088081692, 'compression_ratio': 1.7037037037037037, 'no_speech_prob': 0.050193849951028824}, {'id': 58, 'seek': 60110, 'start': 605.26, 'end': 621.94, 'text': " so so you have let's say a statement s1 and you have a statement s2 so a program point is a", 'tokens': [50572, 370, 370, 291, 362, 718, 311, 584, 257, 5629, 262, 16, 293, 291, 362, 257, 5629, 262, 17, 370, 257, 1461, 935, 307, 257, 51406], 'temperature': 0.0, 'avg_logprob': -0.12629144088081692, 'compression_ratio': 1.7037037037037037, 'no_speech_prob': 0.050193849951028824}, {'id': 59, 'seek': 60110, 'start': 621.94, 'end': 627.34, 'text': ' stable location where I can ask the state of the program so the next question is what is the state', 'tokens': [51406, 8351, 4914, 689, 286, 393, 1029, 264, 1785, 295, 264, 1461, 370, 264, 958, 1168, 307, 437, 307, 264, 1785, 51676], 'temperature': 0.0, 'avg_logprob': -0.12629144088081692, 'compression_ratio': 1.7037037037037037, 'no_speech_prob': 0.050193849951028824}, {'id': 60, 'seek': 62734, 'start': 627.34, 'end': 640.94, 'text': ' of a program so the state of a program is basically thus the vector containing values', 'tokens': [50364, 295, 257, 1461, 370, 264, 1785, 295, 257, 1461, 307, 1936, 8807, 264, 8062, 19273, 4190, 51044], 'temperature': 0.0, 'avg_logprob': -0.1938425064086914, 'compression_ratio': 1.2142857142857142, 'no_speech_prob': 0.10919055342674255}, {'id': 61, 'seek': 64094, 'start': 640.94, 'end': 657.2600000000001, 'text': ' of each variable right so if you have three program variables a b and c so at every program', 'tokens': [50364, 295, 1184, 7006, 558, 370, 498, 291, 362, 1045, 1461, 9102, 257, 272, 293, 269, 370, 412, 633, 1461, 51180], 'temperature': 0.0, 'avg_logprob': -0.10068214640897863, 'compression_ratio': 1.8791946308724832, 'no_speech_prob': 0.0382286012172699}, {'id': 62, 'seek': 64094, 'start': 657.2600000000001, 'end': 661.2600000000001, 'text': ' point I would would like to figure out what is the value of a what is the value of b what is the', 'tokens': [51180, 935, 286, 576, 576, 411, 281, 2573, 484, 437, 307, 264, 2158, 295, 257, 437, 307, 264, 2158, 295, 272, 437, 307, 264, 51380], 'temperature': 0.0, 'avg_logprob': -0.10068214640897863, 'compression_ratio': 1.8791946308724832, 'no_speech_prob': 0.0382286012172699}, {'id': 63, 'seek': 64094, 'start': 661.2600000000001, 'end': 668.5, 'text': ' value of c so that constitutes one state of a program and a program points are points where', 'tokens': [51380, 2158, 295, 269, 370, 300, 44204, 472, 1785, 295, 257, 1461, 293, 257, 1461, 2793, 366, 2793, 689, 51742], 'temperature': 0.0, 'avg_logprob': -0.10068214640897863, 'compression_ratio': 1.8791946308724832, 'no_speech_prob': 0.0382286012172699}, {'id': 64, 'seek': 66850, 'start': 668.5, 'end': 674.1, 'text': ' I may be interested in querying the state of a program so this for two statements s1 and s2', 'tokens': [50364, 286, 815, 312, 3102, 294, 7083, 1840, 264, 1785, 295, 257, 1461, 370, 341, 337, 732, 12363, 262, 16, 293, 262, 17, 50644], 'temperature': 0.0, 'avg_logprob': -0.14648813950388054, 'compression_ratio': 2.0482456140350878, 'no_speech_prob': 0.3055247366428375}, {'id': 65, 'seek': 66850, 'start': 674.1, 'end': 680.34, 'text': " the program for valid program points is this one this one and this one so it's either before", 'tokens': [50644, 264, 1461, 337, 7363, 1461, 2793, 307, 341, 472, 341, 472, 293, 341, 472, 370, 309, 311, 2139, 949, 50956], 'temperature': 0.0, 'avg_logprob': -0.14648813950388054, 'compression_ratio': 2.0482456140350878, 'no_speech_prob': 0.3055247366428375}, {'id': 66, 'seek': 66850, 'start': 680.34, 'end': 684.98, 'text': ' the statement has got executed or after the statement has got executed right so whenever', 'tokens': [50956, 264, 5629, 575, 658, 17577, 420, 934, 264, 5629, 575, 658, 17577, 558, 370, 5699, 51188], 'temperature': 0.0, 'avg_logprob': -0.14648813950388054, 'compression_ratio': 2.0482456140350878, 'no_speech_prob': 0.3055247366428375}, {'id': 67, 'seek': 66850, 'start': 684.98, 'end': 691.14, 'text': ' I ask you say what happens at this program at at statement s2 and you said you should tell me that', 'tokens': [51188, 286, 1029, 291, 584, 437, 2314, 412, 341, 1461, 412, 412, 5629, 262, 17, 293, 291, 848, 291, 820, 980, 385, 300, 51496], 'temperature': 0.0, 'avg_logprob': -0.14648813950388054, 'compression_ratio': 2.0482456140350878, 'no_speech_prob': 0.3055247366428375}, {'id': 68, 'seek': 66850, 'start': 691.14, 'end': 695.86, 'text': " I don't know what happens at s2 I can tell you what happens before s2 or what happens after s2", 'tokens': [51496, 286, 500, 380, 458, 437, 2314, 412, 262, 17, 286, 393, 980, 291, 437, 2314, 949, 262, 17, 420, 437, 2314, 934, 262, 17, 51732], 'temperature': 0.0, 'avg_logprob': -0.14648813950388054, 'compression_ratio': 2.0482456140350878, 'no_speech_prob': 0.3055247366428375}, {'id': 69, 'seek': 69586, 'start': 695.86, 'end': 699.54, 'text': " what is the state before s2 or what is the state after s2 when the statement is running I don't", 'tokens': [50364, 437, 307, 264, 1785, 949, 262, 17, 420, 437, 307, 264, 1785, 934, 262, 17, 562, 264, 5629, 307, 2614, 286, 500, 380, 50548], 'temperature': 0.0, 'avg_logprob': -0.111781796205391, 'compression_ratio': 1.8525896414342629, 'no_speech_prob': 0.11868710070848465}, {'id': 70, 'seek': 69586, 'start': 699.54, 'end': 707.62, 'text': " know what's going on right so these are runtime behavior but your static analysis is also an", 'tokens': [50548, 458, 437, 311, 516, 322, 558, 370, 613, 366, 34474, 5223, 457, 428, 13437, 5215, 307, 611, 364, 50952], 'temperature': 0.0, 'avg_logprob': -0.111781796205391, 'compression_ratio': 1.8525896414342629, 'no_speech_prob': 0.11868710070848465}, {'id': 71, 'seek': 69586, 'start': 707.62, 'end': 712.1, 'text': ' approximation of the runtime behavior right so I will come up with instead of having concrete', 'tokens': [50952, 28023, 295, 264, 34474, 5223, 558, 370, 286, 486, 808, 493, 365, 2602, 295, 1419, 9859, 51176], 'temperature': 0.0, 'avg_logprob': -0.111781796205391, 'compression_ratio': 1.8525896414342629, 'no_speech_prob': 0.11868710070848465}, {'id': 72, 'seek': 69586, 'start': 712.1, 'end': 717.14, 'text': " states of the runtime remainder I'll have abstract states which essentially say what are the", 'tokens': [51176, 4368, 295, 264, 34474, 29837, 286, 603, 362, 12649, 4368, 597, 4476, 584, 437, 366, 264, 51428], 'temperature': 0.0, 'avg_logprob': -0.111781796205391, 'compression_ratio': 1.8525896414342629, 'no_speech_prob': 0.11868710070848465}, {'id': 73, 'seek': 69586, 'start': 717.14, 'end': 721.78, 'text': ' possibilities that can happen here like for instance I can say that I do not know what is', 'tokens': [51428, 12178, 300, 393, 1051, 510, 411, 337, 5197, 286, 393, 584, 300, 286, 360, 406, 458, 437, 307, 51660], 'temperature': 0.0, 'avg_logprob': -0.111781796205391, 'compression_ratio': 1.8525896414342629, 'no_speech_prob': 0.11868710070848465}, {'id': 74, 'seek': 72178, 'start': 721.78, 'end': 725.3, 'text': " the value of x there but I'm sure that all the values of x will be greater than zero", 'tokens': [50364, 264, 2158, 295, 2031, 456, 457, 286, 478, 988, 300, 439, 264, 4190, 295, 2031, 486, 312, 5044, 813, 4018, 50540], 'temperature': 0.0, 'avg_logprob': -0.05429693537020902, 'compression_ratio': 1.9574468085106382, 'no_speech_prob': 0.18675898015499115}, {'id': 75, 'seek': 72178, 'start': 726.02, 'end': 731.06, 'text': ' right so but that also I have to say at a program point I cannot say at instruction what happens', 'tokens': [50576, 558, 370, 457, 300, 611, 286, 362, 281, 584, 412, 257, 1461, 935, 286, 2644, 584, 412, 10951, 437, 2314, 50828], 'temperature': 0.0, 'avg_logprob': -0.05429693537020902, 'compression_ratio': 1.9574468085106382, 'no_speech_prob': 0.18675898015499115}, {'id': 76, 'seek': 72178, 'start': 731.06, 'end': 734.5, 'text': " doesn't really make sense right because things are changing I don't know how they're changing", 'tokens': [50828, 1177, 380, 534, 652, 2020, 558, 570, 721, 366, 4473, 286, 500, 380, 458, 577, 436, 434, 4473, 51000], 'temperature': 0.0, 'avg_logprob': -0.05429693537020902, 'compression_ratio': 1.9574468085106382, 'no_speech_prob': 0.18675898015499115}, {'id': 77, 'seek': 72178, 'start': 734.5, 'end': 739.86, 'text': ' what is happening so I can only tell you what thing what happens so so whenever I query a state', 'tokens': [51000, 437, 307, 2737, 370, 286, 393, 787, 980, 291, 437, 551, 437, 2314, 370, 370, 5699, 286, 14581, 257, 1785, 51268], 'temperature': 0.0, 'avg_logprob': -0.05429693537020902, 'compression_ratio': 1.9574468085106382, 'no_speech_prob': 0.18675898015499115}, {'id': 78, 'seek': 72178, 'start': 739.86, 'end': 743.62, 'text': ' I can only query a state at a program point I cannot query a state at any other location', 'tokens': [51268, 286, 393, 787, 14581, 257, 1785, 412, 257, 1461, 935, 286, 2644, 14581, 257, 1785, 412, 604, 661, 4914, 51456], 'temperature': 0.0, 'avg_logprob': -0.05429693537020902, 'compression_ratio': 1.9574468085106382, 'no_speech_prob': 0.18675898015499115}, {'id': 79, 'seek': 74362, 'start': 743.62, 'end': 754.18, 'text': ' so are we clear about the definition of these four statements of these four analysis okay so', 'tokens': [50364, 370, 366, 321, 1850, 466, 264, 7123, 295, 613, 1451, 12363, 295, 613, 1451, 5215, 1392, 370, 50892], 'temperature': 0.0, 'avg_logprob': -0.14759377713473337, 'compression_ratio': 1.7285714285714286, 'no_speech_prob': 0.037101153284311295}, {'id': 80, 'seek': 74362, 'start': 754.18, 'end': 760.9, 'text': " let's talk about reaching definitions let's say so how does reaching definition work so essentially", 'tokens': [50892, 718, 311, 751, 466, 9906, 21988, 718, 311, 584, 370, 577, 775, 9906, 7123, 589, 370, 4476, 51228], 'temperature': 0.0, 'avg_logprob': -0.14759377713473337, 'compression_ratio': 1.7285714285714286, 'no_speech_prob': 0.037101153284311295}, {'id': 81, 'seek': 74362, 'start': 764.66, 'end': 767.3, 'text': ' so now how will you so what is the computation on', 'tokens': [51416, 370, 586, 577, 486, 291, 370, 437, 307, 264, 24903, 322, 51548], 'temperature': 0.0, 'avg_logprob': -0.14759377713473337, 'compression_ratio': 1.7285714285714286, 'no_speech_prob': 0.037101153284311295}, {'id': 82, 'seek': 76730, 'start': 767.3, 'end': 772.3399999999999, 'text': ' so what is the set of values that you will try to output', 'tokens': [50364, 370, 437, 307, 264, 992, 295, 4190, 300, 291, 486, 853, 281, 5598, 50616], 'temperature': 0.0, 'avg_logprob': -0.09193491141001384, 'compression_ratio': 1.937984496124031, 'no_speech_prob': 0.002478295937180519}, {'id': 83, 'seek': 76730, 'start': 777.54, 'end': 785.4599999999999, 'text': ' so so you would like to figure out that at any program point you would like to figure out the', 'tokens': [50876, 370, 370, 291, 576, 411, 281, 2573, 484, 300, 412, 604, 1461, 935, 291, 576, 411, 281, 2573, 484, 264, 51272], 'temperature': 0.0, 'avg_logprob': -0.09193491141001384, 'compression_ratio': 1.937984496124031, 'no_speech_prob': 0.002478295937180519}, {'id': 84, 'seek': 76730, 'start': 785.4599999999999, 'end': 793.6999999999999, 'text': " set of all reaching definitions so it's a set of all reaching definitions right so so the universal", 'tokens': [51272, 992, 295, 439, 9906, 21988, 370, 309, 311, 257, 992, 295, 439, 9906, 21988, 558, 370, 370, 264, 11455, 51684], 'temperature': 0.0, 'avg_logprob': -0.09193491141001384, 'compression_ratio': 1.937984496124031, 'no_speech_prob': 0.002478295937180519}, {'id': 85, 'seek': 79370, 'start': 793.7, 'end': 799.94, 'text': ' set from which you will be able to extract this particular set at a given point is the set of all', 'tokens': [50364, 992, 490, 597, 291, 486, 312, 1075, 281, 8947, 341, 1729, 992, 412, 257, 2212, 935, 307, 264, 992, 295, 439, 50676], 'temperature': 0.0, 'avg_logprob': -0.025685744626181468, 'compression_ratio': 2.1215469613259668, 'no_speech_prob': 0.003022663062438369}, {'id': 86, 'seek': 79370, 'start': 799.94, 'end': 804.9000000000001, 'text': ' definitions so what are the set of all definitions in the program how will you get the set of all', 'tokens': [50676, 21988, 370, 437, 366, 264, 992, 295, 439, 21988, 294, 264, 1461, 577, 486, 291, 483, 264, 992, 295, 439, 50924], 'temperature': 0.0, 'avg_logprob': -0.025685744626181468, 'compression_ratio': 2.1215469613259668, 'no_speech_prob': 0.003022663062438369}, {'id': 87, 'seek': 79370, 'start': 804.9000000000001, 'end': 811.7, 'text': ' definitions in the program all the assignment statements right so all the statements which', 'tokens': [50924, 21988, 294, 264, 1461, 439, 264, 15187, 12363, 558, 370, 439, 264, 12363, 597, 51264], 'temperature': 0.0, 'avg_logprob': -0.025685744626181468, 'compression_ratio': 2.1215469613259668, 'no_speech_prob': 0.003022663062438369}, {'id': 88, 'seek': 79370, 'start': 811.7, 'end': 817.7, 'text': ' assigned to a given variable is a definition what if there are two statements a equal to one here', 'tokens': [51264, 13279, 281, 257, 2212, 7006, 307, 257, 7123, 437, 498, 456, 366, 732, 12363, 257, 2681, 281, 472, 510, 51564], 'temperature': 0.0, 'avg_logprob': -0.025685744626181468, 'compression_ratio': 2.1215469613259668, 'no_speech_prob': 0.003022663062438369}, {'id': 89, 'seek': 81770, 'start': 817.7, 'end': 823.62, 'text': ' in one basic block and a equals one here in another basic block are they the same definition', 'tokens': [50364, 294, 472, 3875, 3461, 293, 257, 6915, 472, 510, 294, 1071, 3875, 3461, 366, 436, 264, 912, 7123, 50660], 'temperature': 0.0, 'avg_logprob': -0.10650089310436714, 'compression_ratio': 1.8415841584158417, 'no_speech_prob': 0.0009452873491682112}, {'id': 90, 'seek': 81770, 'start': 823.62, 'end': 832.74, 'text': ' or are they different definitions now we are only doing intra-procedural analysis so they are', 'tokens': [50660, 420, 366, 436, 819, 21988, 586, 321, 366, 787, 884, 43358, 12, 4318, 1232, 1807, 5215, 370, 436, 366, 51116], 'temperature': 0.0, 'avg_logprob': -0.10650089310436714, 'compression_ratio': 1.8415841584158417, 'no_speech_prob': 0.0009452873491682112}, {'id': 91, 'seek': 81770, 'start': 832.74, 'end': 839.86, 'text': ' everything is in the same function no they constitute different definitions so they will', 'tokens': [51116, 1203, 307, 294, 264, 912, 2445, 572, 436, 41658, 819, 21988, 370, 436, 486, 51472], 'temperature': 0.0, 'avg_logprob': -0.10650089310436714, 'compression_ratio': 1.8415841584158417, 'no_speech_prob': 0.0009452873491682112}, {'id': 92, 'seek': 81770, 'start': 839.86, 'end': 845.1400000000001, 'text': ' be d1 and d2 two separate definitions and it may happen that one of them reaches a program point', 'tokens': [51472, 312, 274, 16, 293, 274, 17, 732, 4994, 21988, 293, 309, 815, 1051, 300, 472, 295, 552, 14235, 257, 1461, 935, 51736], 'temperature': 0.0, 'avg_logprob': -0.10650089310436714, 'compression_ratio': 1.8415841584158417, 'no_speech_prob': 0.0009452873491682112}, {'id': 93, 'seek': 84514, 'start': 845.14, 'end': 850.18, 'text': ' but the other one does not reach this program point right for instance i may have like a equals', 'tokens': [50364, 457, 264, 661, 472, 775, 406, 2524, 341, 1461, 935, 558, 337, 5197, 741, 815, 362, 411, 257, 6915, 50616], 'temperature': 0.0, 'avg_logprob': -0.07304980357487996, 'compression_ratio': 1.6946902654867257, 'no_speech_prob': 0.0017789662815630436}, {'id': 94, 'seek': 84514, 'start': 850.18, 'end': 861.6999999999999, 'text': " three here or maybe this does not even reach this location right so so my so net let's try to", 'tokens': [50616, 1045, 510, 420, 1310, 341, 775, 406, 754, 2524, 341, 4914, 558, 370, 370, 452, 370, 2533, 718, 311, 853, 281, 51192], 'temperature': 0.0, 'avg_logprob': -0.07304980357487996, 'compression_ratio': 1.6946902654867257, 'no_speech_prob': 0.0017789662815630436}, {'id': 95, 'seek': 84514, 'start': 861.6999999999999, 'end': 866.98, 'text': ' formalize this whole business so i have my universal set here is the set of all definitions in the', 'tokens': [51192, 9860, 1125, 341, 1379, 1606, 370, 741, 362, 452, 11455, 992, 510, 307, 264, 992, 295, 439, 21988, 294, 264, 51456], 'temperature': 0.0, 'avg_logprob': -0.07304980357487996, 'compression_ratio': 1.6946902654867257, 'no_speech_prob': 0.0017789662815630436}, {'id': 96, 'seek': 84514, 'start': 866.98, 'end': 874.9, 'text': ' program and i can arbitrarily give names to each definition so d1 d2 d3 d4 d5 i just these are', 'tokens': [51456, 1461, 293, 741, 393, 19071, 3289, 976, 5288, 281, 1184, 7123, 370, 274, 16, 274, 17, 274, 18, 274, 19, 274, 20, 741, 445, 613, 366, 51852], 'temperature': 0.0, 'avg_logprob': -0.07304980357487996, 'compression_ratio': 1.6946902654867257, 'no_speech_prob': 0.0017789662815630436}, {'id': 97, 'seek': 87490, 'start': 874.9, 'end': 879.9399999999999, 'text': ' static names i just look at an instruction and say give it a name which is what what definition', 'tokens': [50364, 13437, 5288, 741, 445, 574, 412, 364, 10951, 293, 584, 976, 309, 257, 1315, 597, 307, 437, 437, 7123, 50616], 'temperature': 0.0, 'avg_logprob': -0.08759987032091296, 'compression_ratio': 1.911764705882353, 'no_speech_prob': 0.00011306295346003026}, {'id': 98, 'seek': 87490, 'start': 883.38, 'end': 888.5799999999999, 'text': ' now can you give me some idea of how can i compute this switching definition', 'tokens': [50788, 586, 393, 291, 976, 385, 512, 1558, 295, 577, 393, 741, 14722, 341, 16493, 7123, 51048], 'temperature': 0.0, 'avg_logprob': -0.08759987032091296, 'compression_ratio': 1.911764705882353, 'no_speech_prob': 0.00011306295346003026}, {'id': 99, 'seek': 87490, 'start': 889.38, 'end': 896.02, 'text': ' how can you compute this set so when can a definition stop reaching anything else', 'tokens': [51088, 577, 393, 291, 14722, 341, 992, 370, 562, 393, 257, 7123, 1590, 9906, 1340, 1646, 51420], 'temperature': 0.0, 'avg_logprob': -0.08759987032091296, 'compression_ratio': 1.911764705882353, 'no_speech_prob': 0.00011306295346003026}, {'id': 100, 'seek': 87490, 'start': 898.1, 'end': 901.6999999999999, 'text': ' when it is reassigned the same variable is reassigned then it may stop', 'tokens': [51524, 562, 309, 307, 19486, 16690, 264, 912, 7006, 307, 19486, 16690, 550, 309, 815, 1590, 51704], 'temperature': 0.0, 'avg_logprob': -0.08759987032091296, 'compression_ratio': 1.911764705882353, 'no_speech_prob': 0.00011306295346003026}, {'id': 101, 'seek': 90170, 'start': 902.0200000000001, 'end': 909.0600000000001, 'text': ' uh like propagating further the other way it may not reach a given location is if there is no', 'tokens': [50380, 2232, 411, 12425, 990, 3052, 264, 661, 636, 309, 815, 406, 2524, 257, 2212, 4914, 307, 498, 456, 307, 572, 50732], 'temperature': 0.0, 'avg_logprob': -0.07459555539217862, 'compression_ratio': 1.9830508474576272, 'no_speech_prob': 0.002063367748633027}, {'id': 102, 'seek': 90170, 'start': 909.0600000000001, 'end': 913.1400000000001, 'text': ' path that goes from this particular program point to the other program point so there is no way for', 'tokens': [50732, 3100, 300, 1709, 490, 341, 1729, 1461, 935, 281, 264, 661, 1461, 935, 370, 456, 307, 572, 636, 337, 50936], 'temperature': 0.0, 'avg_logprob': -0.07459555539217862, 'compression_ratio': 1.9830508474576272, 'no_speech_prob': 0.002063367748633027}, {'id': 103, 'seek': 90170, 'start': 913.1400000000001, 'end': 919.62, 'text': " this guy to go there right so there are two ways like it's like a car which is moving so the car", 'tokens': [50936, 341, 2146, 281, 352, 456, 558, 370, 456, 366, 732, 2098, 411, 309, 311, 411, 257, 1032, 597, 307, 2684, 370, 264, 1032, 51260], 'temperature': 0.0, 'avg_logprob': -0.07459555539217862, 'compression_ratio': 1.9830508474576272, 'no_speech_prob': 0.002063367748633027}, {'id': 104, 'seek': 90170, 'start': 919.62, 'end': 924.74, 'text': ' may not be able to reach another location either if there does not exist a road which can take it', 'tokens': [51260, 815, 406, 312, 1075, 281, 2524, 1071, 4914, 2139, 498, 456, 775, 406, 2514, 257, 3060, 597, 393, 747, 309, 51516], 'temperature': 0.0, 'avg_logprob': -0.07459555539217862, 'compression_ratio': 1.9830508474576272, 'no_speech_prob': 0.002063367748633027}, {'id': 105, 'seek': 90170, 'start': 924.74, 'end': 929.86, 'text': ' there or somebody stops it in between right so it says no you cannot go further', 'tokens': [51516, 456, 420, 2618, 10094, 309, 294, 1296, 558, 370, 309, 1619, 572, 291, 2644, 352, 3052, 51772], 'temperature': 0.0, 'avg_logprob': -0.07459555539217862, 'compression_ratio': 1.9830508474576272, 'no_speech_prob': 0.002063367748633027}, {'id': 106, 'seek': 92986, 'start': 930.26, 'end': 935.54, 'text': ' okay so now can you think of a way of computing this set of reaching definitions', 'tokens': [50384, 1392, 370, 586, 393, 291, 519, 295, 257, 636, 295, 15866, 341, 992, 295, 9906, 21988, 50648], 'temperature': 0.0, 'avg_logprob': -0.15445619279688055, 'compression_ratio': 1.9898477157360406, 'no_speech_prob': 0.00031971358112059534}, {'id': 107, 'seek': 92986, 'start': 939.46, 'end': 944.1, 'text': ' and maybe you can use the idea of how did we compute the dominators can you extend that idea', 'tokens': [50844, 293, 1310, 291, 393, 764, 264, 1558, 295, 577, 630, 321, 14722, 264, 8859, 3391, 393, 291, 10101, 300, 1558, 51076], 'temperature': 0.0, 'avg_logprob': -0.15445619279688055, 'compression_ratio': 1.9898477157360406, 'no_speech_prob': 0.00031971358112059534}, {'id': 108, 'seek': 92986, 'start': 944.1, 'end': 949.0600000000001, 'text': " somewhat and think of a way of doing it in a slightly more efficient manner so so let's", 'tokens': [51076, 8344, 293, 519, 295, 257, 636, 295, 884, 309, 294, 257, 4748, 544, 7148, 9060, 370, 370, 718, 311, 51324], 'temperature': 0.0, 'avg_logprob': -0.15445619279688055, 'compression_ratio': 1.9898477157360406, 'no_speech_prob': 0.00031971358112059534}, {'id': 109, 'seek': 92986, 'start': 949.0600000000001, 'end': 953.38, 'text': " let's think about the inefficient algorithm first what is the inefficient or the bad algorithm to", 'tokens': [51324, 718, 311, 519, 466, 264, 43495, 9284, 700, 437, 307, 264, 43495, 420, 264, 1578, 9284, 281, 51540], 'temperature': 0.0, 'avg_logprob': -0.15445619279688055, 'compression_ratio': 1.9898477157360406, 'no_speech_prob': 0.00031971358112059534}, {'id': 110, 'seek': 92986, 'start': 953.38, 'end': 954.5, 'text': ' compute the reaching definitions', 'tokens': [51540, 14722, 264, 9906, 21988, 51596], 'temperature': 0.0, 'avg_logprob': -0.15445619279688055, 'compression_ratio': 1.9898477157360406, 'no_speech_prob': 0.00031971358112059534}, {'id': 111, 'seek': 95450, 'start': 954.5, 'end': 965.78, 'text': ' okay collect all um all definitions of the same variable like look for each variable', 'tokens': [50364, 1392, 2500, 439, 1105, 439, 21988, 295, 264, 912, 7006, 411, 574, 337, 1184, 7006, 50928], 'temperature': 0.0, 'avg_logprob': -0.11193935798876213, 'compression_ratio': 1.6545454545454545, 'no_speech_prob': 0.003220720449462533}, {'id': 112, 'seek': 95450, 'start': 965.78, 'end': 969.78, 'text': " i can figure out that if it goes to a certain set or not so let's say i give you a program", 'tokens': [50928, 741, 393, 2573, 484, 300, 498, 309, 1709, 281, 257, 1629, 992, 420, 406, 370, 718, 311, 584, 741, 976, 291, 257, 1461, 51128], 'temperature': 0.0, 'avg_logprob': -0.11193935798876213, 'compression_ratio': 1.6545454545454545, 'no_speech_prob': 0.003220720449462533}, {'id': 113, 'seek': 95450, 'start': 969.78, 'end': 974.18, 'text': ' point and i want to figure out what is the set of reaching definitions here how will you how will', 'tokens': [51128, 935, 293, 741, 528, 281, 2573, 484, 437, 307, 264, 992, 295, 9906, 21988, 510, 577, 486, 291, 577, 486, 51348], 'temperature': 0.0, 'avg_logprob': -0.11193935798876213, 'compression_ratio': 1.6545454545454545, 'no_speech_prob': 0.003220720449462533}, {'id': 114, 'seek': 97418, 'start': 974.18, 'end': 982.66, 'text': ' you compute it traverse it okay um then', 'tokens': [50364, 291, 14722, 309, 45674, 309, 1392, 1105, 550, 50788], 'temperature': 0.0, 'avg_logprob': -0.17372511327266693, 'compression_ratio': 1.3333333333333333, 'no_speech_prob': 0.07573317736387253}, {'id': 115, 'seek': 97418, 'start': 994.42, 'end': 999.62, 'text': ' right so essentially one way to do that is like is basically collect all the program paths which', 'tokens': [51376, 558, 370, 4476, 472, 636, 281, 360, 300, 307, 411, 307, 1936, 2500, 439, 264, 1461, 14518, 597, 51636], 'temperature': 0.0, 'avg_logprob': -0.17372511327266693, 'compression_ratio': 1.3333333333333333, 'no_speech_prob': 0.07573317736387253}, {'id': 116, 'seek': 99962, 'start': 999.62, 'end': 1009.14, 'text': ' are reaching from the entry point to this particular location right and for every path', 'tokens': [50364, 366, 9906, 490, 264, 8729, 935, 281, 341, 1729, 4914, 558, 293, 337, 633, 3100, 50840], 'temperature': 0.0, 'avg_logprob': -0.08092449888398376, 'compression_ratio': 1.8860103626943006, 'no_speech_prob': 0.0024168516974896193}, {'id': 117, 'seek': 99962, 'start': 1009.14, 'end': 1013.78, 'text': ' figure out which is the set of definitions reaching here and take a union over all of them', 'tokens': [50840, 2573, 484, 597, 307, 264, 992, 295, 21988, 9906, 510, 293, 747, 257, 11671, 670, 439, 295, 552, 51072], 'temperature': 0.0, 'avg_logprob': -0.08092449888398376, 'compression_ratio': 1.8860103626943006, 'no_speech_prob': 0.0024168516974896193}, {'id': 118, 'seek': 99962, 'start': 1015.86, 'end': 1021.86, 'text': ' this solution is referred to as the mop solution or the meet of all paths solution so meet means', 'tokens': [51176, 341, 3827, 307, 10839, 281, 382, 264, 48106, 3827, 420, 264, 1677, 295, 439, 14518, 3827, 370, 1677, 1355, 51476], 'temperature': 0.0, 'avg_logprob': -0.08092449888398376, 'compression_ratio': 1.8860103626943006, 'no_speech_prob': 0.0024168516974896193}, {'id': 119, 'seek': 99962, 'start': 1021.86, 'end': 1027.3, 'text': ' you are just taking a union over the solutions over different paths how do you figure out', 'tokens': [51476, 291, 366, 445, 1940, 257, 11671, 670, 264, 6547, 670, 819, 14518, 577, 360, 291, 2573, 484, 51748], 'temperature': 0.0, 'avg_logprob': -0.08092449888398376, 'compression_ratio': 1.8860103626943006, 'no_speech_prob': 0.0024168516974896193}, {'id': 120, 'seek': 102730, 'start': 1027.94, 'end': 1034.58, 'text': " the solution over one path so now let's simplify our case now we take a union over all paths so", 'tokens': [50396, 264, 3827, 670, 472, 3100, 370, 586, 718, 311, 20460, 527, 1389, 586, 321, 747, 257, 11671, 670, 439, 14518, 370, 50728], 'temperature': 0.0, 'avg_logprob': -0.1159820189842811, 'compression_ratio': 1.7112299465240641, 'no_speech_prob': 0.002469079103320837}, {'id': 121, 'seek': 102730, 'start': 1034.58, 'end': 1039.86, 'text': " now let's try to figure out how would you compute reaching definitions on a single path on one given", 'tokens': [50728, 586, 718, 311, 853, 281, 2573, 484, 577, 576, 291, 14722, 9906, 21988, 322, 257, 2167, 3100, 322, 472, 2212, 50992], 'temperature': 0.0, 'avg_logprob': -0.1159820189842811, 'compression_ratio': 1.7112299465240641, 'no_speech_prob': 0.002469079103320837}, {'id': 122, 'seek': 102730, 'start': 1039.86, 'end': 1043.78, 'text': ' path so how can you do that', 'tokens': [50992, 3100, 370, 577, 393, 291, 360, 300, 51188], 'temperature': 0.0, 'avg_logprob': -0.1159820189842811, 'compression_ratio': 1.7112299465240641, 'no_speech_prob': 0.002469079103320837}, {'id': 123, 'seek': 102730, 'start': 1050.98, 'end': 1056.98, 'text': " so let's current means the beginning entry point no the are you saying one from where it is now", 'tokens': [51548, 370, 718, 311, 2190, 1355, 264, 2863, 8729, 935, 572, 264, 366, 291, 1566, 472, 490, 689, 309, 307, 586, 51848], 'temperature': 0.0, 'avg_logprob': -0.1159820189842811, 'compression_ratio': 1.7112299465240641, 'no_speech_prob': 0.002469079103320837}, {'id': 124, 'seek': 105698, 'start': 1056.98, 'end': 1061.46, 'text': ' down here okay okay okay then', 'tokens': [50364, 760, 510, 1392, 1392, 1392, 550, 50588], 'temperature': 0.0, 'avg_logprob': -0.4331647872924805, 'compression_ratio': 1.0740740740740742, 'no_speech_prob': 0.10532280802726746}, {'id': 125, 'seek': 106146, 'start': 1061.46, 'end': 1085.14, 'text': ' Mason', 'tokens': [50364, 25730, 51548], 'temperature': 1.0, 'avg_logprob': -3.5509674072265627, 'compression_ratio': 0.38461538461538464, 'no_speech_prob': 0.047734711319208145}, {'id': 126, 'seek': 108514, 'start': 1085.14, 'end': 1092.14, 'text': ' any any other ideas? Why not go forward?', 'tokens': [50364, 604, 604, 661, 3487, 30, 1545, 406, 352, 2128, 30, 50714], 'temperature': 0.0, 'avg_logprob': -0.40054501985248764, 'compression_ratio': 1.233009708737864, 'no_speech_prob': 0.0664837509393692}, {'id': 127, 'seek': 108514, 'start': 1092.14, 'end': 1107.0, 'text': ' Right, but you will be able to get the whole set in one shot. Right, so you start with', 'tokens': [50714, 1779, 11, 457, 291, 486, 312, 1075, 281, 483, 264, 1379, 992, 294, 472, 3347, 13, 1779, 11, 370, 291, 722, 365, 51457], 'temperature': 0.0, 'avg_logprob': -0.40054501985248764, 'compression_ratio': 1.233009708737864, 'no_speech_prob': 0.0664837509393692}, {'id': 128, 'seek': 110700, 'start': 1107.0, 'end': 1114.36, 'text': ' saying the set being empty, whenever I get a new, if I get a statement of the form A', 'tokens': [50364, 1566, 264, 992, 885, 6707, 11, 5699, 286, 483, 257, 777, 11, 498, 286, 483, 257, 5629, 295, 264, 1254, 316, 50732], 'temperature': 0.0, 'avg_logprob': -0.25179740360804964, 'compression_ratio': 1.5030674846625767, 'no_speech_prob': 0.16653479635715485}, {'id': 129, 'seek': 110700, 'start': 1114.36, 'end': 1123.2, 'text': ' equals B plus C, what would you do? How will you update this state? I will put A in this', 'tokens': [50732, 6915, 363, 1804, 383, 11, 437, 576, 291, 360, 30, 1012, 486, 291, 5623, 341, 1785, 30, 286, 486, 829, 316, 294, 341, 51174], 'temperature': 0.0, 'avg_logprob': -0.25179740360804964, 'compression_ratio': 1.5030674846625767, 'no_speech_prob': 0.16653479635715485}, {'id': 130, 'seek': 110700, 'start': 1123.2, 'end': 1132.76, 'text': ' set and keep on going down. Right, so if you get another A equals, like', 'tokens': [51174, 992, 293, 1066, 322, 516, 760, 13, 1779, 11, 370, 498, 291, 483, 1071, 316, 6915, 11, 411, 51652], 'temperature': 0.0, 'avg_logprob': -0.25179740360804964, 'compression_ratio': 1.5030674846625767, 'no_speech_prob': 0.16653479635715485}, {'id': 131, 'seek': 113276, 'start': 1133.64, 'end': 1138.48, 'text': ' sorry I have to say that A was there and I will have to remember that it is like because', 'tokens': [50408, 2597, 286, 362, 281, 584, 300, 316, 390, 456, 293, 286, 486, 362, 281, 1604, 300, 309, 307, 411, 570, 50650], 'temperature': 0.0, 'avg_logprob': -0.15223585642301118, 'compression_ratio': 1.7303921568627452, 'no_speech_prob': 0.08991233259439468}, {'id': 132, 'seek': 113276, 'start': 1138.48, 'end': 1142.8799999999999, 'text': ' it is definition D1, that is what I need to output, not A. Right, so if I get a definition', 'tokens': [50650, 309, 307, 7123, 413, 16, 11, 300, 307, 437, 286, 643, 281, 5598, 11, 406, 316, 13, 1779, 11, 370, 498, 286, 483, 257, 7123, 50870], 'temperature': 0.0, 'avg_logprob': -0.15223585642301118, 'compression_ratio': 1.7303921568627452, 'no_speech_prob': 0.08991233259439468}, {'id': 133, 'seek': 113276, 'start': 1142.8799999999999, 'end': 1151.2, 'text': ' D2 with A again, so here I will say D1 is in the set for variable A. Right, so when', 'tokens': [50870, 413, 17, 365, 316, 797, 11, 370, 510, 286, 486, 584, 413, 16, 307, 294, 264, 992, 337, 7006, 316, 13, 1779, 11, 370, 562, 51286], 'temperature': 0.0, 'avg_logprob': -0.15223585642301118, 'compression_ratio': 1.7303921568627452, 'no_speech_prob': 0.08991233259439468}, {'id': 134, 'seek': 113276, 'start': 1151.2, 'end': 1157.24, 'text': ' you get D2, you will have to remove this out and you say now I know that this is D2 which', 'tokens': [51286, 291, 483, 413, 17, 11, 291, 486, 362, 281, 4159, 341, 484, 293, 291, 584, 586, 286, 458, 300, 341, 307, 413, 17, 597, 51588], 'temperature': 0.0, 'avg_logprob': -0.15223585642301118, 'compression_ratio': 1.7303921568627452, 'no_speech_prob': 0.08991233259439468}, {'id': 135, 'seek': 115724, 'start': 1157.24, 'end': 1163.04, 'text': ' is the most recent definition of A. So when you reach here, you keep on doing it, so you', 'tokens': [50364, 307, 264, 881, 5162, 7123, 295, 316, 13, 407, 562, 291, 2524, 510, 11, 291, 1066, 322, 884, 309, 11, 370, 291, 50654], 'temperature': 0.0, 'avg_logprob': -0.21139984972336712, 'compression_ratio': 1.6149425287356323, 'no_speech_prob': 0.024862999096512794}, {'id': 136, 'seek': 115724, 'start': 1163.04, 'end': 1166.84, 'text': ' will then you will have the most recent definitions for all variables in the program along that', 'tokens': [50654, 486, 550, 291, 486, 362, 264, 881, 5162, 21988, 337, 439, 9102, 294, 264, 1461, 2051, 300, 50844], 'temperature': 0.0, 'avg_logprob': -0.21139984972336712, 'compression_ratio': 1.6149425287356323, 'no_speech_prob': 0.024862999096512794}, {'id': 137, 'seek': 115724, 'start': 1166.84, 'end': 1187.2, 'text': ' path. Can this work? So right now we are assuming there is only one path because we are thinking', 'tokens': [50844, 3100, 13, 1664, 341, 589, 30, 407, 558, 586, 321, 366, 11926, 456, 307, 787, 472, 3100, 570, 321, 366, 1953, 51862], 'temperature': 0.0, 'avg_logprob': -0.21139984972336712, 'compression_ratio': 1.6149425287356323, 'no_speech_prob': 0.024862999096512794}, {'id': 138, 'seek': 118720, 'start': 1187.2, 'end': 1200.04, 'text': ' of this particular solution. But so are you convinced about this particular solution?', 'tokens': [50364, 295, 341, 1729, 3827, 13, 583, 370, 366, 291, 12561, 466, 341, 1729, 3827, 30, 51006], 'temperature': 0.0, 'avg_logprob': -0.36786307786640365, 'compression_ratio': 1.2318840579710144, 'no_speech_prob': 0.6154593229293823}, {'id': 139, 'seek': 120004, 'start': 1200.04, 'end': 1218.68, 'text': ' Will this work? After this particular definition, after that statement, there is another definition', 'tokens': [50364, 3099, 341, 589, 30, 2381, 341, 1729, 7123, 11, 934, 300, 5629, 11, 456, 307, 1071, 7123, 51296], 'temperature': 0.0, 'avg_logprob': -0.23763654543005902, 'compression_ratio': 1.56, 'no_speech_prob': 0.2877330780029297}, {'id': 140, 'seek': 120004, 'start': 1218.68, 'end': 1228.2, 'text': ' A, yeah, yeah but that is another path, right, that becomes another path. So path is a sequence', 'tokens': [51296, 316, 11, 1338, 11, 1338, 457, 300, 307, 1071, 3100, 11, 558, 11, 300, 3643, 1071, 3100, 13, 407, 3100, 307, 257, 8310, 51772], 'temperature': 0.0, 'avg_logprob': -0.23763654543005902, 'compression_ratio': 1.56, 'no_speech_prob': 0.2877330780029297}, {'id': 141, 'seek': 122820, 'start': 1228.2, 'end': 1232.24, 'text': ' of instructions, so one path would have terminated here, the other path would have gone through', 'tokens': [50364, 295, 9415, 11, 370, 472, 3100, 576, 362, 1433, 5410, 510, 11, 264, 661, 3100, 576, 362, 2780, 807, 50566], 'temperature': 0.0, 'avg_logprob': -0.1822290593927557, 'compression_ratio': 1.765625, 'no_speech_prob': 0.9114842414855957}, {'id': 142, 'seek': 122820, 'start': 1232.24, 'end': 1237.88, 'text': ' this and come back here. So this, the problem, you can immediately see the problem here,', 'tokens': [50566, 341, 293, 808, 646, 510, 13, 407, 341, 11, 264, 1154, 11, 291, 393, 4258, 536, 264, 1154, 510, 11, 50848], 'temperature': 0.0, 'avg_logprob': -0.1822290593927557, 'compression_ratio': 1.765625, 'no_speech_prob': 0.9114842414855957}, {'id': 143, 'seek': 122820, 'start': 1237.88, 'end': 1242.88, 'text': ' there can be infinite paths. So immediately understand this solution is not going to work', 'tokens': [50848, 456, 393, 312, 13785, 14518, 13, 407, 4258, 1223, 341, 3827, 307, 406, 516, 281, 589, 51098], 'temperature': 0.0, 'avg_logprob': -0.1822290593927557, 'compression_ratio': 1.765625, 'no_speech_prob': 0.9114842414855957}, {'id': 144, 'seek': 122820, 'start': 1242.88, 'end': 1250.32, 'text': ' for us. But still, I mean, let us do the stupid thing first before we do something else. But', 'tokens': [51098, 337, 505, 13, 583, 920, 11, 286, 914, 11, 718, 505, 360, 264, 6631, 551, 700, 949, 321, 360, 746, 1646, 13, 583, 51470], 'temperature': 0.0, 'avg_logprob': -0.1822290593927557, 'compression_ratio': 1.765625, 'no_speech_prob': 0.9114842414855957}, {'id': 145, 'seek': 122820, 'start': 1250.32, 'end': 1254.68, 'text': ' at least we know that if given a path, we have a not so bad algorithm to compute the', 'tokens': [51470, 412, 1935, 321, 458, 300, 498, 2212, 257, 3100, 11, 321, 362, 257, 406, 370, 1578, 9284, 281, 14722, 264, 51688], 'temperature': 0.0, 'avg_logprob': -0.1822290593927557, 'compression_ratio': 1.765625, 'no_speech_prob': 0.9114842414855957}, {'id': 146, 'seek': 125468, 'start': 1254.68, 'end': 1262.5600000000002, 'text': ' strategic definitions. Now let us try to extend this algorithm to something slightly more', 'tokens': [50364, 10924, 21988, 13, 823, 718, 505, 853, 281, 10101, 341, 9284, 281, 746, 4748, 544, 50758], 'temperature': 0.0, 'avg_logprob': -0.691062425312243, 'compression_ratio': 1.1265822784810127, 'no_speech_prob': 0.978348433971405}, {'id': 147, 'seek': 126256, 'start': 1262.56, 'end': 1288.2, 'text': ' doable. So what can we do? So now let us say we have a control flow graph', 'tokens': [50364, 41183, 13, 407, 437, 393, 321, 360, 30, 407, 586, 718, 505, 584, 321, 362, 257, 1969, 3095, 4295, 51646], 'temperature': 0.0, 'avg_logprob': -0.13103250835252844, 'compression_ratio': 1.0, 'no_speech_prob': 0.12076332420110703}, {'id': 148, 'seek': 128820, 'start': 1289.04, 'end': 1294.28, 'text': ' out what are the reaching definitions at this location. So at this location, maybe at this', 'tokens': [50406, 484, 437, 366, 264, 9906, 21988, 412, 341, 4914, 13, 407, 412, 341, 4914, 11, 1310, 412, 341, 50668], 'temperature': 0.0, 'avg_logprob': -0.2840762244330512, 'compression_ratio': 1.5299145299145298, 'no_speech_prob': 0.7478327751159668}, {'id': 149, 'seek': 128820, 'start': 1294.28, 'end': 1316.28, 'text': ' location. No. So this guy is not dominating it. But if there is a definition of A equals', 'tokens': [50668, 4914, 13, 883, 13, 407, 341, 2146, 307, 406, 43306, 309, 13, 583, 498, 456, 307, 257, 7123, 295, 316, 6915, 51768], 'temperature': 0.0, 'avg_logprob': -0.2840762244330512, 'compression_ratio': 1.5299145299145298, 'no_speech_prob': 0.7478327751159668}, {'id': 150, 'seek': 131628, 'start': 1316.28, 'end': 1320.56, 'text': ' something that will also reach here, because there is a path through this and there is', 'tokens': [50364, 746, 300, 486, 611, 2524, 510, 11, 570, 456, 307, 257, 3100, 807, 341, 293, 456, 307, 50578], 'temperature': 0.0, 'avg_logprob': -0.17084014138510062, 'compression_ratio': 1.7681159420289856, 'no_speech_prob': 0.2479521483182907}, {'id': 151, 'seek': 131628, 'start': 1320.56, 'end': 1327.96, 'text': ' nobody stopping it here. Okay, so we start, so let us do a full summary once more, right.', 'tokens': [50578, 5079, 12767, 309, 510, 13, 1033, 11, 370, 321, 722, 11, 370, 718, 505, 360, 257, 1577, 12691, 1564, 544, 11, 558, 13, 50948], 'temperature': 0.0, 'avg_logprob': -0.17084014138510062, 'compression_ratio': 1.7681159420289856, 'no_speech_prob': 0.2479521483182907}, {'id': 152, 'seek': 131628, 'start': 1327.96, 'end': 1335.56, 'text': ' First, we are looking at this problem of reaching definitions. What is this problem of reaching', 'tokens': [50948, 2386, 11, 321, 366, 1237, 412, 341, 1154, 295, 9906, 21988, 13, 708, 307, 341, 1154, 295, 9906, 51328], 'temperature': 0.0, 'avg_logprob': -0.17084014138510062, 'compression_ratio': 1.7681159420289856, 'no_speech_prob': 0.2479521483182907}, {'id': 153, 'seek': 131628, 'start': 1335.56, 'end': 1341.0, 'text': ' definitions? The problem of reaching definition is that at any program point, I would like to', 'tokens': [51328, 21988, 30, 440, 1154, 295, 9906, 7123, 307, 300, 412, 604, 1461, 935, 11, 286, 576, 411, 281, 51600], 'temperature': 0.0, 'avg_logprob': -0.17084014138510062, 'compression_ratio': 1.7681159420289856, 'no_speech_prob': 0.2479521483182907}, {'id': 154, 'seek': 134100, 'start': 1341.0, 'end': 1349.96, 'text': ' figure out what is the set of definitions which are reaching this program point. Question was,', 'tokens': [50364, 2573, 484, 437, 307, 264, 992, 295, 21988, 597, 366, 9906, 341, 1461, 935, 13, 14464, 390, 11, 50812], 'temperature': 0.0, 'avg_logprob': -0.1774010385785784, 'compression_ratio': 1.6783625730994152, 'no_speech_prob': 0.07758519798517227}, {'id': 155, 'seek': 134100, 'start': 1349.96, 'end': 1357.28, 'text': ' what is the definition? A definition is something of the form D equals or A equals B plus C. A', 'tokens': [50812, 437, 307, 264, 7123, 30, 316, 7123, 307, 746, 295, 264, 1254, 413, 6915, 420, 316, 6915, 363, 1804, 383, 13, 316, 51178], 'temperature': 0.0, 'avg_logprob': -0.1774010385785784, 'compression_ratio': 1.6783625730994152, 'no_speech_prob': 0.07758519798517227}, {'id': 156, 'seek': 134100, 'start': 1357.28, 'end': 1368.0, 'text': ' equals B plus C is a definition of A because it is assigning a value to A. Every such assignment,', 'tokens': [51178, 6915, 363, 1804, 383, 307, 257, 7123, 295, 316, 570, 309, 307, 49602, 257, 2158, 281, 316, 13, 2048, 1270, 15187, 11, 51714], 'temperature': 0.0, 'avg_logprob': -0.1774010385785784, 'compression_ratio': 1.6783625730994152, 'no_speech_prob': 0.07758519798517227}, {'id': 157, 'seek': 136800, 'start': 1368.0, 'end': 1373.4, 'text': ' every such lexically different instruction in the program, no matter they look alike or they look', 'tokens': [50364, 633, 1270, 476, 87, 984, 819, 10951, 294, 264, 1461, 11, 572, 1871, 436, 574, 20025, 420, 436, 574, 50634], 'temperature': 0.0, 'avg_logprob': -0.15328684518503588, 'compression_ratio': 1.7867298578199051, 'no_speech_prob': 0.004325553774833679}, {'id': 158, 'seek': 136800, 'start': 1373.4, 'end': 1381.52, 'text': ' different, constitutes a different definition. Now in my program, I can construct my universal', 'tokens': [50634, 819, 11, 44204, 257, 819, 7123, 13, 823, 294, 452, 1461, 11, 286, 393, 7690, 452, 11455, 51040], 'temperature': 0.0, 'avg_logprob': -0.15328684518503588, 'compression_ratio': 1.7867298578199051, 'no_speech_prob': 0.004325553774833679}, {'id': 159, 'seek': 136800, 'start': 1381.52, 'end': 1388.6, 'text': ' set. That is a set of all definitions in the program. How? By simply collecting all the', 'tokens': [51040, 992, 13, 663, 307, 257, 992, 295, 439, 21988, 294, 264, 1461, 13, 1012, 30, 3146, 2935, 12510, 439, 264, 51394], 'temperature': 0.0, 'avg_logprob': -0.15328684518503588, 'compression_ratio': 1.7867298578199051, 'no_speech_prob': 0.004325553774833679}, {'id': 160, 'seek': 136800, 'start': 1388.6, 'end': 1396.4, 'text': ' assignments to these, all these assignments to this in the program. So every time I get A equals', 'tokens': [51394, 22546, 281, 613, 11, 439, 613, 22546, 281, 341, 294, 264, 1461, 13, 407, 633, 565, 286, 483, 316, 6915, 51784], 'temperature': 0.0, 'avg_logprob': -0.15328684518503588, 'compression_ratio': 1.7867298578199051, 'no_speech_prob': 0.004325553774833679}, {'id': 161, 'seek': 139640, 'start': 1396.4, 'end': 1401.6000000000001, 'text': ' B plus C, C equals A plus B plus A, I mean all of them I just collect and that becomes my set of', 'tokens': [50364, 363, 1804, 383, 11, 383, 6915, 316, 1804, 363, 1804, 316, 11, 286, 914, 439, 295, 552, 286, 445, 2500, 293, 300, 3643, 452, 992, 295, 50624], 'temperature': 0.0, 'avg_logprob': -0.11934696276163317, 'compression_ratio': 1.6752136752136753, 'no_speech_prob': 0.05167502537369728}, {'id': 162, 'seek': 139640, 'start': 1401.6000000000001, 'end': 1408.88, 'text': ' all possible definitions. That is my universal set. Now what we want to do is, I would like to figure', 'tokens': [50624, 439, 1944, 21988, 13, 663, 307, 452, 11455, 992, 13, 823, 437, 321, 528, 281, 360, 307, 11, 286, 576, 411, 281, 2573, 50988], 'temperature': 0.0, 'avg_logprob': -0.11934696276163317, 'compression_ratio': 1.6752136752136753, 'no_speech_prob': 0.05167502537369728}, {'id': 163, 'seek': 139640, 'start': 1408.88, 'end': 1414.0400000000002, 'text': ' out that what are the set of definitions reaching a particular program point. Let us say this', 'tokens': [50988, 484, 300, 437, 366, 264, 992, 295, 21988, 9906, 257, 1729, 1461, 935, 13, 961, 505, 584, 341, 51246], 'temperature': 0.0, 'avg_logprob': -0.11934696276163317, 'compression_ratio': 1.6752136752136753, 'no_speech_prob': 0.05167502537369728}, {'id': 164, 'seek': 139640, 'start': 1414.0400000000002, 'end': 1423.64, 'text': ' particular program point. How would we do that? So one solution we first figured out was a solution', 'tokens': [51246, 1729, 1461, 935, 13, 1012, 576, 321, 360, 300, 30, 407, 472, 3827, 321, 700, 8932, 484, 390, 257, 3827, 51726], 'temperature': 0.0, 'avg_logprob': -0.11934696276163317, 'compression_ratio': 1.6752136752136753, 'no_speech_prob': 0.05167502537369728}, {'id': 165, 'seek': 142364, 'start': 1423.64, 'end': 1430.0, 'text': ' which is not even practical, but one way to do that would have been that I figure out all possible', 'tokens': [50364, 597, 307, 406, 754, 8496, 11, 457, 472, 636, 281, 360, 300, 576, 362, 668, 300, 286, 2573, 484, 439, 1944, 50682], 'temperature': 0.0, 'avg_logprob': -0.1367679001182638, 'compression_ratio': 1.6899563318777293, 'no_speech_prob': 0.21585063636302948}, {'id': 166, 'seek': 142364, 'start': 1430.0, 'end': 1436.64, 'text': ' paths to that location from the program entry point. Along every path, every path is nothing', 'tokens': [50682, 14518, 281, 300, 4914, 490, 264, 1461, 8729, 935, 13, 17457, 633, 3100, 11, 633, 3100, 307, 1825, 51014], 'temperature': 0.0, 'avg_logprob': -0.1367679001182638, 'compression_ratio': 1.6899563318777293, 'no_speech_prob': 0.21585063636302948}, {'id': 167, 'seek': 142364, 'start': 1436.64, 'end': 1442.1200000000001, 'text': ' but a sequence of instructions. Along every path, I collect what are the definitions which finally', 'tokens': [51014, 457, 257, 8310, 295, 9415, 13, 17457, 633, 3100, 11, 286, 2500, 437, 366, 264, 21988, 597, 2721, 51288], 'temperature': 0.0, 'avg_logprob': -0.1367679001182638, 'compression_ratio': 1.6899563318777293, 'no_speech_prob': 0.21585063636302948}, {'id': 168, 'seek': 142364, 'start': 1442.1200000000001, 'end': 1453.0800000000002, 'text': ' reach the final location. So how do we do that? So in the path, I start with an empty set. I say', 'tokens': [51288, 2524, 264, 2572, 4914, 13, 407, 577, 360, 321, 360, 300, 30, 407, 294, 264, 3100, 11, 286, 722, 365, 364, 6707, 992, 13, 286, 584, 51836], 'temperature': 0.0, 'avg_logprob': -0.1367679001182638, 'compression_ratio': 1.6899563318777293, 'no_speech_prob': 0.21585063636302948}, {'id': 169, 'seek': 145308, 'start': 1453.08, 'end': 1459.6, 'text': ' I do not know what all is there. Whenever I encounter a statement of the form A equals B', 'tokens': [50364, 286, 360, 406, 458, 437, 439, 307, 456, 13, 14159, 286, 8593, 257, 5629, 295, 264, 1254, 316, 6915, 363, 50690], 'temperature': 0.0, 'avg_logprob': -0.13302501472266945, 'compression_ratio': 1.5806451612903225, 'no_speech_prob': 0.006272393744438887}, {'id': 170, 'seek': 145308, 'start': 1459.6, 'end': 1475.6799999999998, 'text': ' plus C, then what do we do? We add this definition D1 as the new definition which may reach the final', 'tokens': [50690, 1804, 383, 11, 550, 437, 360, 321, 360, 30, 492, 909, 341, 7123, 413, 16, 382, 264, 777, 7123, 597, 815, 2524, 264, 2572, 51494], 'temperature': 0.0, 'avg_logprob': -0.13302501472266945, 'compression_ratio': 1.5806451612903225, 'no_speech_prob': 0.006272393744438887}, {'id': 171, 'seek': 145308, 'start': 1475.6799999999998, 'end': 1482.48, 'text': ' location. But at the same time, if this A redefines, if this particular definition redefines some other', 'tokens': [51494, 4914, 13, 583, 412, 264, 912, 565, 11, 498, 341, 316, 38818, 1652, 11, 498, 341, 1729, 7123, 38818, 1652, 512, 661, 51834], 'temperature': 0.0, 'avg_logprob': -0.13302501472266945, 'compression_ratio': 1.5806451612903225, 'no_speech_prob': 0.006272393744438887}, {'id': 172, 'seek': 148248, 'start': 1482.48, 'end': 1487.28, 'text': ' definition, that definition has to be removed from the set. So at every point we do two things.', 'tokens': [50364, 7123, 11, 300, 7123, 575, 281, 312, 7261, 490, 264, 992, 13, 407, 412, 633, 935, 321, 360, 732, 721, 13, 50604], 'temperature': 0.0, 'avg_logprob': -0.14429952881552957, 'compression_ratio': 1.9895833333333333, 'no_speech_prob': 0.004589073825627565}, {'id': 173, 'seek': 148248, 'start': 1487.28, 'end': 1495.2, 'text': ' We do something called generation which is we add something to this set and we do something', 'tokens': [50604, 492, 360, 746, 1219, 5125, 597, 307, 321, 909, 746, 281, 341, 992, 293, 321, 360, 746, 51000], 'temperature': 0.0, 'avg_logprob': -0.14429952881552957, 'compression_ratio': 1.9895833333333333, 'no_speech_prob': 0.004589073825627565}, {'id': 174, 'seek': 148248, 'start': 1495.2, 'end': 1501.3600000000001, 'text': ' called killing which is we kill something of the set. We remove something of the set. What do we', 'tokens': [51000, 1219, 8011, 597, 307, 321, 1961, 746, 295, 264, 992, 13, 492, 4159, 746, 295, 264, 992, 13, 708, 360, 321, 51308], 'temperature': 0.0, 'avg_logprob': -0.14429952881552957, 'compression_ratio': 1.9895833333333333, 'no_speech_prob': 0.004589073825627565}, {'id': 175, 'seek': 148248, 'start': 1501.3600000000001, 'end': 1509.8, 'text': ' generate? The new definition that I encounter and what do we kill? Any existing definition to the', 'tokens': [51308, 8460, 30, 440, 777, 7123, 300, 286, 8593, 293, 437, 360, 321, 1961, 30, 2639, 6741, 7123, 281, 264, 51730], 'temperature': 0.0, 'avg_logprob': -0.14429952881552957, 'compression_ratio': 1.9895833333333333, 'no_speech_prob': 0.004589073825627565}, {'id': 176, 'seek': 150980, 'start': 1509.8, 'end': 1519.04, 'text': ' same variable. So my set will always contain. So that is what this thing is. So these are the two', 'tokens': [50364, 912, 7006, 13, 407, 452, 992, 486, 1009, 5304, 13, 407, 300, 307, 437, 341, 551, 307, 13, 407, 613, 366, 264, 732, 50826], 'temperature': 0.0, 'avg_logprob': -0.15241681204901802, 'compression_ratio': 1.5161290322580645, 'no_speech_prob': 0.274381548166275}, {'id': 177, 'seek': 150980, 'start': 1519.04, 'end': 1528.08, 'text': ' steps that I have to keep on checking. What to generate and what to kill. Now can we extend?', 'tokens': [50826, 4439, 300, 286, 362, 281, 1066, 322, 8568, 13, 708, 281, 8460, 293, 437, 281, 1961, 13, 823, 393, 321, 10101, 30, 51278], 'temperature': 0.0, 'avg_logprob': -0.15241681204901802, 'compression_ratio': 1.5161290322580645, 'no_speech_prob': 0.274381548166275}, {'id': 178, 'seek': 150980, 'start': 1528.08, 'end': 1533.2, 'text': ' Now the question is instead of a sequence of statements, if we have a complex control flow,', 'tokens': [51278, 823, 264, 1168, 307, 2602, 295, 257, 8310, 295, 12363, 11, 498, 321, 362, 257, 3997, 1969, 3095, 11, 51534], 'temperature': 0.0, 'avg_logprob': -0.15241681204901802, 'compression_ratio': 1.5161290322580645, 'no_speech_prob': 0.274381548166275}, {'id': 179, 'seek': 153320, 'start': 1533.2, 'end': 1540.8, 'text': ' can we answer this question in somewhat not so expensive manner? So let us see what is the', 'tokens': [50364, 393, 321, 1867, 341, 1168, 294, 8344, 406, 370, 5124, 9060, 30, 407, 718, 505, 536, 437, 307, 264, 50744], 'temperature': 0.0, 'avg_logprob': -0.14146156101436405, 'compression_ratio': 1.6304347826086956, 'no_speech_prob': 0.2383771389722824}, {'id': 180, 'seek': 153320, 'start': 1540.8, 'end': 1548.8400000000001, 'text': ' problem here. Till this point there is no issue at all. I can start with an empty set. Whatever', 'tokens': [50744, 1154, 510, 13, 20227, 341, 935, 456, 307, 572, 2734, 412, 439, 13, 286, 393, 722, 365, 364, 6707, 992, 13, 8541, 51146], 'temperature': 0.0, 'avg_logprob': -0.14146156101436405, 'compression_ratio': 1.6304347826086956, 'no_speech_prob': 0.2383771389722824}, {'id': 181, 'seek': 153320, 'start': 1548.8400000000001, 'end': 1556.0800000000002, 'text': ' definition is in this basic block, I add it to this set. Again I go to this particular location.', 'tokens': [51146, 7123, 307, 294, 341, 3875, 3461, 11, 286, 909, 309, 281, 341, 992, 13, 3764, 286, 352, 281, 341, 1729, 4914, 13, 51508], 'temperature': 0.0, 'avg_logprob': -0.14146156101436405, 'compression_ratio': 1.6304347826086956, 'no_speech_prob': 0.2383771389722824}, {'id': 182, 'seek': 153320, 'start': 1556.0800000000002, 'end': 1561.2, 'text': ' Whatever is in this set, I generate the new instruction and if it redefines something which', 'tokens': [51508, 8541, 307, 294, 341, 992, 11, 286, 8460, 264, 777, 10951, 293, 498, 309, 38818, 1652, 746, 597, 51764], 'temperature': 0.0, 'avg_logprob': -0.14146156101436405, 'compression_ratio': 1.6304347826086956, 'no_speech_prob': 0.2383771389722824}, {'id': 183, 'seek': 156120, 'start': 1561.2, 'end': 1570.16, 'text': ' was in the previous basic block, I kill it. The problem is here. Whenever these two basic', 'tokens': [50364, 390, 294, 264, 3894, 3875, 3461, 11, 286, 1961, 309, 13, 440, 1154, 307, 510, 13, 14159, 613, 732, 3875, 50812], 'temperature': 0.0, 'avg_logprob': -0.15868191008872173, 'compression_ratio': 1.6533333333333333, 'no_speech_prob': 0.05807562917470932}, {'id': 184, 'seek': 156120, 'start': 1570.16, 'end': 1575.3600000000001, 'text': ' blocks merge at a basic block, what do I say is the reaching definition here at the entry to this', 'tokens': [50812, 8474, 22183, 412, 257, 3875, 3461, 11, 437, 360, 286, 584, 307, 264, 9906, 7123, 510, 412, 264, 8729, 281, 341, 51072], 'temperature': 0.0, 'avg_logprob': -0.15868191008872173, 'compression_ratio': 1.6533333333333333, 'no_speech_prob': 0.05807562917470932}, {'id': 185, 'seek': 156120, 'start': 1575.3600000000001, 'end': 1580.44, 'text': ' basic block? That is just before the statement contained in this basic block. What should I do?', 'tokens': [51072, 3875, 3461, 30, 663, 307, 445, 949, 264, 5629, 16212, 294, 341, 3875, 3461, 13, 708, 820, 286, 360, 30, 51326], 'temperature': 0.0, 'avg_logprob': -0.15868191008872173, 'compression_ratio': 1.6533333333333333, 'no_speech_prob': 0.05807562917470932}, {'id': 186, 'seek': 156120, 'start': 1580.44, 'end': 1589.0, 'text': ' I need to keep both. Excellent. Why is that? Because if there is a A equal to this and A', 'tokens': [51326, 286, 643, 281, 1066, 1293, 13, 16723, 13, 1545, 307, 300, 30, 1436, 498, 456, 307, 257, 316, 2681, 281, 341, 293, 316, 51754], 'temperature': 0.0, 'avg_logprob': -0.15868191008872173, 'compression_ratio': 1.6533333333333333, 'no_speech_prob': 0.05807562917470932}, {'id': 187, 'seek': 158900, 'start': 1589.0, 'end': 1593.76, 'text': ' equal to this, then both of them are reaching definition to this particular location. Both of', 'tokens': [50364, 2681, 281, 341, 11, 550, 1293, 295, 552, 366, 9906, 7123, 281, 341, 1729, 4914, 13, 6767, 295, 50602], 'temperature': 0.0, 'avg_logprob': -0.1440368143717448, 'compression_ratio': 1.6826347305389222, 'no_speech_prob': 0.019550679251551628}, {'id': 188, 'seek': 158900, 'start': 1593.76, 'end': 1600.88, 'text': ' them are reaching here. So what should I do? So I have a set here, I have a set here, so I need', 'tokens': [50602, 552, 366, 9906, 510, 13, 407, 437, 820, 286, 360, 30, 407, 286, 362, 257, 992, 510, 11, 286, 362, 257, 992, 510, 11, 370, 286, 643, 50958], 'temperature': 0.0, 'avg_logprob': -0.1440368143717448, 'compression_ratio': 1.6826347305389222, 'no_speech_prob': 0.019550679251551628}, {'id': 189, 'seek': 158900, 'start': 1600.88, 'end': 1613.08, 'text': ' to take the union of the sets. So essentially what we do is, so one way of visualizing this', 'tokens': [50958, 281, 747, 264, 11671, 295, 264, 6352, 13, 407, 4476, 437, 321, 360, 307, 11, 370, 472, 636, 295, 5056, 3319, 341, 51568], 'temperature': 0.0, 'avg_logprob': -0.1440368143717448, 'compression_ratio': 1.6826347305389222, 'no_speech_prob': 0.019550679251551628}, {'id': 190, 'seek': 161308, 'start': 1613.08, 'end': 1621.52, 'text': ' whole business is that you can think that every definition is generating a token stamped with that', 'tokens': [50364, 1379, 1606, 307, 300, 291, 393, 519, 300, 633, 7123, 307, 17746, 257, 14862, 39111, 365, 300, 50786], 'temperature': 0.0, 'avg_logprob': -0.15827148144061748, 'compression_ratio': 1.576086956521739, 'no_speech_prob': 0.647820234298706}, {'id': 191, 'seek': 161308, 'start': 1621.52, 'end': 1629.22, 'text': ' location, the program point where it is generated. And then we leave these tokens to flow in the CFG.', 'tokens': [50786, 4914, 11, 264, 1461, 935, 689, 309, 307, 10833, 13, 400, 550, 321, 1856, 613, 22667, 281, 3095, 294, 264, 21792, 38, 13, 51171], 'temperature': 0.0, 'avg_logprob': -0.15827148144061748, 'compression_ratio': 1.576086956521739, 'no_speech_prob': 0.647820234298706}, {'id': 192, 'seek': 161308, 'start': 1629.22, 'end': 1638.84, 'text': ' If it hits another location which has a token marked for the same variable or a statement', 'tokens': [51171, 759, 309, 8664, 1071, 4914, 597, 575, 257, 14862, 12658, 337, 264, 912, 7006, 420, 257, 5629, 51652], 'temperature': 0.0, 'avg_logprob': -0.15827148144061748, 'compression_ratio': 1.576086956521739, 'no_speech_prob': 0.647820234298706}, {'id': 193, 'seek': 163884, 'start': 1638.84, 'end': 1644.08, 'text': ' which redefines the token for that variable, that token gets killed, it gets thrown out.', 'tokens': [50364, 597, 38818, 1652, 264, 14862, 337, 300, 7006, 11, 300, 14862, 2170, 4652, 11, 309, 2170, 11732, 484, 13, 50626], 'temperature': 0.0, 'avg_logprob': -0.13040447235107422, 'compression_ratio': 1.9256198347107438, 'no_speech_prob': 0.22722718119621277}, {'id': 194, 'seek': 163884, 'start': 1644.08, 'end': 1650.4399999999998, 'text': ' Whenever it gets to a position where it can go in both the directions, either of the two', 'tokens': [50626, 14159, 309, 2170, 281, 257, 2535, 689, 309, 393, 352, 294, 1293, 264, 11095, 11, 2139, 295, 264, 732, 50944], 'temperature': 0.0, 'avg_logprob': -0.13040447235107422, 'compression_ratio': 1.9256198347107438, 'no_speech_prob': 0.22722718119621277}, {'id': 195, 'seek': 163884, 'start': 1650.4399999999998, 'end': 1657.28, 'text': ' directions, what does it do? It propagates a token in both the directions. And if there is a', 'tokens': [50944, 11095, 11, 437, 775, 309, 360, 30, 467, 12425, 1024, 257, 14862, 294, 1293, 264, 11095, 13, 400, 498, 456, 307, 257, 51286], 'temperature': 0.0, 'avg_logprob': -0.13040447235107422, 'compression_ratio': 1.9256198347107438, 'no_speech_prob': 0.22722718119621277}, {'id': 196, 'seek': 163884, 'start': 1657.28, 'end': 1661.9199999999998, 'text': ' location where I have multiple arrows coming in, then I collect the tokens along all the directions.', 'tokens': [51286, 4914, 689, 286, 362, 3866, 19669, 1348, 294, 11, 550, 286, 2500, 264, 22667, 2051, 439, 264, 11095, 13, 51518], 'temperature': 0.0, 'avg_logprob': -0.13040447235107422, 'compression_ratio': 1.9256198347107438, 'no_speech_prob': 0.22722718119621277}, {'id': 197, 'seek': 163884, 'start': 1661.9199999999998, 'end': 1667.48, 'text': ' I keep on doing this collection of tokens and at any location what is the set of tokens I have', 'tokens': [51518, 286, 1066, 322, 884, 341, 5765, 295, 22667, 293, 412, 604, 4914, 437, 307, 264, 992, 295, 22667, 286, 362, 51796], 'temperature': 0.0, 'avg_logprob': -0.13040447235107422, 'compression_ratio': 1.9256198347107438, 'no_speech_prob': 0.22722718119621277}, {'id': 198, 'seek': 166748, 'start': 1667.48, 'end': 1672.16, 'text': ' becomes my set of reaching definitions. All the predecessors means how would you do it,', 'tokens': [50364, 3643, 452, 992, 295, 9906, 21988, 13, 1057, 264, 24874, 45700, 1355, 577, 576, 291, 360, 309, 11, 50598], 'temperature': 0.0, 'avg_logprob': -0.2016779768700693, 'compression_ratio': 1.8803418803418803, 'no_speech_prob': 0.13622643053531647}, {'id': 199, 'seek': 166748, 'start': 1672.16, 'end': 1679.0, 'text': ' how will you update this guy saying what definitions were there? So this algorithm,', 'tokens': [50598, 577, 486, 291, 5623, 341, 2146, 1566, 437, 21988, 645, 456, 30, 407, 341, 9284, 11, 50940], 'temperature': 0.0, 'avg_logprob': -0.2016779768700693, 'compression_ratio': 1.8803418803418803, 'no_speech_prob': 0.13622643053531647}, {'id': 200, 'seek': 166748, 'start': 1679.0, 'end': 1685.28, 'text': ' I would like to get the set of all reaching definitions at all program points, not at one', 'tokens': [50940, 286, 576, 411, 281, 483, 264, 992, 295, 439, 9906, 21988, 412, 439, 1461, 2793, 11, 406, 412, 472, 51254], 'temperature': 0.0, 'avg_logprob': -0.2016779768700693, 'compression_ratio': 1.8803418803418803, 'no_speech_prob': 0.13622643053531647}, {'id': 201, 'seek': 166748, 'start': 1685.28, 'end': 1691.46, 'text': ' program point. In one shot like the way we were computing dominators, we wanted to get the', 'tokens': [51254, 1461, 935, 13, 682, 472, 3347, 411, 264, 636, 321, 645, 15866, 8859, 3391, 11, 321, 1415, 281, 483, 264, 51563], 'temperature': 0.0, 'avg_logprob': -0.2016779768700693, 'compression_ratio': 1.8803418803418803, 'no_speech_prob': 0.13622643053531647}, {'id': 202, 'seek': 166748, 'start': 1691.46, 'end': 1697.1200000000001, 'text': ' dominator tree which means I need to compute the dominators for all instructions or all', 'tokens': [51563, 8859, 1639, 4230, 597, 1355, 286, 643, 281, 14722, 264, 8859, 3391, 337, 439, 9415, 420, 439, 51846], 'temperature': 0.0, 'avg_logprob': -0.2016779768700693, 'compression_ratio': 1.8803418803418803, 'no_speech_prob': 0.13622643053531647}, {'id': 203, 'seek': 169712, 'start': 1697.12, 'end': 1701.9199999999998, 'text': ' statements. Similarly, here I would like to get the reaching definition set for all locations.', 'tokens': [50364, 12363, 13, 13157, 11, 510, 286, 576, 411, 281, 483, 264, 9906, 7123, 992, 337, 439, 9253, 13, 50604], 'temperature': 0.0, 'avg_logprob': -0.1249525805553758, 'compression_ratio': 1.7941176470588236, 'no_speech_prob': 0.0009985709330067039}, {'id': 204, 'seek': 169712, 'start': 1704.8, 'end': 1710.1999999999998, 'text': ' So this algorithm is very similar to the dominator algorithm. So we compute something and extend it', 'tokens': [50748, 407, 341, 9284, 307, 588, 2531, 281, 264, 8859, 1639, 9284, 13, 407, 321, 14722, 746, 293, 10101, 309, 51018], 'temperature': 0.0, 'avg_logprob': -0.1249525805553758, 'compression_ratio': 1.7941176470588236, 'no_speech_prob': 0.0009985709330067039}, {'id': 205, 'seek': 169712, 'start': 1710.1999999999998, 'end': 1714.1599999999999, 'text': ' slightly for the next location, extend it slightly for the next location and keep on', 'tokens': [51018, 4748, 337, 264, 958, 4914, 11, 10101, 309, 4748, 337, 264, 958, 4914, 293, 1066, 322, 51216], 'temperature': 0.0, 'avg_logprob': -0.1249525805553758, 'compression_ratio': 1.7941176470588236, 'no_speech_prob': 0.0009985709330067039}, {'id': 206, 'seek': 169712, 'start': 1714.1599999999999, 'end': 1722.08, 'text': ' doing it till my full flow graph is full. So essentially what we do is there is a flow', 'tokens': [51216, 884, 309, 4288, 452, 1577, 3095, 4295, 307, 1577, 13, 407, 4476, 437, 321, 360, 307, 456, 307, 257, 3095, 51612], 'temperature': 0.0, 'avg_logprob': -0.1249525805553758, 'compression_ratio': 1.7941176470588236, 'no_speech_prob': 0.0009985709330067039}, {'id': 207, 'seek': 172208, 'start': 1722.08, 'end': 1729.8, 'text': ' of information forward because we move from one basic block and the tokens move downwards', 'tokens': [50364, 295, 1589, 2128, 570, 321, 1286, 490, 472, 3875, 3461, 293, 264, 22667, 1286, 39880, 50750], 'temperature': 0.0, 'avg_logprob': -0.19358794312728078, 'compression_ratio': 1.4473684210526316, 'no_speech_prob': 0.0740792378783226}, {'id': 208, 'seek': 172208, 'start': 1729.8, 'end': 1737.84, 'text': ' and whenever we collect these tokens what we do is we take a union of them.', 'tokens': [50750, 293, 5699, 321, 2500, 613, 22667, 437, 321, 360, 307, 321, 747, 257, 11671, 295, 552, 13, 51152], 'temperature': 0.0, 'avg_logprob': -0.19358794312728078, 'compression_ratio': 1.4473684210526316, 'no_speech_prob': 0.0740792378783226}, {'id': 209, 'seek': 173784, 'start': 1737.84, 'end': 1754.0, 'text': ' Right? Okay. Do you sort of at least intuitively understand this algorithm? Can we try writing', 'tokens': [50364, 1779, 30, 1033, 13, 1144, 291, 1333, 295, 412, 1935, 46506, 1223, 341, 9284, 30, 1664, 321, 853, 3579, 51172], 'temperature': 0.0, 'avg_logprob': -0.229326579881751, 'compression_ratio': 1.3507462686567164, 'no_speech_prob': 0.04329866170883179}, {'id': 210, 'seek': 173784, 'start': 1754.0, 'end': 1760.36, 'text': ' out or maybe it is too early but let us try. Can we try to write some equation like we', 'tokens': [51172, 484, 420, 1310, 309, 307, 886, 2440, 457, 718, 505, 853, 13, 1664, 321, 853, 281, 2464, 512, 5367, 411, 321, 51490], 'temperature': 0.0, 'avg_logprob': -0.229326579881751, 'compression_ratio': 1.3507462686567164, 'no_speech_prob': 0.04329866170883179}, {'id': 211, 'seek': 176036, 'start': 1760.36, 'end': 1771.36, 'text': ' wrote for dominators? So if I want to write the reaching definition at a node n, at a node n,', 'tokens': [50364, 4114, 337, 8859, 3391, 30, 407, 498, 286, 528, 281, 2464, 264, 9906, 7123, 412, 257, 9984, 297, 11, 412, 257, 9984, 297, 11, 50914], 'temperature': 0.0, 'avg_logprob': -0.2062409210205078, 'compression_ratio': 1.5080645161290323, 'no_speech_prob': 0.3047606348991394}, {'id': 212, 'seek': 176036, 'start': 1771.36, 'end': 1777.7199999999998, 'text': ' maybe I just say it is just after the node has executed, after the statement has executed. So', 'tokens': [50914, 1310, 286, 445, 584, 309, 307, 445, 934, 264, 9984, 575, 17577, 11, 934, 264, 5629, 575, 17577, 13, 407, 51232], 'temperature': 0.0, 'avg_logprob': -0.2062409210205078, 'compression_ratio': 1.5080645161290323, 'no_speech_prob': 0.3047606348991394}, {'id': 213, 'seek': 177772, 'start': 1777.72, 'end': 1790.88, 'text': ' how do I write it? Right. So there are two things. So let us assume that I can say that if it is,', 'tokens': [50364, 577, 360, 286, 2464, 309, 30, 1779, 13, 407, 456, 366, 732, 721, 13, 407, 718, 505, 6552, 300, 286, 393, 584, 300, 498, 309, 307, 11, 51022], 'temperature': 0.0, 'avg_logprob': -0.16099431437830772, 'compression_ratio': 1.1149425287356323, 'no_speech_prob': 0.07349922508001328}, {'id': 214, 'seek': 179088, 'start': 1790.88, 'end': 1811.4, 'text': ' let us say entry to the node, entry to n or it is exit from n. Has it become too bad? Still', 'tokens': [50364, 718, 505, 584, 8729, 281, 264, 9984, 11, 8729, 281, 297, 420, 309, 307, 11043, 490, 297, 13, 8646, 309, 1813, 886, 1578, 30, 8291, 51390], 'temperature': 0.0, 'avg_logprob': -0.1454430284171269, 'compression_ratio': 1.0963855421686748, 'no_speech_prob': 0.012090631760656834}, {'id': 215, 'seek': 181140, 'start': 1811.4, 'end': 1828.96, 'text': ' reach readable? Tell me when it is too, not very good at, let me type it here. Okay. So entry to n,', 'tokens': [50364, 2524, 49857, 30, 5115, 385, 562, 309, 307, 886, 11, 406, 588, 665, 412, 11, 718, 385, 2010, 309, 510, 13, 1033, 13, 407, 8729, 281, 297, 11, 51242], 'temperature': 0.0, 'avg_logprob': -0.4033740162849426, 'compression_ratio': 1.125, 'no_speech_prob': 0.5666669607162476}, {'id': 216, 'seek': 182896, 'start': 1828.96, 'end': 1842.8400000000001, 'text': ' what are we going to do? When take a union over, union over predecessors of, where p is basically', 'tokens': [50364, 437, 366, 321, 516, 281, 360, 30, 1133, 747, 257, 11671, 670, 11, 11671, 670, 24874, 45700, 295, 11, 689, 280, 307, 1936, 51058], 'temperature': 0.0, 'avg_logprob': -0.1954946143954408, 'compression_ratio': 1.437956204379562, 'no_speech_prob': 0.05822504311800003}, {'id': 217, 'seek': 182896, 'start': 1842.8400000000001, 'end': 1852.44, 'text': ' the predecessors of n and I take this thing over the reaching definition of p. Okay. And what about', 'tokens': [51058, 264, 24874, 45700, 295, 297, 293, 286, 747, 341, 551, 670, 264, 9906, 7123, 295, 280, 13, 1033, 13, 400, 437, 466, 51538], 'temperature': 0.0, 'avg_logprob': -0.1954946143954408, 'compression_ratio': 1.437956204379562, 'no_speech_prob': 0.05822504311800003}, {'id': 218, 'seek': 185244, 'start': 1852.44, 'end': 1861.3200000000002, 'text': ' at the exit? So now you can assume that I have the reaching definition of like n begin. You', 'tokens': [50364, 412, 264, 11043, 30, 407, 586, 291, 393, 6552, 300, 286, 362, 264, 9906, 7123, 295, 411, 297, 1841, 13, 509, 50808], 'temperature': 0.0, 'avg_logprob': -0.16450092196464539, 'compression_ratio': 1.3333333333333333, 'no_speech_prob': 0.8328104615211487}, {'id': 219, 'seek': 185244, 'start': 1861.3200000000002, 'end': 1881.64, 'text': ' already have this, then how can I use it? Excellent, excellent. Do you all agree? So', 'tokens': [50808, 1217, 362, 341, 11, 550, 577, 393, 286, 764, 309, 30, 16723, 11, 7103, 13, 1144, 291, 439, 3986, 30, 407, 51824], 'temperature': 0.0, 'avg_logprob': -0.16450092196464539, 'compression_ratio': 1.3333333333333333, 'no_speech_prob': 0.8328104615211487}, {'id': 220, 'seek': 188164, 'start': 1881.64, 'end': 1893.72, 'text': ' what I will do is I will just say this is RD of n begin, union the set of statements that are', 'tokens': [50364, 437, 286, 486, 360, 307, 286, 486, 445, 584, 341, 307, 49488, 295, 297, 1841, 11, 11671, 264, 992, 295, 12363, 300, 366, 50968], 'temperature': 0.0, 'avg_logprob': -0.1755269241333008, 'compression_ratio': 1.7058823529411764, 'no_speech_prob': 0.022184301167726517}, {'id': 221, 'seek': 188164, 'start': 1893.72, 'end': 1899.96, 'text': ' generated. So let us say I have gen of n, which is saying the set of statements generated at this', 'tokens': [50968, 10833, 13, 407, 718, 505, 584, 286, 362, 1049, 295, 297, 11, 597, 307, 1566, 264, 992, 295, 12363, 10833, 412, 341, 51280], 'temperature': 0.0, 'avg_logprob': -0.1755269241333008, 'compression_ratio': 1.7058823529411764, 'no_speech_prob': 0.022184301167726517}, {'id': 222, 'seek': 188164, 'start': 1899.96, 'end': 1908.5200000000002, 'text': ' node minus kill of n, which essentially says what are the set of definitions or did I screw it up,', 'tokens': [51280, 9984, 3175, 1961, 295, 297, 11, 597, 4476, 1619, 437, 366, 264, 992, 295, 21988, 420, 630, 286, 5630, 309, 493, 11, 51708], 'temperature': 0.0, 'avg_logprob': -0.1755269241333008, 'compression_ratio': 1.7058823529411764, 'no_speech_prob': 0.022184301167726517}, {'id': 223, 'seek': 190852, 'start': 1908.52, 'end': 1917.96, 'text': ' I should have done it the other way. Hoping that it will not create a problem, but still. So I just', 'tokens': [50364, 286, 820, 362, 1096, 309, 264, 661, 636, 13, 13438, 278, 300, 309, 486, 406, 1884, 257, 1154, 11, 457, 920, 13, 407, 286, 445, 50836], 'temperature': 0.0, 'avg_logprob': -0.18993596026771947, 'compression_ratio': 1.4818652849740932, 'no_speech_prob': 0.012037074193358421}, {'id': 224, 'seek': 190852, 'start': 1917.96, 'end': 1930.76, 'text': ' kill the definitions which are in this set and then union with gen. If there is self loop and', 'tokens': [50836, 1961, 264, 21988, 597, 366, 294, 341, 992, 293, 550, 11671, 365, 1049, 13, 759, 456, 307, 2698, 6367, 293, 51476], 'temperature': 0.0, 'avg_logprob': -0.18993596026771947, 'compression_ratio': 1.4818652849740932, 'no_speech_prob': 0.012037074193358421}, {'id': 225, 'seek': 190852, 'start': 1930.76, 'end': 1935.76, 'text': ' something I do not want to screw up. This, do you sort of understand what is going on? Okay.', 'tokens': [51476, 746, 286, 360, 406, 528, 281, 5630, 493, 13, 639, 11, 360, 291, 1333, 295, 1223, 437, 307, 516, 322, 30, 1033, 13, 51726], 'temperature': 0.0, 'avg_logprob': -0.18993596026771947, 'compression_ratio': 1.4818652849740932, 'no_speech_prob': 0.012037074193358421}, {'id': 226, 'seek': 193576, 'start': 1935.76, 'end': 1942.64, 'text': ' Now let us come to liveness analysis. So what is liveness analysis? I would like to find out the', 'tokens': [50364, 823, 718, 505, 808, 281, 375, 553, 442, 5215, 13, 407, 437, 307, 375, 553, 442, 5215, 30, 286, 576, 411, 281, 915, 484, 264, 50708], 'temperature': 0.0, 'avg_logprob': -0.20412460126374898, 'compression_ratio': 1.5196850393700787, 'no_speech_prob': 0.005722621455788612}, {'id': 227, 'seek': 193576, 'start': 1942.64, 'end': 1955.68, 'text': ' set of statements which are. So again, how do I do liveness analysis? I want to find, so for any', 'tokens': [50708, 992, 295, 12363, 597, 366, 13, 407, 797, 11, 577, 360, 286, 360, 375, 553, 442, 5215, 30, 286, 528, 281, 915, 11, 370, 337, 604, 51360], 'temperature': 0.0, 'avg_logprob': -0.20412460126374898, 'compression_ratio': 1.5196850393700787, 'no_speech_prob': 0.005722621455788612}, {'id': 228, 'seek': 195568, 'start': 1955.68, 'end': 1965.6000000000001, 'text': ' location. So now let us look at liveness analysis. So again recap what is liveness analysis? So', 'tokens': [50364, 4914, 13, 407, 586, 718, 505, 574, 412, 375, 553, 442, 5215, 13, 407, 797, 20928, 437, 307, 375, 553, 442, 5215, 30, 407, 50860], 'temperature': 0.0, 'avg_logprob': -0.10168472620157096, 'compression_ratio': 1.5447154471544715, 'no_speech_prob': 0.22205431759357452}, {'id': 229, 'seek': 195568, 'start': 1965.6000000000001, 'end': 1973.88, 'text': ' liveness analysis is that at any program point, I would like to find out that what are the set', 'tokens': [50860, 375, 553, 442, 5215, 307, 300, 412, 604, 1461, 935, 11, 286, 576, 411, 281, 915, 484, 300, 437, 366, 264, 992, 51274], 'temperature': 0.0, 'avg_logprob': -0.10168472620157096, 'compression_ratio': 1.5447154471544715, 'no_speech_prob': 0.22205431759357452}, {'id': 230, 'seek': 197388, 'start': 1973.88, 'end': 1992.6000000000001, 'text': ' of variables which have a use in the future. So in this case, what is my universal set? Set of all?', 'tokens': [50364, 295, 9102, 597, 362, 257, 764, 294, 264, 2027, 13, 407, 294, 341, 1389, 11, 437, 307, 452, 11455, 992, 30, 8928, 295, 439, 30, 51300], 'temperature': 0.0, 'avg_logprob': -0.14715211479752152, 'compression_ratio': 1.5203252032520325, 'no_speech_prob': 0.06519554555416107}, {'id': 231, 'seek': 197388, 'start': 1992.6000000000001, 'end': 2002.3600000000001, 'text': ' No, this is liveness analysis. So liveness analysis tells me liveness analysis. It just', 'tokens': [51300, 883, 11, 341, 307, 375, 553, 442, 5215, 13, 407, 375, 553, 442, 5215, 5112, 385, 375, 553, 442, 5215, 13, 467, 445, 51788], 'temperature': 0.0, 'avg_logprob': -0.14715211479752152, 'compression_ratio': 1.5203252032520325, 'no_speech_prob': 0.06519554555416107}, {'id': 232, 'seek': 200236, 'start': 2002.36, 'end': 2010.6999999999998, 'text': ' says what are the variables which are live at a given location. Set of all variables. So this is', 'tokens': [50364, 1619, 437, 366, 264, 9102, 597, 366, 1621, 412, 257, 2212, 4914, 13, 8928, 295, 439, 9102, 13, 407, 341, 307, 50781], 'temperature': 0.0, 'avg_logprob': -0.13258594274520874, 'compression_ratio': 1.5737704918032787, 'no_speech_prob': 0.151421919465065}, {'id': 233, 'seek': 200236, 'start': 2010.6999999999998, 'end': 2022.1999999999998, 'text': ' set of all variables. And what I want to figure out is that which of these variables have a use', 'tokens': [50781, 992, 295, 439, 9102, 13, 400, 437, 286, 528, 281, 2573, 484, 307, 300, 597, 295, 613, 9102, 362, 257, 764, 51356], 'temperature': 0.0, 'avg_logprob': -0.13258594274520874, 'compression_ratio': 1.5737704918032787, 'no_speech_prob': 0.151421919465065}, {'id': 234, 'seek': 202220, 'start': 2022.2, 'end': 2040.68, 'text': ' in the future. How can I do this analysis? How can I set it up? Till the end. Till the end. So it has no future', 'tokens': [50364, 294, 264, 2027, 13, 1012, 393, 286, 360, 341, 5215, 30, 1012, 393, 286, 992, 309, 493, 30, 20227, 264, 917, 13, 20227, 264, 917, 13, 407, 309, 575, 572, 2027, 51288], 'temperature': 0.0, 'avg_logprob': -0.20562191615028988, 'compression_ratio': 1.4758620689655173, 'no_speech_prob': 0.28060197830200195}, {'id': 235, 'seek': 202220, 'start': 2040.68, 'end': 2047.04, 'text': ' use. It means that I can, like for instance, as I said, we can do dead code elimination. Like if there', 'tokens': [51288, 764, 13, 467, 1355, 300, 286, 393, 11, 411, 337, 5197, 11, 382, 286, 848, 11, 321, 393, 360, 3116, 3089, 29224, 13, 1743, 498, 456, 51606], 'temperature': 0.0, 'avg_logprob': -0.20562191615028988, 'compression_ratio': 1.4758620689655173, 'no_speech_prob': 0.28060197830200195}, {'id': 236, 'seek': 204704, 'start': 2047.04, 'end': 2053.12, 'text': ' is a definition of a variable and it has no further use, then that is dead code and I can delete it.', 'tokens': [50364, 307, 257, 7123, 295, 257, 7006, 293, 309, 575, 572, 3052, 764, 11, 550, 300, 307, 3116, 3089, 293, 286, 393, 12097, 309, 13, 50668], 'temperature': 0.0, 'avg_logprob': -0.16603985611273317, 'compression_ratio': 1.6926406926406927, 'no_speech_prob': 0.35952213406562805}, {'id': 237, 'seek': 204704, 'start': 2053.12, 'end': 2062.24, 'text': ' Yeah, so to begin with, we will not care about that I am going to use it in dead code elimination.', 'tokens': [50668, 865, 11, 370, 281, 1841, 365, 11, 321, 486, 406, 1127, 466, 300, 286, 669, 516, 281, 764, 309, 294, 3116, 3089, 29224, 13, 51124], 'temperature': 0.0, 'avg_logprob': -0.16603985611273317, 'compression_ratio': 1.6926406926406927, 'no_speech_prob': 0.35952213406562805}, {'id': 238, 'seek': 204704, 'start': 2062.24, 'end': 2066.16, 'text': ' When we are doing the analysis, we should not worry about what the optimizations. We just ask', 'tokens': [51124, 1133, 321, 366, 884, 264, 5215, 11, 321, 820, 406, 3292, 466, 437, 264, 5028, 14455, 13, 492, 445, 1029, 51320], 'temperature': 0.0, 'avg_logprob': -0.16603985611273317, 'compression_ratio': 1.6926406926406927, 'no_speech_prob': 0.35952213406562805}, {'id': 239, 'seek': 204704, 'start': 2066.16, 'end': 2071.82, 'text': ' a question and we should be able to answer that question. Right now the question is that what are', 'tokens': [51320, 257, 1168, 293, 321, 820, 312, 1075, 281, 1867, 300, 1168, 13, 1779, 586, 264, 1168, 307, 300, 437, 366, 51603], 'temperature': 0.0, 'avg_logprob': -0.16603985611273317, 'compression_ratio': 1.6926406926406927, 'no_speech_prob': 0.35952213406562805}, {'id': 240, 'seek': 207182, 'start': 2071.82, 'end': 2077.98, 'text': ' the variables which are live from this location. Yes, I want to find out what variables like,', 'tokens': [50364, 264, 9102, 597, 366, 1621, 490, 341, 4914, 13, 1079, 11, 286, 528, 281, 915, 484, 437, 9102, 411, 11, 50672], 'temperature': 0.0, 'avg_logprob': -0.2496544671437097, 'compression_ratio': 1.8205128205128205, 'no_speech_prob': 0.2745496928691864}, {'id': 241, 'seek': 207182, 'start': 2077.98, 'end': 2086.4, 'text': ' what are the set of variables which have a future use. See available expressions. Come to available', 'tokens': [50672, 437, 366, 264, 992, 295, 9102, 597, 362, 257, 2027, 764, 13, 3008, 2435, 15277, 13, 2492, 281, 2435, 51093], 'temperature': 0.0, 'avg_logprob': -0.2496544671437097, 'compression_ratio': 1.8205128205128205, 'no_speech_prob': 0.2745496928691864}, {'id': 242, 'seek': 207182, 'start': 2086.4, 'end': 2095.6200000000003, 'text': ' expressions or, okay. So let us say available expressions. What does available expressions', 'tokens': [51093, 15277, 420, 11, 1392, 13, 407, 718, 505, 584, 2435, 15277, 13, 708, 775, 2435, 15277, 51554], 'temperature': 0.0, 'avg_logprob': -0.2496544671437097, 'compression_ratio': 1.8205128205128205, 'no_speech_prob': 0.2745496928691864}, {'id': 243, 'seek': 209562, 'start': 2095.62, 'end': 2101.7999999999997, 'text': ' compute? Available expression computes. I will say an expression is available at this location', 'tokens': [50364, 14722, 30, 11667, 32699, 6114, 715, 1819, 13, 286, 486, 584, 364, 6114, 307, 2435, 412, 341, 4914, 50673], 'temperature': 0.0, 'avg_logprob': -0.14627930474659753, 'compression_ratio': 1.7961783439490446, 'no_speech_prob': 0.3786075711250305}, {'id': 244, 'seek': 209562, 'start': 2101.7999999999997, 'end': 2111.3399999999997, 'text': ' if that expression has been computed along all paths reaching this location. So what is the', 'tokens': [50673, 498, 300, 6114, 575, 668, 40610, 2051, 439, 14518, 9906, 341, 4914, 13, 407, 437, 307, 264, 51150], 'temperature': 0.0, 'avg_logprob': -0.14627930474659753, 'compression_ratio': 1.7961783439490446, 'no_speech_prob': 0.3786075711250305}, {'id': 245, 'seek': 209562, 'start': 2111.3399999999997, 'end': 2122.74, 'text': ' universal set in case of available expressions? All the set of expressions. Here it is a set of', 'tokens': [51150, 11455, 992, 294, 1389, 295, 2435, 15277, 30, 1057, 264, 992, 295, 15277, 13, 1692, 309, 307, 257, 992, 295, 51720], 'temperature': 0.0, 'avg_logprob': -0.14627930474659753, 'compression_ratio': 1.7961783439490446, 'no_speech_prob': 0.3786075711250305}, {'id': 246, 'seek': 212274, 'start': 2122.74, 'end': 2128.2599999999998, 'text': ' all variables. But you do not care about what expression it is. It has been used,', 'tokens': [50364, 439, 9102, 13, 583, 291, 360, 406, 1127, 466, 437, 6114, 309, 307, 13, 467, 575, 668, 1143, 11, 50640], 'temperature': 0.0, 'avg_logprob': -0.17467184364795685, 'compression_ratio': 1.8392156862745097, 'no_speech_prob': 0.7972719073295593}, {'id': 247, 'seek': 212274, 'start': 2128.2599999999998, 'end': 2132.9399999999996, 'text': ' it has been used, that is it. It has been used in A plus B or used in A minus B or A star B.', 'tokens': [50640, 309, 575, 668, 1143, 11, 300, 307, 309, 13, 467, 575, 668, 1143, 294, 316, 1804, 363, 420, 1143, 294, 316, 3175, 363, 420, 316, 3543, 363, 13, 50874], 'temperature': 0.0, 'avg_logprob': -0.17467184364795685, 'compression_ratio': 1.8392156862745097, 'no_speech_prob': 0.7972719073295593}, {'id': 248, 'seek': 212274, 'start': 2132.9399999999996, 'end': 2138.02, 'text': ' How does it matter? It has been used, that is all we care about. Okay, so now think in a slightly', 'tokens': [50874, 1012, 775, 309, 1871, 30, 467, 575, 668, 1143, 11, 300, 307, 439, 321, 1127, 466, 13, 1033, 11, 370, 586, 519, 294, 257, 4748, 51128], 'temperature': 0.0, 'avg_logprob': -0.17467184364795685, 'compression_ratio': 1.8392156862745097, 'no_speech_prob': 0.7972719073295593}, {'id': 249, 'seek': 212274, 'start': 2138.02, 'end': 2142.9399999999996, 'text': ' different manner. Think in a similar way we talked about this token business. We generate a token and', 'tokens': [51128, 819, 9060, 13, 6557, 294, 257, 2531, 636, 321, 2825, 466, 341, 14862, 1606, 13, 492, 8460, 257, 14862, 293, 51374], 'temperature': 0.0, 'avg_logprob': -0.17467184364795685, 'compression_ratio': 1.8392156862745097, 'no_speech_prob': 0.7972719073295593}, {'id': 250, 'seek': 212274, 'start': 2142.9399999999996, 'end': 2150.2599999999998, 'text': ' try to flow it. So where will I generate a token and how will I flow it? Excellent, very nice,', 'tokens': [51374, 853, 281, 3095, 309, 13, 407, 689, 486, 286, 8460, 257, 14862, 293, 577, 486, 286, 3095, 309, 30, 16723, 11, 588, 1481, 11, 51740], 'temperature': 0.0, 'avg_logprob': -0.17467184364795685, 'compression_ratio': 1.8392156862745097, 'no_speech_prob': 0.7972719073295593}, {'id': 251, 'seek': 215026, 'start': 2150.26, 'end': 2156.0600000000004, 'text': ' very very nice. So okay, now we have an algorithm. So the algorithm is that we,', 'tokens': [50364, 588, 588, 1481, 13, 407, 1392, 11, 586, 321, 362, 364, 9284, 13, 407, 264, 9284, 307, 300, 321, 11, 50654], 'temperature': 0.0, 'avg_logprob': -0.1795824740795379, 'compression_ratio': 1.7536231884057971, 'no_speech_prob': 0.010943884029984474}, {'id': 252, 'seek': 215026, 'start': 2156.0600000000004, 'end': 2166.26, 'text': ' so what is the set of the set of live variables at the exit to the program? Nothing, empty set,', 'tokens': [50654, 370, 437, 307, 264, 992, 295, 264, 992, 295, 1621, 9102, 412, 264, 11043, 281, 264, 1461, 30, 6693, 11, 6707, 992, 11, 51164], 'temperature': 0.0, 'avg_logprob': -0.1795824740795379, 'compression_ratio': 1.7536231884057971, 'no_speech_prob': 0.010943884029984474}, {'id': 253, 'seek': 215026, 'start': 2166.26, 'end': 2171.98, 'text': ' because well I am done with the program so nothing is going to be used anymore. And then', 'tokens': [51164, 570, 731, 286, 669, 1096, 365, 264, 1461, 370, 1825, 307, 516, 281, 312, 1143, 3602, 13, 400, 550, 51450], 'temperature': 0.0, 'avg_logprob': -0.1795824740795379, 'compression_ratio': 1.7536231884057971, 'no_speech_prob': 0.010943884029984474}, {'id': 254, 'seek': 215026, 'start': 2171.98, 'end': 2179.34, 'text': ' what I can do is for the statement before it, for the statement before it, how can I construct the', 'tokens': [51450, 437, 286, 393, 360, 307, 337, 264, 5629, 949, 309, 11, 337, 264, 5629, 949, 309, 11, 577, 393, 286, 7690, 264, 51818], 'temperature': 0.0, 'avg_logprob': -0.1795824740795379, 'compression_ratio': 1.7536231884057971, 'no_speech_prob': 0.010943884029984474}, {'id': 255, 'seek': 217934, 'start': 2179.82, 'end': 2186.9, 'text': ' set of available, set of live variables? So let us say I have a statement of the form A equals B', 'tokens': [50388, 992, 295, 2435, 11, 992, 295, 1621, 9102, 30, 407, 718, 505, 584, 286, 362, 257, 5629, 295, 264, 1254, 316, 6915, 363, 50742], 'temperature': 0.0, 'avg_logprob': -0.17136311848958333, 'compression_ratio': 1.532967032967033, 'no_speech_prob': 0.007335858419537544}, {'id': 256, 'seek': 217934, 'start': 2186.9, 'end': 2199.26, 'text': ' plus C. V and C are used, very good. So B and C are used, so these will get added to the set or', 'tokens': [50742, 1804, 383, 13, 691, 293, 383, 366, 1143, 11, 588, 665, 13, 407, 363, 293, 383, 366, 1143, 11, 370, 613, 486, 483, 3869, 281, 264, 992, 420, 51360], 'temperature': 0.0, 'avg_logprob': -0.17136311848958333, 'compression_ratio': 1.532967032967033, 'no_speech_prob': 0.007335858419537544}, {'id': 257, 'seek': 217934, 'start': 2199.26, 'end': 2207.2200000000003, 'text': ' these are generated when we go through this particular statement. And anything else? A', 'tokens': [51360, 613, 366, 10833, 562, 321, 352, 807, 341, 1729, 5629, 13, 400, 1340, 1646, 30, 316, 51758], 'temperature': 0.0, 'avg_logprob': -0.17136311848958333, 'compression_ratio': 1.532967032967033, 'no_speech_prob': 0.007335858419537544}, {'id': 258, 'seek': 220722, 'start': 2208.22, 'end': 2214.3799999999997, 'text': ' was not there, yeah, that is okay, but I want to like now let us say that okay, I should not', 'tokens': [50414, 390, 406, 456, 11, 1338, 11, 300, 307, 1392, 11, 457, 286, 528, 281, 411, 586, 718, 505, 584, 300, 1392, 11, 286, 820, 406, 50722], 'temperature': 0.0, 'avg_logprob': -0.2029104609238474, 'compression_ratio': 1.8181818181818181, 'no_speech_prob': 0.029178038239479065}, {'id': 259, 'seek': 220722, 'start': 2214.3799999999997, 'end': 2222.4599999999996, 'text': ' confuse you. So let us say I know what happens at exit. Now let us say I have the set of live', 'tokens': [50722, 28584, 291, 13, 407, 718, 505, 584, 286, 458, 437, 2314, 412, 11043, 13, 823, 718, 505, 584, 286, 362, 264, 992, 295, 1621, 51126], 'temperature': 0.0, 'avg_logprob': -0.2029104609238474, 'compression_ratio': 1.8181818181818181, 'no_speech_prob': 0.029178038239479065}, {'id': 260, 'seek': 220722, 'start': 2222.4599999999996, 'end': 2229.62, 'text': ' variables at this location and I want to find out the set of live variables at this location.', 'tokens': [51126, 9102, 412, 341, 4914, 293, 286, 528, 281, 915, 484, 264, 992, 295, 1621, 9102, 412, 341, 4914, 13, 51484], 'temperature': 0.0, 'avg_logprob': -0.2029104609238474, 'compression_ratio': 1.8181818181818181, 'no_speech_prob': 0.029178038239479065}, {'id': 261, 'seek': 222962, 'start': 2229.62, 'end': 2237.5, 'text': ' And this is a statement A equals B plus C. I have to kill the use of A, why is that? Because', 'tokens': [50364, 400, 341, 307, 257, 5629, 316, 6915, 363, 1804, 383, 13, 286, 362, 281, 1961, 264, 764, 295, 316, 11, 983, 307, 300, 30, 1436, 50758], 'temperature': 0.0, 'avg_logprob': -0.12269494268629286, 'compression_ratio': 1.701834862385321, 'no_speech_prob': 0.02091974951326847}, {'id': 262, 'seek': 222962, 'start': 2237.5, 'end': 2244.66, 'text': ' think about it, so if you have A equals 1 and again I say A equals 2, the value that got set', 'tokens': [50758, 519, 466, 309, 11, 370, 498, 291, 362, 316, 6915, 502, 293, 797, 286, 584, 316, 6915, 568, 11, 264, 2158, 300, 658, 992, 51116], 'temperature': 0.0, 'avg_logprob': -0.12269494268629286, 'compression_ratio': 1.701834862385321, 'no_speech_prob': 0.02091974951326847}, {'id': 263, 'seek': 222962, 'start': 2244.66, 'end': 2251.7, 'text': ' in this A equals 1 has no use because immediately I set A to 2. So this is as good as any other', 'tokens': [51116, 294, 341, 316, 6915, 502, 575, 572, 764, 570, 4258, 286, 992, 316, 281, 568, 13, 407, 341, 307, 382, 665, 382, 604, 661, 51468], 'temperature': 0.0, 'avg_logprob': -0.12269494268629286, 'compression_ratio': 1.701834862385321, 'no_speech_prob': 0.02091974951326847}, {'id': 264, 'seek': 222962, 'start': 2251.7, 'end': 2257.9, 'text': ' place, any other thing, it is as good as not setting it at all. So if I redefine a value,', 'tokens': [51468, 1081, 11, 604, 661, 551, 11, 309, 307, 382, 665, 382, 406, 3287, 309, 412, 439, 13, 407, 498, 286, 38818, 533, 257, 2158, 11, 51778], 'temperature': 0.0, 'avg_logprob': -0.12269494268629286, 'compression_ratio': 1.701834862385321, 'no_speech_prob': 0.02091974951326847}, {'id': 265, 'seek': 225790, 'start': 2257.9, 'end': 2266.2200000000003, 'text': ' if I redefine a variable, then the old value is lost. So it is no point computing that old value.', 'tokens': [50364, 498, 286, 38818, 533, 257, 7006, 11, 550, 264, 1331, 2158, 307, 2731, 13, 407, 309, 307, 572, 935, 15866, 300, 1331, 2158, 13, 50780], 'temperature': 0.0, 'avg_logprob': -0.20138063822707084, 'compression_ratio': 1.6140350877192982, 'no_speech_prob': 0.004748100880533457}, {'id': 266, 'seek': 225790, 'start': 2266.2200000000003, 'end': 2271.3, 'text': ' So if there was a statement which let us say instead of 1 it was an expensive statement', 'tokens': [50780, 407, 498, 456, 390, 257, 5629, 597, 718, 505, 584, 2602, 295, 502, 309, 390, 364, 5124, 5629, 51034], 'temperature': 0.0, 'avg_logprob': -0.20138063822707084, 'compression_ratio': 1.6140350877192982, 'no_speech_prob': 0.004748100880533457}, {'id': 267, 'seek': 225790, 'start': 2271.3, 'end': 2278.3, 'text': ' foo, that some function was computed and it computed some foo and after that I set A to 2,', 'tokens': [51034, 726, 78, 11, 300, 512, 2445, 390, 40610, 293, 309, 40610, 512, 726, 78, 293, 934, 300, 286, 992, 316, 281, 568, 11, 51384], 'temperature': 0.0, 'avg_logprob': -0.20138063822707084, 'compression_ratio': 1.6140350877192982, 'no_speech_prob': 0.004748100880533457}, {'id': 268, 'seek': 227830, 'start': 2278.3, 'end': 2287.78, 'text': ' what is the point of doing A equal 2? Make sense? So whenever I get a definition,', 'tokens': [50364, 437, 307, 264, 935, 295, 884, 316, 2681, 568, 30, 4387, 2020, 30, 407, 5699, 286, 483, 257, 7123, 11, 50838], 'temperature': 0.0, 'avg_logprob': -0.19323184524757275, 'compression_ratio': 1.6198830409356726, 'no_speech_prob': 0.01196652464568615}, {'id': 269, 'seek': 227830, 'start': 2287.78, 'end': 2299.9, 'text': ' I would kill the definition A, whatever has been is being redefined. So what is my generated sets?', 'tokens': [50838, 286, 576, 1961, 264, 7123, 316, 11, 2035, 575, 668, 307, 885, 38818, 2001, 13, 407, 437, 307, 452, 10833, 6352, 30, 51444], 'temperature': 0.0, 'avg_logprob': -0.19323184524757275, 'compression_ratio': 1.6198830409356726, 'no_speech_prob': 0.01196652464568615}, {'id': 270, 'seek': 227830, 'start': 2299.9, 'end': 2305.1000000000004, 'text': ' The whatever is being used in the expression in the statement are the generated set, what is the', 'tokens': [51444, 440, 2035, 307, 885, 1143, 294, 264, 6114, 294, 264, 5629, 366, 264, 10833, 992, 11, 437, 307, 264, 51704], 'temperature': 0.0, 'avg_logprob': -0.19323184524757275, 'compression_ratio': 1.6198830409356726, 'no_speech_prob': 0.01196652464568615}, {'id': 271, 'seek': 230510, 'start': 2305.1, 'end': 2313.42, 'text': ' kill set? Whatever is being defined is going to be the kill set. So again this analysis,', 'tokens': [50364, 1961, 992, 30, 8541, 307, 885, 7642, 307, 516, 281, 312, 264, 1961, 992, 13, 407, 797, 341, 5215, 11, 50780], 'temperature': 0.0, 'avg_logprob': -0.13148941463894315, 'compression_ratio': 1.3692307692307693, 'no_speech_prob': 0.10339966416358948}, {'id': 272, 'seek': 230510, 'start': 2313.42, 'end': 2322.42, 'text': ' how does it flow? It flows backwards. I have defined how to generate and kill statements.', 'tokens': [50780, 577, 775, 309, 3095, 30, 467, 12867, 12204, 13, 286, 362, 7642, 577, 281, 8460, 293, 1961, 12363, 13, 51230], 'temperature': 0.0, 'avg_logprob': -0.13148941463894315, 'compression_ratio': 1.3692307692307693, 'no_speech_prob': 0.10339966416358948}, {'id': 273, 'seek': 232242, 'start': 2322.42, 'end': 2338.42, 'text': ' What about if you hit a statement like this? So if particular statement has multiple predecessors,', 'tokens': [50364, 708, 466, 498, 291, 2045, 257, 5629, 411, 341, 30, 407, 498, 1729, 5629, 575, 3866, 24874, 45700, 11, 51164], 'temperature': 0.0, 'avg_logprob': -0.12308018103889797, 'compression_ratio': 1.375, 'no_speech_prob': 0.017101218923926353}, {'id': 274, 'seek': 232242, 'start': 2338.42, 'end': 2350.5, 'text': ' then what do you do? You have propagated to both of them and if the other thing happens,', 'tokens': [51164, 550, 437, 360, 291, 360, 30, 509, 362, 12425, 770, 281, 1293, 295, 552, 293, 498, 264, 661, 551, 2314, 11, 51768], 'temperature': 0.0, 'avg_logprob': -0.12308018103889797, 'compression_ratio': 1.375, 'no_speech_prob': 0.017101218923926353}, {'id': 275, 'seek': 235050, 'start': 2350.5, 'end': 2369.74, 'text': ' that multiple statements have the same predecessor, then what should I do? Union,', 'tokens': [50364, 300, 3866, 12363, 362, 264, 912, 34991, 11, 550, 437, 820, 286, 360, 30, 8133, 11, 51326], 'temperature': 0.0, 'avg_logprob': -0.19498347073066524, 'compression_ratio': 1.4, 'no_speech_prob': 0.02098744548857212}, {'id': 276, 'seek': 235050, 'start': 2369.74, 'end': 2377.02, 'text': ' because what is the definition? The definition is if it is used in any path going through the', 'tokens': [51326, 570, 437, 307, 264, 7123, 30, 440, 7123, 307, 498, 309, 307, 1143, 294, 604, 3100, 516, 807, 264, 51690], 'temperature': 0.0, 'avg_logprob': -0.19498347073066524, 'compression_ratio': 1.4, 'no_speech_prob': 0.02098744548857212}, {'id': 277, 'seek': 237702, 'start': 2377.02, 'end': 2388.74, 'text': ' from that location. So this is union, available expression. So what is my universal set? You', 'tokens': [50364, 490, 300, 4914, 13, 407, 341, 307, 11671, 11, 2435, 6114, 13, 407, 437, 307, 452, 11455, 992, 30, 509, 50950], 'temperature': 0.0, 'avg_logprob': -0.2999133454992416, 'compression_ratio': 1.3795620437956204, 'no_speech_prob': 0.48290205001831055}, {'id': 278, 'seek': 237702, 'start': 2388.74, 'end': 2393.54, 'text': ' guys told me that instead of all expressions in the formula. Now the same business if I ask you,', 'tokens': [50950, 1074, 1907, 385, 300, 2602, 295, 439, 15277, 294, 264, 8513, 13, 823, 264, 912, 1606, 498, 286, 1029, 291, 11, 51190], 'temperature': 0.0, 'avg_logprob': -0.2999133454992416, 'compression_ratio': 1.3795620437956204, 'no_speech_prob': 0.48290205001831055}, {'id': 279, 'seek': 239354, 'start': 2393.54, 'end': 2407.2599999999998, 'text': ' if you have a statement A equals B plus C, how will you handle it? So where do you know', 'tokens': [50364, 498, 291, 362, 257, 5629, 316, 6915, 363, 1804, 383, 11, 577, 486, 291, 4813, 309, 30, 407, 689, 360, 291, 458, 51050], 'temperature': 0.0, 'avg_logprob': -0.20169859221487335, 'compression_ratio': 1.6144578313253013, 'no_speech_prob': 0.01044715940952301}, {'id': 280, 'seek': 239354, 'start': 2407.2599999999998, 'end': 2417.62, 'text': ' the set of available expressions? Where is it known already? What is the set of available', 'tokens': [51050, 264, 992, 295, 2435, 15277, 30, 2305, 307, 309, 2570, 1217, 30, 708, 307, 264, 992, 295, 2435, 51568], 'temperature': 0.0, 'avg_logprob': -0.20169859221487335, 'compression_ratio': 1.6144578313253013, 'no_speech_prob': 0.01044715940952301}, {'id': 281, 'seek': 239354, 'start': 2417.62, 'end': 2421.5, 'text': ' expressions? Like for instance, like variables you could say that the exit of the program,', 'tokens': [51568, 15277, 30, 1743, 337, 5197, 11, 411, 9102, 291, 727, 584, 300, 264, 11043, 295, 264, 1461, 11, 51762], 'temperature': 0.0, 'avg_logprob': -0.20169859221487335, 'compression_ratio': 1.6144578313253013, 'no_speech_prob': 0.01044715940952301}, {'id': 282, 'seek': 242150, 'start': 2421.5, 'end': 2426.22, 'text': ' nothing is live because nothing gets used after that. You really nothing gets used because I just', 'tokens': [50364, 1825, 307, 1621, 570, 1825, 2170, 1143, 934, 300, 13, 509, 534, 1825, 2170, 1143, 570, 286, 445, 50600], 'temperature': 0.0, 'avg_logprob': -0.32525549994574654, 'compression_ratio': 1.690909090909091, 'no_speech_prob': 0.020709987729787827}, {'id': 283, 'seek': 242150, 'start': 2426.22, 'end': 2434.34, 'text': ' have never. Here do you see where is it? Which location you already know the set of available', 'tokens': [50600, 362, 1128, 13, 1692, 360, 291, 536, 689, 307, 309, 30, 3013, 4914, 291, 1217, 458, 264, 992, 295, 2435, 51006], 'temperature': 0.0, 'avg_logprob': -0.32525549994574654, 'compression_ratio': 1.690909090909091, 'no_speech_prob': 0.020709987729787827}, {'id': 284, 'seek': 242150, 'start': 2434.34, 'end': 2440.9, 'text': ' expressions? So now can you think of this analysis? How will this go? Okay, good, good,', 'tokens': [51006, 15277, 30, 407, 586, 393, 291, 519, 295, 341, 5215, 30, 1012, 486, 341, 352, 30, 1033, 11, 665, 11, 665, 11, 51334], 'temperature': 0.0, 'avg_logprob': -0.32525549994574654, 'compression_ratio': 1.690909090909091, 'no_speech_prob': 0.020709987729787827}, {'id': 285, 'seek': 242150, 'start': 2440.9, 'end': 2446.26, 'text': ' good, good, good, good. Intersection wise that because it should be coming from all possible', 'tokens': [51334, 665, 11, 665, 11, 665, 11, 665, 13, 5751, 11963, 10829, 300, 570, 309, 820, 312, 1348, 490, 439, 1944, 51602], 'temperature': 0.0, 'avg_logprob': -0.32525549994574654, 'compression_ratio': 1.690909090909091, 'no_speech_prob': 0.020709987729787827}, {'id': 286, 'seek': 244626, 'start': 2446.26, 'end': 2451.38, 'text': ' paths. So I will take an intersection ensure that available means it is surely available.', 'tokens': [50364, 14518, 13, 407, 286, 486, 747, 364, 15236, 5586, 300, 2435, 1355, 309, 307, 11468, 2435, 13, 50620], 'temperature': 0.0, 'avg_logprob': -0.21085246916740172, 'compression_ratio': 1.6504424778761062, 'no_speech_prob': 0.5594863891601562}, {'id': 287, 'seek': 244626, 'start': 2451.38, 'end': 2455.92, 'text': ' I am surely computed it. So I should not miss out on any path through which it has not been', 'tokens': [50620, 286, 669, 11468, 40610, 309, 13, 407, 286, 820, 406, 1713, 484, 322, 604, 3100, 807, 597, 309, 575, 406, 668, 50847], 'temperature': 0.0, 'avg_logprob': -0.21085246916740172, 'compression_ratio': 1.6504424778761062, 'no_speech_prob': 0.5594863891601562}, {'id': 288, 'seek': 244626, 'start': 2455.92, 'end': 2463.9, 'text': ' computed. So I take a intersection over all paths and the flow is forward or backward? Forward.', 'tokens': [50847, 40610, 13, 407, 286, 747, 257, 15236, 670, 439, 14518, 293, 264, 3095, 307, 2128, 420, 23897, 30, 35524, 13, 51246], 'temperature': 0.0, 'avg_logprob': -0.21085246916740172, 'compression_ratio': 1.6504424778761062, 'no_speech_prob': 0.5594863891601562}, {'id': 289, 'seek': 244626, 'start': 2463.9, 'end': 2470.26, 'text': ' And how do I do this particular thing? A equals B plus C. So how what is my how do I generate a', 'tokens': [51246, 400, 577, 360, 286, 360, 341, 1729, 551, 30, 316, 6915, 363, 1804, 383, 13, 407, 577, 437, 307, 452, 577, 360, 286, 8460, 257, 51564], 'temperature': 0.0, 'avg_logprob': -0.21085246916740172, 'compression_ratio': 1.6504424778761062, 'no_speech_prob': 0.5594863891601562}, {'id': 290, 'seek': 247026, 'start': 2470.26, 'end': 2477.46, 'text': ' statement? How do I generate a data flow and how do I kill a data flow? So what is the statement', 'tokens': [50364, 5629, 30, 1012, 360, 286, 8460, 257, 1412, 3095, 293, 577, 360, 286, 1961, 257, 1412, 3095, 30, 407, 437, 307, 264, 5629, 50724], 'temperature': 0.0, 'avg_logprob': -0.15447176419771635, 'compression_ratio': 1.886138613861386, 'no_speech_prob': 0.25596025586128235}, {'id': 291, 'seek': 247026, 'start': 2477.46, 'end': 2482.78, 'text': ' when I do A equals B plus C? Let us say I know what is the set of available expressions at N1.', 'tokens': [50724, 562, 286, 360, 316, 6915, 363, 1804, 383, 30, 961, 505, 584, 286, 458, 437, 307, 264, 992, 295, 2435, 15277, 412, 426, 16, 13, 50990], 'temperature': 0.0, 'avg_logprob': -0.15447176419771635, 'compression_ratio': 1.886138613861386, 'no_speech_prob': 0.25596025586128235}, {'id': 292, 'seek': 247026, 'start': 2482.78, 'end': 2489.7000000000003, 'text': ' How do I get available expressions at N2? How do I do that? So what is what gets generated here?', 'tokens': [50990, 1012, 360, 286, 483, 2435, 15277, 412, 426, 17, 30, 1012, 360, 286, 360, 300, 30, 407, 437, 307, 437, 2170, 10833, 510, 30, 51336], 'temperature': 0.0, 'avg_logprob': -0.15447176419771635, 'compression_ratio': 1.886138613861386, 'no_speech_prob': 0.25596025586128235}, {'id': 293, 'seek': 247026, 'start': 2489.7000000000003, 'end': 2498.34, 'text': ' Yeah, the expression B plus C gets generated here. What gets killed? Not uses. Right. So all', 'tokens': [51336, 865, 11, 264, 6114, 363, 1804, 383, 2170, 10833, 510, 13, 708, 2170, 4652, 30, 1726, 4960, 13, 1779, 13, 407, 439, 51768], 'temperature': 0.0, 'avg_logprob': -0.15447176419771635, 'compression_ratio': 1.886138613861386, 'no_speech_prob': 0.25596025586128235}, {'id': 294, 'seek': 249834, 'start': 2498.34, 'end': 2503.58, 'text': ' previous expressions, whichever expressions were there, which contained A, those are killed', 'tokens': [50364, 3894, 15277, 11, 24123, 15277, 645, 456, 11, 597, 16212, 316, 11, 729, 366, 4652, 50626], 'temperature': 0.0, 'avg_logprob': -0.15730616142009868, 'compression_ratio': 1.991701244813278, 'no_speech_prob': 0.07793168723583221}, {'id': 295, 'seek': 249834, 'start': 2503.58, 'end': 2507.7000000000003, 'text': ' because they are no more available. I computed them with some value of A, but the value of A', 'tokens': [50626, 570, 436, 366, 572, 544, 2435, 13, 286, 40610, 552, 365, 512, 2158, 295, 316, 11, 457, 264, 2158, 295, 316, 50832], 'temperature': 0.0, 'avg_logprob': -0.15730616142009868, 'compression_ratio': 1.991701244813278, 'no_speech_prob': 0.07793168723583221}, {'id': 296, 'seek': 249834, 'start': 2507.7000000000003, 'end': 2511.94, 'text': ' has changed now. So it is not the same value that I computed them with. So I cannot use that value', 'tokens': [50832, 575, 3105, 586, 13, 407, 309, 307, 406, 264, 912, 2158, 300, 286, 40610, 552, 365, 13, 407, 286, 2644, 764, 300, 2158, 51044], 'temperature': 0.0, 'avg_logprob': -0.15730616142009868, 'compression_ratio': 1.991701244813278, 'no_speech_prob': 0.07793168723583221}, {'id': 297, 'seek': 249834, 'start': 2511.94, 'end': 2516.2200000000003, 'text': ' anymore. Remember what we wanted to do? We wanted to store it in a temporary and use that value,', 'tokens': [51044, 3602, 13, 5459, 437, 321, 1415, 281, 360, 30, 492, 1415, 281, 3531, 309, 294, 257, 13413, 293, 764, 300, 2158, 11, 51258], 'temperature': 0.0, 'avg_logprob': -0.15730616142009868, 'compression_ratio': 1.991701244813278, 'no_speech_prob': 0.07793168723583221}, {'id': 298, 'seek': 249834, 'start': 2516.2200000000003, 'end': 2521.06, 'text': ' but it was computed with the whole value of A. Now the value of A has changed. So that value cannot', 'tokens': [51258, 457, 309, 390, 40610, 365, 264, 1379, 2158, 295, 316, 13, 823, 264, 2158, 295, 316, 575, 3105, 13, 407, 300, 2158, 2644, 51500], 'temperature': 0.0, 'avg_logprob': -0.15730616142009868, 'compression_ratio': 1.991701244813278, 'no_speech_prob': 0.07793168723583221}, {'id': 299, 'seek': 252106, 'start': 2521.06, 'end': 2529.5, 'text': ' be used again. So my kill will be all expressions with A containing A. All of them get killed.', 'tokens': [50364, 312, 1143, 797, 13, 407, 452, 1961, 486, 312, 439, 15277, 365, 316, 19273, 316, 13, 1057, 295, 552, 483, 4652, 13, 50786], 'temperature': 0.0, 'avg_logprob': -0.18989692909130151, 'compression_ratio': 1.6571428571428573, 'no_speech_prob': 0.1914767324924469}, {'id': 300, 'seek': 252106, 'start': 2532.58, 'end': 2540.7, 'text': ' So again, this is a forward analysis. This I flow the tokens forward. The generating set is the set', 'tokens': [50940, 407, 797, 11, 341, 307, 257, 2128, 5215, 13, 639, 286, 3095, 264, 22667, 2128, 13, 440, 17746, 992, 307, 264, 992, 51346], 'temperature': 0.0, 'avg_logprob': -0.18989692909130151, 'compression_ratio': 1.6571428571428573, 'no_speech_prob': 0.1914767324924469}, {'id': 301, 'seek': 252106, 'start': 2540.7, 'end': 2548.98, 'text': ' of all the expression that gets computed at that state and kill are all expressions assigned to', 'tokens': [51346, 295, 439, 264, 6114, 300, 2170, 40610, 412, 300, 1785, 293, 1961, 366, 439, 15277, 13279, 281, 51760], 'temperature': 0.0, 'avg_logprob': -0.18989692909130151, 'compression_ratio': 1.6571428571428573, 'no_speech_prob': 0.1914767324924469}, {'id': 302, 'seek': 254898, 'start': 2548.98, 'end': 2555.34, 'text': ' any or that variable is assigning. Whatever that variable is assigning to, all expressions with', 'tokens': [50364, 604, 420, 300, 7006, 307, 49602, 13, 8541, 300, 7006, 307, 49602, 281, 11, 439, 15277, 365, 50682], 'temperature': 0.0, 'avg_logprob': -0.3039788870975889, 'compression_ratio': 1.8838383838383839, 'no_speech_prob': 0.025912461802363396}, {'id': 303, 'seek': 254898, 'start': 2555.34, 'end': 2561.18, 'text': ' that particular variable participating gets killed, gets removed from the set. No definitions. Here', 'tokens': [50682, 300, 1729, 7006, 13950, 2170, 4652, 11, 2170, 7261, 490, 264, 992, 13, 883, 21988, 13, 1692, 50974], 'temperature': 0.0, 'avg_logprob': -0.3039788870975889, 'compression_ratio': 1.8838383838383839, 'no_speech_prob': 0.025912461802363396}, {'id': 304, 'seek': 254898, 'start': 2561.18, 'end': 2566.46, 'text': ' is the set of expressions. Now the data flow facts are expressions, set of expressions.', 'tokens': [50974, 307, 264, 992, 295, 15277, 13, 823, 264, 1412, 3095, 9130, 366, 15277, 11, 992, 295, 15277, 13, 51238], 'temperature': 0.0, 'avg_logprob': -0.3039788870975889, 'compression_ratio': 1.8838383838383839, 'no_speech_prob': 0.025912461802363396}, {'id': 305, 'seek': 254898, 'start': 2566.46, 'end': 2572.26, 'text': ' No, no, no. So again, let us go at the, so whenever you have an expression, like you have', 'tokens': [51238, 883, 11, 572, 11, 572, 13, 407, 797, 11, 718, 505, 352, 412, 264, 11, 370, 5699, 291, 362, 364, 6114, 11, 411, 291, 362, 51528], 'temperature': 0.0, 'avg_logprob': -0.3039788870975889, 'compression_ratio': 1.8838383838383839, 'no_speech_prob': 0.025912461802363396}, {'id': 306, 'seek': 257226, 'start': 2572.26, 'end': 2581.7000000000003, 'text': ' z equals A plus B, then this is a definition of z and these are the uses of A. So whatever', 'tokens': [50364, 710, 6915, 316, 1804, 363, 11, 550, 341, 307, 257, 7123, 295, 710, 293, 613, 366, 264, 4960, 295, 316, 13, 407, 2035, 50836], 'temperature': 0.0, 'avg_logprob': -0.19217522079880173, 'compression_ratio': 1.5754189944134078, 'no_speech_prob': 0.42779961228370667}, {'id': 307, 'seek': 257226, 'start': 2581.7000000000003, 'end': 2592.5400000000004, 'text': ' variables are used in that expression gets killed. All variables which are, okay, so maybe let us', 'tokens': [50836, 9102, 366, 1143, 294, 300, 6114, 2170, 4652, 13, 1057, 9102, 597, 366, 11, 1392, 11, 370, 1310, 718, 505, 51378], 'temperature': 0.0, 'avg_logprob': -0.19217522079880173, 'compression_ratio': 1.5754189944134078, 'no_speech_prob': 0.42779961228370667}, {'id': 308, 'seek': 257226, 'start': 2592.5400000000004, 'end': 2601.7000000000003, 'text': ' use this term, like A equals B plus C. So A is being defined. So all expressions which have A', 'tokens': [51378, 764, 341, 1433, 11, 411, 316, 6915, 363, 1804, 383, 13, 407, 316, 307, 885, 7642, 13, 407, 439, 15277, 597, 362, 316, 51836], 'temperature': 0.0, 'avg_logprob': -0.19217522079880173, 'compression_ratio': 1.5754189944134078, 'no_speech_prob': 0.42779961228370667}, {'id': 309, 'seek': 260170, 'start': 2601.7, 'end': 2613.74, 'text': ' as a used variable, as A where A is being used will get killed. And is it, or what we do at,', 'tokens': [50364, 382, 257, 1143, 7006, 11, 382, 316, 689, 316, 307, 885, 1143, 486, 483, 4652, 13, 400, 307, 309, 11, 420, 437, 321, 360, 412, 11, 50966], 'temperature': 0.0, 'avg_logprob': -0.28349736272072307, 'compression_ratio': 1.393939393939394, 'no_speech_prob': 0.010194133035838604}, {'id': 310, 'seek': 260170, 'start': 2613.74, 'end': 2622.06, 'text': ' like wherever I need to combine multiple data flow facts when I have multiple predecessors?', 'tokens': [50966, 411, 8660, 286, 643, 281, 10432, 3866, 1412, 3095, 9130, 562, 286, 362, 3866, 24874, 45700, 30, 51382], 'temperature': 0.0, 'avg_logprob': -0.28349736272072307, 'compression_ratio': 1.393939393939394, 'no_speech_prob': 0.010194133035838604}, {'id': 311, 'seek': 262206, 'start': 2622.06, 'end': 2639.7, 'text': ' So what should I do? I think we have already discussed it, right? It is intersection. Because', 'tokens': [50364, 407, 437, 820, 286, 360, 30, 286, 519, 321, 362, 1217, 7152, 309, 11, 558, 30, 467, 307, 15236, 13, 1436, 51246], 'temperature': 0.0, 'avg_logprob': -0.17561549034671506, 'compression_ratio': 1.5274725274725274, 'no_speech_prob': 0.04866725206375122}, {'id': 312, 'seek': 262206, 'start': 2639.7, 'end': 2644.46, 'text': ' I want to make sure that it is surely available along all paths. It is like no matter where I', 'tokens': [51246, 286, 528, 281, 652, 988, 300, 309, 307, 11468, 2435, 2051, 439, 14518, 13, 467, 307, 411, 572, 1871, 689, 286, 51484], 'temperature': 0.0, 'avg_logprob': -0.17561549034671506, 'compression_ratio': 1.5274725274725274, 'no_speech_prob': 0.04866725206375122}, {'id': 313, 'seek': 262206, 'start': 2644.46, 'end': 2650.82, 'text': ' come from, it is there. What about very busy expressions? So what do very busy expressions', 'tokens': [51484, 808, 490, 11, 309, 307, 456, 13, 708, 466, 588, 5856, 15277, 30, 407, 437, 360, 588, 5856, 15277, 51802], 'temperature': 0.0, 'avg_logprob': -0.17561549034671506, 'compression_ratio': 1.5274725274725274, 'no_speech_prob': 0.04866725206375122}, {'id': 314, 'seek': 265082, 'start': 2650.82, 'end': 2662.1000000000004, 'text': ' say that from a given location I want to figure out what are the expressions which are surely', 'tokens': [50364, 584, 300, 490, 257, 2212, 4914, 286, 528, 281, 2573, 484, 437, 366, 264, 15277, 597, 366, 11468, 50928], 'temperature': 0.0, 'avg_logprob': -0.1816898567089136, 'compression_ratio': 1.545945945945946, 'no_speech_prob': 0.25555694103240967}, {'id': 315, 'seek': 265082, 'start': 2662.1000000000004, 'end': 2667.7000000000003, 'text': ' computed. So these expressions are very busy. I mean they are always being computed again and', 'tokens': [50928, 40610, 13, 407, 613, 15277, 366, 588, 5856, 13, 286, 914, 436, 366, 1009, 885, 40610, 797, 293, 51208], 'temperature': 0.0, 'avg_logprob': -0.1816898567089136, 'compression_ratio': 1.545945945945946, 'no_speech_prob': 0.25555694103240967}, {'id': 316, 'seek': 265082, 'start': 2667.7000000000003, 'end': 2675.6200000000003, 'text': ' again, right? So in that case what I can do is, why would I like to do this optimization? How does', 'tokens': [51208, 797, 11, 558, 30, 407, 294, 300, 1389, 437, 286, 393, 360, 307, 11, 983, 576, 286, 411, 281, 360, 341, 19618, 30, 1012, 775, 51604], 'temperature': 0.0, 'avg_logprob': -0.1816898567089136, 'compression_ratio': 1.545945945945946, 'no_speech_prob': 0.25555694103240967}, {'id': 317, 'seek': 267562, 'start': 2675.62, 'end': 2695.98, 'text': ' it help? Because the, I know, but still I am computing it only once along all paths. Okay,', 'tokens': [50364, 309, 854, 30, 1436, 264, 11, 286, 458, 11, 457, 920, 286, 669, 15866, 309, 787, 1564, 2051, 439, 14518, 13, 1033, 11, 51382], 'temperature': 0.0, 'avg_logprob': -0.2173940658569336, 'compression_ratio': 1.3216783216783217, 'no_speech_prob': 0.46274667978286743}, {'id': 318, 'seek': 267562, 'start': 2695.98, 'end': 2704.46, 'text': ' that is one idea. But again, they may not be the same if statement. They may be, no, you cannot do', 'tokens': [51382, 300, 307, 472, 1558, 13, 583, 797, 11, 436, 815, 406, 312, 264, 912, 498, 5629, 13, 814, 815, 312, 11, 572, 11, 291, 2644, 360, 51806], 'temperature': 0.0, 'avg_logprob': -0.2173940658569336, 'compression_ratio': 1.3216783216783217, 'no_speech_prob': 0.46274667978286743}, {'id': 319, 'seek': 270446, 'start': 2704.46, 'end': 2708.54, 'text': ' that. I am sorry. You cannot do that because somebody else might be using that statement. Oh,', 'tokens': [50364, 300, 13, 286, 669, 2597, 13, 509, 2644, 360, 300, 570, 2618, 1646, 1062, 312, 1228, 300, 5629, 13, 876, 11, 50568], 'temperature': 0.0, 'avg_logprob': -0.1703268557178731, 'compression_ratio': 1.668103448275862, 'no_speech_prob': 0.020287053659558296}, {'id': 320, 'seek': 270446, 'start': 2708.54, 'end': 2715.54, 'text': ' you can use the statement structure you are saying. Sure, sure, sure, sure, sure. But what', 'tokens': [50568, 291, 393, 764, 264, 5629, 3877, 291, 366, 1566, 13, 4894, 11, 988, 11, 988, 11, 988, 11, 988, 13, 583, 437, 50918], 'temperature': 0.0, 'avg_logprob': -0.1703268557178731, 'compression_ratio': 1.668103448275862, 'no_speech_prob': 0.020287053659558296}, {'id': 321, 'seek': 270446, 'start': 2715.54, 'end': 2723.3, 'text': ' does it help with? It does not help the computation cost. That comparison, okay, fine. But other than', 'tokens': [50918, 775, 309, 854, 365, 30, 467, 775, 406, 854, 264, 24903, 2063, 13, 663, 9660, 11, 1392, 11, 2489, 13, 583, 661, 813, 51306], 'temperature': 0.0, 'avg_logprob': -0.1703268557178731, 'compression_ratio': 1.668103448275862, 'no_speech_prob': 0.020287053659558296}, {'id': 322, 'seek': 270446, 'start': 2723.3, 'end': 2728.58, 'text': ' that what it does is mainly reduce code size. The same code of A plus B is happening multiple times.', 'tokens': [51306, 300, 437, 309, 775, 307, 8704, 5407, 3089, 2744, 13, 440, 912, 3089, 295, 316, 1804, 363, 307, 2737, 3866, 1413, 13, 51570], 'temperature': 0.0, 'avg_logprob': -0.1703268557178731, 'compression_ratio': 1.668103448275862, 'no_speech_prob': 0.020287053659558296}, {'id': 323, 'seek': 272858, 'start': 2728.58, 'end': 2735.14, 'text': ' It can be large expression and then you just do it once. But it is a nice classic analysis. So', 'tokens': [50364, 467, 393, 312, 2416, 6114, 293, 550, 291, 445, 360, 309, 1564, 13, 583, 309, 307, 257, 1481, 7230, 5215, 13, 407, 50692], 'temperature': 0.0, 'avg_logprob': -0.18797102161482268, 'compression_ratio': 1.3533834586466165, 'no_speech_prob': 0.2628842890262604}, {'id': 324, 'seek': 272858, 'start': 2735.14, 'end': 2753.5, 'text': ' we will keep it. Now how do I do this? So here how can I do this analysis? Excellent.', 'tokens': [50692, 321, 486, 1066, 309, 13, 823, 577, 360, 286, 360, 341, 30, 407, 510, 577, 393, 286, 360, 341, 5215, 30, 16723, 13, 51610], 'temperature': 0.0, 'avg_logprob': -0.18797102161482268, 'compression_ratio': 1.3533834586466165, 'no_speech_prob': 0.2628842890262604}, {'id': 325, 'seek': 275350, 'start': 2754.1, 'end': 2765.5, 'text': ' Intersection of and so the flow is backwards because I have to do it. And what is the gen and', 'tokens': [50394, 5751, 11963, 295, 293, 370, 264, 3095, 307, 12204, 570, 286, 362, 281, 360, 309, 13, 400, 437, 307, 264, 1049, 293, 50964], 'temperature': 0.0, 'avg_logprob': -0.15488751391147046, 'compression_ratio': 1.663716814159292, 'no_speech_prob': 0.26214948296546936}, {'id': 326, 'seek': 275350, 'start': 2765.5, 'end': 2770.46, 'text': ' kill? The expression being generated is a gen. So it is very similar to available expressions,', 'tokens': [50964, 1961, 30, 440, 6114, 885, 10833, 307, 257, 1049, 13, 407, 309, 307, 588, 2531, 281, 2435, 15277, 11, 51212], 'temperature': 0.0, 'avg_logprob': -0.15488751391147046, 'compression_ratio': 1.663716814159292, 'no_speech_prob': 0.26214948296546936}, {'id': 327, 'seek': 275350, 'start': 2770.46, 'end': 2775.06, 'text': ' right? Because I again want to say that I want to waste it up so that I can compute it once and', 'tokens': [51212, 558, 30, 1436, 286, 797, 528, 281, 584, 300, 286, 528, 281, 5964, 309, 493, 370, 300, 286, 393, 14722, 309, 1564, 293, 51442], 'temperature': 0.0, 'avg_logprob': -0.15488751391147046, 'compression_ratio': 1.663716814159292, 'no_speech_prob': 0.26214948296546936}, {'id': 328, 'seek': 275350, 'start': 2775.06, 'end': 2779.1, 'text': ' all the branches I end up using that value. So when does it get killed? When the expression', 'tokens': [51442, 439, 264, 14770, 286, 917, 493, 1228, 300, 2158, 13, 407, 562, 775, 309, 483, 4652, 30, 1133, 264, 6114, 51644], 'temperature': 0.0, 'avg_logprob': -0.15488751391147046, 'compression_ratio': 1.663716814159292, 'no_speech_prob': 0.26214948296546936}, {'id': 329, 'seek': 277910, 'start': 2779.1, 'end': 2785.58, 'text': ' does not, it is not valid anymore. Re-definition to one of the variables values, expression', 'tokens': [50364, 775, 406, 11, 309, 307, 406, 7363, 3602, 13, 1300, 12, 1479, 5194, 849, 281, 472, 295, 264, 9102, 4190, 11, 6114, 50688], 'temperature': 0.0, 'avg_logprob': -0.20491684361508017, 'compression_ratio': 1.7864077669902914, 'no_speech_prob': 0.02746831253170967}, {'id': 330, 'seek': 277910, 'start': 2785.58, 'end': 2791.2599999999998, 'text': ' variables values. So the same, it is the same business. So these are four very different', 'tokens': [50688, 9102, 4190, 13, 407, 264, 912, 11, 309, 307, 264, 912, 1606, 13, 407, 613, 366, 1451, 588, 819, 50972], 'temperature': 0.0, 'avg_logprob': -0.20491684361508017, 'compression_ratio': 1.7864077669902914, 'no_speech_prob': 0.02746831253170967}, {'id': 331, 'seek': 277910, 'start': 2791.2599999999998, 'end': 2798.02, 'text': ' analysis. But can you see some commonality here? There is a lot of commonality, right? There is,', 'tokens': [50972, 5215, 13, 583, 393, 291, 536, 512, 2689, 1860, 510, 30, 821, 307, 257, 688, 295, 2689, 1860, 11, 558, 30, 821, 307, 11, 51310], 'temperature': 0.0, 'avg_logprob': -0.20491684361508017, 'compression_ratio': 1.7864077669902914, 'no_speech_prob': 0.02746831253170967}, {'id': 332, 'seek': 277910, 'start': 2798.02, 'end': 2804.54, 'text': ' it is almost the same structure, right? So that is the cool part about data flow analysis.', 'tokens': [51310, 309, 307, 1920, 264, 912, 3877, 11, 558, 30, 407, 300, 307, 264, 1627, 644, 466, 1412, 3095, 5215, 13, 51636], 'temperature': 0.0, 'avg_logprob': -0.20491684361508017, 'compression_ratio': 1.7864077669902914, 'no_speech_prob': 0.02746831253170967}, {'id': 333, 'seek': 280454, 'start': 2804.54, 'end': 2810.82, 'text': ' Now to get an image, okay. So essentially now we have reaching definitions which we have', 'tokens': [50364, 823, 281, 483, 364, 3256, 11, 1392, 13, 407, 4476, 586, 321, 362, 9906, 21988, 597, 321, 362, 50678], 'temperature': 0.0, 'avg_logprob': -0.2684294685484871, 'compression_ratio': 1.608433734939759, 'no_speech_prob': 0.020979398861527443}, {'id': 334, 'seek': 280454, 'start': 2810.82, 'end': 2817.94, 'text': ' available expressions, we have very busy expressions and we have liveness analysis,', 'tokens': [50678, 2435, 15277, 11, 321, 362, 588, 5856, 15277, 293, 321, 362, 375, 553, 442, 5215, 11, 51034], 'temperature': 0.0, 'avg_logprob': -0.2684294685484871, 'compression_ratio': 1.608433734939759, 'no_speech_prob': 0.020979398861527443}, {'id': 335, 'seek': 280454, 'start': 2817.94, 'end': 2827.18, 'text': ' right? So we have one aspect is the direction in which I need to move my analysis. In reaching', 'tokens': [51034, 558, 30, 407, 321, 362, 472, 4171, 307, 264, 3513, 294, 597, 286, 643, 281, 1286, 452, 5215, 13, 682, 9906, 51496], 'temperature': 0.0, 'avg_logprob': -0.2684294685484871, 'compression_ratio': 1.608433734939759, 'no_speech_prob': 0.020979398861527443}, {'id': 336, 'seek': 282718, 'start': 2827.18, 'end': 2840.7799999999997, 'text': ' definition I move it forward. In available expression, forward. Very busy expression', 'tokens': [50364, 7123, 286, 1286, 309, 2128, 13, 682, 2435, 6114, 11, 2128, 13, 4372, 5856, 6114, 51044], 'temperature': 0.0, 'avg_logprob': -0.1929482120578572, 'compression_ratio': 1.5773809523809523, 'no_speech_prob': 0.2738986313343048}, {'id': 337, 'seek': 282718, 'start': 2840.7799999999997, 'end': 2851.18, 'text': ' backwards and liveness analysis backward. So the meet operator, what do I do when multiple', 'tokens': [51044, 12204, 293, 375, 553, 442, 5215, 23897, 13, 407, 264, 1677, 12973, 11, 437, 360, 286, 360, 562, 3866, 51564], 'temperature': 0.0, 'avg_logprob': -0.1929482120578572, 'compression_ratio': 1.5773809523809523, 'no_speech_prob': 0.2738986313343048}, {'id': 338, 'seek': 282718, 'start': 2851.18, 'end': 2855.58, 'text': ' flows meet either in a backward direction or in the forward direction, right? So reaching', 'tokens': [51564, 12867, 1677, 2139, 294, 257, 23897, 3513, 420, 294, 264, 2128, 3513, 11, 558, 30, 407, 9906, 51784], 'temperature': 0.0, 'avg_logprob': -0.1929482120578572, 'compression_ratio': 1.5773809523809523, 'no_speech_prob': 0.2738986313343048}, {'id': 339, 'seek': 285558, 'start': 2855.58, 'end': 2868.98, 'text': ' definition what do we do? Union. Available expressions? Available expressions surely', 'tokens': [50364, 7123, 437, 360, 321, 360, 30, 8133, 13, 11667, 32699, 15277, 30, 11667, 32699, 15277, 11468, 51034], 'temperature': 0.0, 'avg_logprob': -0.24598278143467048, 'compression_ratio': 1.6018518518518519, 'no_speech_prob': 0.2560070753097534}, {'id': 340, 'seek': 285558, 'start': 2868.98, 'end': 2876.8199999999997, 'text': ' must be available. Intersection. Intersection. Very busy expression surely must be there', 'tokens': [51034, 1633, 312, 2435, 13, 5751, 11963, 13, 5751, 11963, 13, 4372, 5856, 6114, 11468, 1633, 312, 456, 51426], 'temperature': 0.0, 'avg_logprob': -0.24598278143467048, 'compression_ratio': 1.6018518518518519, 'no_speech_prob': 0.2560070753097534}, {'id': 341, 'seek': 287682, 'start': 2876.82, 'end': 2892.26, 'text': ' in all blocks. Available liveness analysis should be used along some path. Union, right? So these are,', 'tokens': [50364, 294, 439, 8474, 13, 11667, 32699, 375, 553, 442, 5215, 820, 312, 1143, 2051, 512, 3100, 13, 8133, 11, 558, 30, 407, 613, 366, 11, 51136], 'temperature': 0.0, 'avg_logprob': -0.22755588334182214, 'compression_ratio': 1.0851063829787233, 'no_speech_prob': 0.5607205629348755}, {'id': 342, 'seek': 289226, 'start': 2892.26, 'end': 2909.3, 'text': ' these unions are some path or any path and these are all paths. Then there is how to compute the', 'tokens': [50364, 613, 24914, 366, 512, 3100, 420, 604, 3100, 293, 613, 366, 439, 14518, 13, 1396, 456, 307, 577, 281, 14722, 264, 51216], 'temperature': 0.0, 'avg_logprob': -0.11751653710190131, 'compression_ratio': 1.476923076923077, 'no_speech_prob': 0.044522929936647415}, {'id': 343, 'seek': 289226, 'start': 2909.3, 'end': 2920.0200000000004, 'text': ' gen and the kill sets, right? And then there is this question of the universal set. So the data', 'tokens': [51216, 1049, 293, 264, 1961, 6352, 11, 558, 30, 400, 550, 456, 307, 341, 1168, 295, 264, 11455, 992, 13, 407, 264, 1412, 51752], 'temperature': 0.0, 'avg_logprob': -0.11751653710190131, 'compression_ratio': 1.476923076923077, 'no_speech_prob': 0.044522929936647415}, {'id': 344, 'seek': 292002, 'start': 2920.02, 'end': 2927.42, 'text': ' flow facts. So the facts in this case in reaching definitions they are what? They are definitions.', 'tokens': [50364, 3095, 9130, 13, 407, 264, 9130, 294, 341, 1389, 294, 9906, 21988, 436, 366, 437, 30, 814, 366, 21988, 13, 50734], 'temperature': 0.0, 'avg_logprob': -0.13517111264742337, 'compression_ratio': 1.8164556962025316, 'no_speech_prob': 0.38454124331474304}, {'id': 345, 'seek': 292002, 'start': 2927.42, 'end': 2933.34, 'text': ' In available expression they are expressions. In very busy expression they are expressions', 'tokens': [50734, 682, 2435, 6114, 436, 366, 15277, 13, 682, 588, 5856, 6114, 436, 366, 15277, 51030], 'temperature': 0.0, 'avg_logprob': -0.13517111264742337, 'compression_ratio': 1.8164556962025316, 'no_speech_prob': 0.38454124331474304}, {'id': 346, 'seek': 292002, 'start': 2933.34, 'end': 2949.14, 'text': ' and liveness analysis they are variables. Agreed? So now for all this analysis can I try to write', 'tokens': [51030, 293, 375, 553, 442, 5215, 436, 366, 9102, 13, 29324, 292, 30, 407, 586, 337, 439, 341, 5215, 393, 286, 853, 281, 2464, 51820], 'temperature': 0.0, 'avg_logprob': -0.13517111264742337, 'compression_ratio': 1.8164556962025316, 'no_speech_prob': 0.38454124331474304}, {'id': 347, 'seek': 294914, 'start': 2949.14, 'end': 2957.42, 'text': ' a unified equation? So either forward or backward it sort of accumulates. So there are two things.', 'tokens': [50364, 257, 26787, 5367, 30, 407, 2139, 2128, 420, 23897, 309, 1333, 295, 12989, 26192, 13, 407, 456, 366, 732, 721, 13, 50778], 'temperature': 0.0, 'avg_logprob': -0.16256514839504077, 'compression_ratio': 1.7034883720930232, 'no_speech_prob': 0.03510653227567673}, {'id': 348, 'seek': 294914, 'start': 2957.42, 'end': 2964.5, 'text': ' One is either at the forward point or at the backward point the data flow facts are accumulated', 'tokens': [50778, 1485, 307, 2139, 412, 264, 2128, 935, 420, 412, 264, 23897, 935, 264, 1412, 3095, 9130, 366, 31346, 51132], 'temperature': 0.0, 'avg_logprob': -0.16256514839504077, 'compression_ratio': 1.7034883720930232, 'no_speech_prob': 0.03510653227567673}, {'id': 349, 'seek': 294914, 'start': 2964.5, 'end': 2977.9, 'text': ' using the meet operation. That is one part, right? So one is accumulation. The second part is that', 'tokens': [51132, 1228, 264, 1677, 6916, 13, 663, 307, 472, 644, 11, 558, 30, 407, 472, 307, 35647, 13, 440, 1150, 644, 307, 300, 51802], 'temperature': 0.0, 'avg_logprob': -0.16256514839504077, 'compression_ratio': 1.7034883720930232, 'no_speech_prob': 0.03510653227567673}, {'id': 350, 'seek': 297790, 'start': 2978.02, 'end': 2985.06, 'text': ' once I have accumulated these values I need to say that what is going to be the value of that,', 'tokens': [50370, 1564, 286, 362, 31346, 613, 4190, 286, 643, 281, 584, 300, 437, 307, 516, 281, 312, 264, 2158, 295, 300, 11, 50722], 'temperature': 0.0, 'avg_logprob': -0.17277394964339884, 'compression_ratio': 1.7897196261682242, 'no_speech_prob': 0.017946695908904076}, {'id': 351, 'seek': 297790, 'start': 2985.06, 'end': 2992.3, 'text': ' of the set the data flow facts after I cross the basic block, right? This is something referred', 'tokens': [50722, 295, 264, 992, 264, 1412, 3095, 9130, 934, 286, 3278, 264, 3875, 3461, 11, 558, 30, 639, 307, 746, 10839, 51084], 'temperature': 0.0, 'avg_logprob': -0.17277394964339884, 'compression_ratio': 1.7897196261682242, 'no_speech_prob': 0.017946695908904076}, {'id': 352, 'seek': 297790, 'start': 2992.3, 'end': 3002.58, 'text': ' to as a transfer form. So it says that if I know the data flow facts either at the beginning or', 'tokens': [51084, 281, 382, 257, 5003, 1254, 13, 407, 309, 1619, 300, 498, 286, 458, 264, 1412, 3095, 9130, 2139, 412, 264, 2863, 420, 51598], 'temperature': 0.0, 'avg_logprob': -0.17277394964339884, 'compression_ratio': 1.7897196261682242, 'no_speech_prob': 0.017946695908904076}, {'id': 353, 'seek': 297790, 'start': 3002.58, 'end': 3007.58, 'text': ' end of the basic block how do I move it to the end or beginning of the basic block respectively?', 'tokens': [51598, 917, 295, 264, 3875, 3461, 577, 360, 286, 1286, 309, 281, 264, 917, 420, 2863, 295, 264, 3875, 3461, 25009, 30, 51848], 'temperature': 0.0, 'avg_logprob': -0.17277394964339884, 'compression_ratio': 1.7897196261682242, 'no_speech_prob': 0.017946695908904076}, {'id': 354, 'seek': 300758, 'start': 3007.58, 'end': 3015.34, 'text': ' That is how do I transfer it through the basic block, right? So what does the transfer function', 'tokens': [50364, 663, 307, 577, 360, 286, 5003, 309, 807, 264, 3875, 3461, 11, 558, 30, 407, 437, 775, 264, 5003, 2445, 50752], 'temperature': 0.0, 'avg_logprob': -0.343928355091023, 'compression_ratio': 1.4057971014492754, 'no_speech_prob': 0.01236819103360176}, {'id': 355, 'seek': 300758, 'start': 3015.34, 'end': 3029.94, 'text': ' for all of them look like? Gen of n minus, sorry, sorry, sorry, I should have, where is the value?', 'tokens': [50752, 337, 439, 295, 552, 574, 411, 30, 3632, 295, 297, 3175, 11, 2597, 11, 2597, 11, 2597, 11, 286, 820, 362, 11, 689, 307, 264, 2158, 30, 51482], 'temperature': 0.0, 'avg_logprob': -0.343928355091023, 'compression_ratio': 1.4057971014492754, 'no_speech_prob': 0.01236819103360176}, {'id': 356, 'seek': 302994, 'start': 3029.94, 'end': 3050.2200000000003, 'text': ' Still of n union gen of n. Really interesting, right? Four different problems. All of them', 'tokens': [50364, 8291, 295, 297, 11671, 1049, 295, 297, 13, 4083, 1880, 11, 558, 30, 7451, 819, 2740, 13, 1057, 295, 552, 51378], 'temperature': 0.0, 'avg_logprob': -0.2646613348098028, 'compression_ratio': 1.3021582733812949, 'no_speech_prob': 0.033022381365299225}, {'id': 357, 'seek': 302994, 'start': 3050.2200000000003, 'end': 3057.7000000000003, 'text': ' have looking transfer function. And what is the accumulation thing? It just basically does', 'tokens': [51378, 362, 1237, 5003, 2445, 13, 400, 437, 307, 264, 35647, 551, 30, 467, 445, 1936, 775, 51752], 'temperature': 0.0, 'avg_logprob': -0.2646613348098028, 'compression_ratio': 1.3021582733812949, 'no_speech_prob': 0.033022381365299225}, {'id': 358, 'seek': 305770, 'start': 3057.7, 'end': 3069.9399999999996, 'text': ' either meet or union or intersection over either the successor or the predecessor of the previous', 'tokens': [50364, 2139, 1677, 420, 11671, 420, 15236, 670, 2139, 264, 31864, 420, 264, 34991, 295, 264, 3894, 50976], 'temperature': 0.0, 'avg_logprob': -0.1362563206599309, 'compression_ratio': 1.7005988023952097, 'no_speech_prob': 0.04592462629079819}, {'id': 359, 'seek': 305770, 'start': 3069.9399999999996, 'end': 3080.18, 'text': ' or the next set of data flow facts. So the question now is that if there is and no matter', 'tokens': [50976, 420, 264, 958, 992, 295, 1412, 3095, 9130, 13, 407, 264, 1168, 586, 307, 300, 498, 456, 307, 293, 572, 1871, 51488], 'temperature': 0.0, 'avg_logprob': -0.1362563206599309, 'compression_ratio': 1.7005988023952097, 'no_speech_prob': 0.04592462629079819}, {'id': 360, 'seek': 305770, 'start': 3080.18, 'end': 3085.14, 'text': ' what, no matter which data flow analysis you come up with you will see a very similar structure.', 'tokens': [51488, 437, 11, 572, 1871, 597, 1412, 3095, 5215, 291, 808, 493, 365, 291, 486, 536, 257, 588, 2531, 3877, 13, 51736], 'temperature': 0.0, 'avg_logprob': -0.1362563206599309, 'compression_ratio': 1.7005988023952097, 'no_speech_prob': 0.04592462629079819}, {'id': 361, 'seek': 308514, 'start': 3085.14, 'end': 3089.22, 'text': ' What can change is the transfer function may look very different, the meet operator might', 'tokens': [50364, 708, 393, 1319, 307, 264, 5003, 2445, 815, 574, 588, 819, 11, 264, 1677, 12973, 1062, 50568], 'temperature': 0.0, 'avg_logprob': -0.15194278888488083, 'compression_ratio': 1.6972477064220184, 'no_speech_prob': 0.05172709375619888}, {'id': 362, 'seek': 308514, 'start': 3089.22, 'end': 3096.8599999999997, 'text': ' look very different, but the idea would sort of remain the same, right? So if that is the case,', 'tokens': [50568, 574, 588, 819, 11, 457, 264, 1558, 576, 1333, 295, 6222, 264, 912, 11, 558, 30, 407, 498, 300, 307, 264, 1389, 11, 50950], 'temperature': 0.0, 'avg_logprob': -0.15194278888488083, 'compression_ratio': 1.6972477064220184, 'no_speech_prob': 0.05172709375619888}, {'id': 363, 'seek': 308514, 'start': 3096.8599999999997, 'end': 3106.3799999999997, 'text': ' the question is can we come up with a unified model of such analysis, right? Which will say', 'tokens': [50950, 264, 1168, 307, 393, 321, 808, 493, 365, 257, 26787, 2316, 295, 1270, 5215, 11, 558, 30, 3013, 486, 584, 51426], 'temperature': 0.0, 'avg_logprob': -0.15194278888488083, 'compression_ratio': 1.6972477064220184, 'no_speech_prob': 0.05172709375619888}, {'id': 364, 'seek': 308514, 'start': 3106.3799999999997, 'end': 3111.62, 'text': ' that okay now if this is algorithm you can use, you can plug in your version of the transfer', 'tokens': [51426, 300, 1392, 586, 498, 341, 307, 9284, 291, 393, 764, 11, 291, 393, 5452, 294, 428, 3037, 295, 264, 5003, 51688], 'temperature': 0.0, 'avg_logprob': -0.15194278888488083, 'compression_ratio': 1.6972477064220184, 'no_speech_prob': 0.05172709375619888}, {'id': 365, 'seek': 311162, 'start': 3111.62, 'end': 3118.66, 'text': ' function, you can plug in your version of the meet operator, you can plug in like the direction', 'tokens': [50364, 2445, 11, 291, 393, 5452, 294, 428, 3037, 295, 264, 1677, 12973, 11, 291, 393, 5452, 294, 411, 264, 3513, 50716], 'temperature': 0.0, 'avg_logprob': -0.13995193110571968, 'compression_ratio': 1.595505617977528, 'no_speech_prob': 0.7940741181373596}, {'id': 366, 'seek': 311162, 'start': 3118.66, 'end': 3123.58, 'text': ' which way you want the data flow analysis to work, but everything else I will take care of,', 'tokens': [50716, 597, 636, 291, 528, 264, 1412, 3095, 5215, 281, 589, 11, 457, 1203, 1646, 286, 486, 747, 1127, 295, 11, 50962], 'temperature': 0.0, 'avg_logprob': -0.13995193110571968, 'compression_ratio': 1.595505617977528, 'no_speech_prob': 0.7940741181373596}, {'id': 367, 'seek': 311162, 'start': 3123.58, 'end': 3129.22, 'text': ' right? How do I propagate it, which order I do it, when do I say I have computed the whole thing', 'tokens': [50962, 558, 30, 1012, 360, 286, 48256, 309, 11, 597, 1668, 286, 360, 309, 11, 562, 360, 286, 584, 286, 362, 40610, 264, 1379, 551, 51244], 'temperature': 0.0, 'avg_logprob': -0.13995193110571968, 'compression_ratio': 1.595505617977528, 'no_speech_prob': 0.7940741181373596}, {'id': 368, 'seek': 312922, 'start': 3129.22, 'end': 3151.3399999999997, 'text': ' and so on. Yes, that is a very good question. So yeah, so we also have to plug in the initial', 'tokens': [50364, 293, 370, 322, 13, 1079, 11, 300, 307, 257, 588, 665, 1168, 13, 407, 1338, 11, 370, 321, 611, 362, 281, 5452, 294, 264, 5883, 51470], 'temperature': 0.0, 'avg_logprob': -0.1784305191040039, 'compression_ratio': 1.4045801526717556, 'no_speech_prob': 0.1033739373087883}, {'id': 369, 'seek': 312922, 'start': 3151.3399999999997, 'end': 3157.2999999999997, 'text': ' set and for the moment I would say that for case to case basis you should see for yourself', 'tokens': [51470, 992, 293, 337, 264, 1623, 286, 576, 584, 300, 337, 1389, 281, 1389, 5143, 291, 820, 536, 337, 1803, 51768], 'temperature': 0.0, 'avg_logprob': -0.1784305191040039, 'compression_ratio': 1.4045801526717556, 'no_speech_prob': 0.1033739373087883}, {'id': 370, 'seek': 315730, 'start': 3157.5800000000004, 'end': 3165.94, 'text': ' which works, the empty set works or the universal set works. Can you give some idea of which one', 'tokens': [50378, 597, 1985, 11, 264, 6707, 992, 1985, 420, 264, 11455, 992, 1985, 13, 1664, 291, 976, 512, 1558, 295, 597, 472, 50796], 'temperature': 0.0, 'avg_logprob': -0.1519724494532535, 'compression_ratio': 1.668122270742358, 'no_speech_prob': 0.1947058141231537}, {'id': 371, 'seek': 315730, 'start': 3165.94, 'end': 3172.6600000000003, 'text': ' will work? So initial set is like for the first or the last basic block. Now again we have the', 'tokens': [50796, 486, 589, 30, 407, 5883, 992, 307, 411, 337, 264, 700, 420, 264, 1036, 3875, 3461, 13, 823, 797, 321, 362, 264, 51132], 'temperature': 0.0, 'avg_logprob': -0.1519724494532535, 'compression_ratio': 1.668122270742358, 'no_speech_prob': 0.1947058141231537}, {'id': 372, 'seek': 315730, 'start': 3172.6600000000003, 'end': 3176.6600000000003, 'text': ' same problem if you look at it, right? We have the same problem that we had with dominators.', 'tokens': [51132, 912, 1154, 498, 291, 574, 412, 309, 11, 558, 30, 492, 362, 264, 912, 1154, 300, 321, 632, 365, 8859, 3391, 13, 51332], 'temperature': 0.0, 'avg_logprob': -0.1519724494532535, 'compression_ratio': 1.668122270742358, 'no_speech_prob': 0.1947058141231537}, {'id': 373, 'seek': 315730, 'start': 3176.6600000000003, 'end': 3185.1000000000004, 'text': " I can apply a very similar algorithm for every node essentially now if I say that let's say right", 'tokens': [51332, 286, 393, 3079, 257, 588, 2531, 9284, 337, 633, 9984, 4476, 586, 498, 286, 584, 300, 718, 311, 584, 558, 51754], 'temperature': 0.0, 'avg_logprob': -0.1519724494532535, 'compression_ratio': 1.668122270742358, 'no_speech_prob': 0.1947058141231537}, {'id': 374, 'seek': 318510, 'start': 3185.1, 'end': 3190.02, 'text': " pick it for something it is easier to do it than let's say reaching definition. So for reaching", 'tokens': [50364, 1888, 309, 337, 746, 309, 307, 3571, 281, 360, 309, 813, 718, 311, 584, 9906, 7123, 13, 407, 337, 9906, 50610], 'temperature': 0.0, 'avg_logprob': -0.24111567372861115, 'compression_ratio': 1.48, 'no_speech_prob': 0.033751942217350006}, {'id': 375, 'seek': 318510, 'start': 3190.02, 'end': 3204.8199999999997, 'text': ' definition for a given node n, now the equation turns out to be you take a union over the', 'tokens': [50610, 7123, 337, 257, 2212, 9984, 297, 11, 586, 264, 5367, 4523, 484, 281, 312, 291, 747, 257, 11671, 670, 264, 51350], 'temperature': 0.0, 'avg_logprob': -0.24111567372861115, 'compression_ratio': 1.48, 'no_speech_prob': 0.033751942217350006}, {'id': 376, 'seek': 320482, 'start': 3204.82, 'end': 3221.2200000000003, 'text': ' predecessors r d of p, right? Minus for you subtract what happens at that particular node', 'tokens': [50364, 24874, 45700, 367, 274, 295, 280, 11, 558, 30, 2829, 301, 337, 291, 16390, 437, 2314, 412, 300, 1729, 9984, 51184], 'temperature': 0.0, 'avg_logprob': -0.1839643096923828, 'compression_ratio': 1.421875, 'no_speech_prob': 0.20651397109031677}, {'id': 377, 'seek': 320482, 'start': 3221.2200000000003, 'end': 3232.98, 'text': ' n and you union with the gen of that n, right? So for every node n you can come up with such', 'tokens': [51184, 297, 293, 291, 11671, 365, 264, 1049, 295, 300, 297, 11, 558, 30, 407, 337, 633, 9984, 297, 291, 393, 808, 493, 365, 1270, 51772], 'temperature': 0.0, 'avg_logprob': -0.1839643096923828, 'compression_ratio': 1.421875, 'no_speech_prob': 0.20651397109031677}, {'id': 378, 'seek': 323298, 'start': 3232.98, 'end': 3241.06, 'text': ' a equation. Agreed? So the problem is that there is a r d here sitting here and there is a r d', 'tokens': [50364, 257, 5367, 13, 29324, 292, 30, 407, 264, 1154, 307, 300, 456, 307, 257, 367, 274, 510, 3798, 510, 293, 456, 307, 257, 367, 274, 50768], 'temperature': 0.0, 'avg_logprob': -0.1939289715825295, 'compression_ratio': 1.7971014492753623, 'no_speech_prob': 0.044368017464876175}, {'id': 379, 'seek': 323298, 'start': 3241.06, 'end': 3248.66, 'text': ' sitting here, right? And not just that I mean this p can like if what if there is a self loop,', 'tokens': [50768, 3798, 510, 11, 558, 30, 400, 406, 445, 300, 286, 914, 341, 280, 393, 411, 498, 437, 498, 456, 307, 257, 2698, 6367, 11, 51148], 'temperature': 0.0, 'avg_logprob': -0.1939289715825295, 'compression_ratio': 1.7971014492753623, 'no_speech_prob': 0.044368017464876175}, {'id': 380, 'seek': 323298, 'start': 3248.66, 'end': 3254.54, 'text': ' I can immediately see that there is n can occur in both sides, right? And even if there is a', 'tokens': [51148, 286, 393, 4258, 536, 300, 456, 307, 297, 393, 5160, 294, 1293, 4881, 11, 558, 30, 400, 754, 498, 456, 307, 257, 51442], 'temperature': 0.0, 'avg_logprob': -0.1939289715825295, 'compression_ratio': 1.7971014492753623, 'no_speech_prob': 0.044368017464876175}, {'id': 381, 'seek': 323298, 'start': 3254.54, 'end': 3259.14, 'text': ' longer cyclic loop there will be a transit dependency. Eventually I mean you will compute', 'tokens': [51442, 2854, 38154, 1050, 6367, 456, 486, 312, 257, 17976, 33621, 13, 17586, 286, 914, 291, 486, 14722, 51672], 'temperature': 0.0, 'avg_logprob': -0.1939289715825295, 'compression_ratio': 1.7971014492753623, 'no_speech_prob': 0.044368017464876175}, {'id': 382, 'seek': 325914, 'start': 3259.14, 'end': 3264.42, 'text': ' some value which through that cycle will again propagate and affect this value again. Again for', 'tokens': [50364, 512, 2158, 597, 807, 300, 6586, 486, 797, 48256, 293, 3345, 341, 2158, 797, 13, 3764, 337, 50628], 'temperature': 0.0, 'avg_logprob': -0.14489667961396366, 'compression_ratio': 1.7181818181818183, 'no_speech_prob': 0.21616318821907043}, {'id': 383, 'seek': 325914, 'start': 3264.42, 'end': 3272.2599999999998, 'text': ' this particular business I have to do this fixed point computation. The problem is dominators we', 'tokens': [50628, 341, 1729, 1606, 286, 362, 281, 360, 341, 6806, 935, 24903, 13, 440, 1154, 307, 8859, 3391, 321, 51020], 'temperature': 0.0, 'avg_logprob': -0.14489667961396366, 'compression_ratio': 1.7181818181818183, 'no_speech_prob': 0.21616318821907043}, {'id': 384, 'seek': 325914, 'start': 3272.2599999999998, 'end': 3277.1, 'text': ' could get away with removing the back edges in this case we cannot get away with removing the', 'tokens': [51020, 727, 483, 1314, 365, 12720, 264, 646, 8819, 294, 341, 1389, 321, 2644, 483, 1314, 365, 12720, 264, 51262], 'temperature': 0.0, 'avg_logprob': -0.14489667961396366, 'compression_ratio': 1.7181818181818183, 'no_speech_prob': 0.21616318821907043}, {'id': 385, 'seek': 325914, 'start': 3277.1, 'end': 3284.7, 'text': ' back edges, right? So essentially the way you will do it is you will keep on applying these', 'tokens': [51262, 646, 8819, 11, 558, 30, 407, 4476, 264, 636, 291, 486, 360, 309, 307, 291, 486, 1066, 322, 9275, 613, 51642], 'temperature': 0.0, 'avg_logprob': -0.14489667961396366, 'compression_ratio': 1.7181818181818183, 'no_speech_prob': 0.21616318821907043}, {'id': 386, 'seek': 328470, 'start': 3284.7, 'end': 3290.9399999999996, 'text': ' equations again and again and again and again till it comes to a stage where even if you go', 'tokens': [50364, 11787, 797, 293, 797, 293, 797, 293, 797, 4288, 309, 1487, 281, 257, 3233, 689, 754, 498, 291, 352, 50676], 'temperature': 0.0, 'avg_logprob': -0.12239301204681396, 'compression_ratio': 1.7713004484304933, 'no_speech_prob': 0.5207152366638184}, {'id': 387, 'seek': 328470, 'start': 3290.9399999999996, 'end': 3297.4199999999996, 'text': ' through all the nodes apply the equations again it will not create any change to the whole algorithm,', 'tokens': [50676, 807, 439, 264, 13891, 3079, 264, 11787, 797, 309, 486, 406, 1884, 604, 1319, 281, 264, 1379, 9284, 11, 51000], 'temperature': 0.0, 'avg_logprob': -0.12239301204681396, 'compression_ratio': 1.7713004484304933, 'no_speech_prob': 0.5207152366638184}, {'id': 388, 'seek': 328470, 'start': 3297.4199999999996, 'end': 3303.58, 'text': ' right? So now the question is that if I am going to do that I need to initialize my sets the initial', 'tokens': [51000, 558, 30, 407, 586, 264, 1168, 307, 300, 498, 286, 669, 516, 281, 360, 300, 286, 643, 281, 5883, 1125, 452, 6352, 264, 5883, 51308], 'temperature': 0.0, 'avg_logprob': -0.12239301204681396, 'compression_ratio': 1.7713004484304933, 'no_speech_prob': 0.5207152366638184}, {'id': 389, 'seek': 328470, 'start': 3303.58, 'end': 3310.98, 'text': ' sets. What are going to be my initial sets? So for liveness analysis like maybe so that is homework.', 'tokens': [51308, 6352, 13, 708, 366, 516, 281, 312, 452, 5883, 6352, 30, 407, 337, 375, 553, 442, 5215, 411, 1310, 370, 300, 307, 14578, 13, 51678], 'temperature': 0.0, 'avg_logprob': -0.12239301204681396, 'compression_ratio': 1.7713004484304933, 'no_speech_prob': 0.5207152366638184}, {'id': 390, 'seek': 331098, 'start': 3311.38, 'end': 3316.46, 'text': ' For each of these cases can you figure out what are going to be your initial sets? What should', 'tokens': [50384, 1171, 1184, 295, 613, 3331, 393, 291, 2573, 484, 437, 366, 516, 281, 312, 428, 5883, 6352, 30, 708, 820, 50638], 'temperature': 0.0, 'avg_logprob': -0.17642616439651657, 'compression_ratio': 1.6625, 'no_speech_prob': 0.08022704720497131}, {'id': 391, 'seek': 331098, 'start': 3316.46, 'end': 3321.02, 'text': ' you initialize every basic block width? Of course I mean there will be exception the top most or the', 'tokens': [50638, 291, 5883, 1125, 633, 3875, 3461, 11402, 30, 2720, 1164, 286, 914, 456, 486, 312, 11183, 264, 1192, 881, 420, 264, 50866], 'temperature': 0.0, 'avg_logprob': -0.17642616439651657, 'compression_ratio': 1.6625, 'no_speech_prob': 0.08022704720497131}, {'id': 392, 'seek': 331098, 'start': 3321.02, 'end': 3325.3, 'text': ' bottom most will be initialized slightly differently because you already know that set. The others you', 'tokens': [50866, 2767, 881, 486, 312, 5883, 1602, 4748, 7614, 570, 291, 1217, 458, 300, 992, 13, 440, 2357, 291, 51080], 'temperature': 0.0, 'avg_logprob': -0.17642616439651657, 'compression_ratio': 1.6625, 'no_speech_prob': 0.08022704720497131}, {'id': 393, 'seek': 331098, 'start': 3325.3, 'end': 3334.7400000000002, 'text': ' have to initialize to either the empty set or the full set the universal set. So which case what you', 'tokens': [51080, 362, 281, 5883, 1125, 281, 2139, 264, 6707, 992, 420, 264, 1577, 992, 264, 11455, 992, 13, 407, 597, 1389, 437, 291, 51552], 'temperature': 0.0, 'avg_logprob': -0.17642616439651657, 'compression_ratio': 1.6625, 'no_speech_prob': 0.08022704720497131}, {'id': 394, 'seek': 333474, 'start': 3334.74, 'end': 3346.9799999999996, 'text': ' have to initialize it. So that is something you have to think about. So now I will just leave', 'tokens': [50364, 362, 281, 5883, 1125, 309, 13, 407, 300, 307, 746, 291, 362, 281, 519, 466, 13, 407, 586, 286, 486, 445, 1856, 50976], 'temperature': 0.0, 'avg_logprob': -0.19343397352430555, 'compression_ratio': 1.540983606557377, 'no_speech_prob': 0.06812053173780441}, {'id': 395, 'seek': 333474, 'start': 3346.9799999999996, 'end': 3355.02, 'text': ' with this. Can we write out this algorithm very quickly? What would this algorithm look like?', 'tokens': [50976, 365, 341, 13, 1664, 321, 2464, 484, 341, 9284, 588, 2661, 30, 708, 576, 341, 9284, 574, 411, 30, 51378], 'temperature': 0.0, 'avg_logprob': -0.19343397352430555, 'compression_ratio': 1.540983606557377, 'no_speech_prob': 0.06812053173780441}, {'id': 396, 'seek': 333474, 'start': 3355.02, 'end': 3363.1, 'text': ' Or maybe we will do it tomorrow. I think we are already cross time. So have it in your just in', 'tokens': [51378, 1610, 1310, 321, 486, 360, 309, 4153, 13, 286, 519, 321, 366, 1217, 3278, 565, 13, 407, 362, 309, 294, 428, 445, 294, 51782], 'temperature': 0.0, 'avg_logprob': -0.19343397352430555, 'compression_ratio': 1.540983606557377, 'no_speech_prob': 0.06812053173780441}, {'id': 397, 'seek': 336310, 'start': 3363.1, 'end': 3369.3399999999997, 'text': ' your head try to get this going. Try to come up with a clean formulation of what this algorithm', 'tokens': [50364, 428, 1378, 853, 281, 483, 341, 516, 13, 6526, 281, 808, 493, 365, 257, 2541, 37642, 295, 437, 341, 9284, 50676], 'temperature': 0.0, 'avg_logprob': -0.15101370239257814, 'compression_ratio': 1.8812260536398469, 'no_speech_prob': 0.03504436835646629}, {'id': 398, 'seek': 336310, 'start': 3369.3399999999997, 'end': 3373.66, 'text': ' might be doing. Tomorrow first thing we will do is we will try to see if we can we will try to', 'tokens': [50676, 1062, 312, 884, 13, 17499, 700, 551, 321, 486, 360, 307, 321, 486, 853, 281, 536, 498, 321, 393, 321, 486, 853, 281, 50892], 'temperature': 0.0, 'avg_logprob': -0.15101370239257814, 'compression_ratio': 1.8812260536398469, 'no_speech_prob': 0.03504436835646629}, {'id': 399, 'seek': 336310, 'start': 3373.66, 'end': 3378.98, 'text': ' actually write down the algorithm and then we will argue about it. What does it why is it correct to', 'tokens': [50892, 767, 2464, 760, 264, 9284, 293, 550, 321, 486, 9695, 466, 309, 13, 708, 775, 309, 983, 307, 309, 3006, 281, 51158], 'temperature': 0.0, 'avg_logprob': -0.15101370239257814, 'compression_ratio': 1.8812260536398469, 'no_speech_prob': 0.03504436835646629}, {'id': 400, 'seek': 336310, 'start': 3378.98, 'end': 3383.54, 'text': ' do something like this? Why do you think it will compute the correct solution? Why do you think it', 'tokens': [51158, 360, 746, 411, 341, 30, 1545, 360, 291, 519, 309, 486, 14722, 264, 3006, 3827, 30, 1545, 360, 291, 519, 309, 51386], 'temperature': 0.0, 'avg_logprob': -0.15101370239257814, 'compression_ratio': 1.8812260536398469, 'no_speech_prob': 0.03504436835646629}, {'id': 401, 'seek': 336310, 'start': 3383.54, 'end': 3391.46, 'text': ' will terminate? What will happen? So we will try to analyze that. Also try to use LLVM play with it.', 'tokens': [51386, 486, 10761, 473, 30, 708, 486, 1051, 30, 407, 321, 486, 853, 281, 12477, 300, 13, 2743, 853, 281, 764, 441, 43, 53, 44, 862, 365, 309, 13, 51782], 'temperature': 0.0, 'avg_logprob': -0.15101370239257814, 'compression_ratio': 1.8812260536398469, 'no_speech_prob': 0.03504436835646629}, {'id': 402, 'seek': 339146, 'start': 3391.78, 'end': 3398.54, 'text': ' See what it looks like tomorrow. We will look at how the statements in LLVM look like. And then', 'tokens': [50380, 3008, 437, 309, 1542, 411, 4153, 13, 492, 486, 574, 412, 577, 264, 12363, 294, 441, 43, 53, 44, 574, 411, 13, 400, 550, 50718], 'temperature': 0.0, 'avg_logprob': -0.159580481679816, 'compression_ratio': 1.816793893129771, 'no_speech_prob': 0.03943580389022827}, {'id': 403, 'seek': 339146, 'start': 3398.54, 'end': 3404.78, 'text': ' we will try to implement a data flow analysis. So my plan is actually that for the motivating', 'tokens': [50718, 321, 486, 853, 281, 4445, 257, 1412, 3095, 5215, 13, 407, 452, 1393, 307, 767, 300, 337, 264, 41066, 51030], 'temperature': 0.0, 'avg_logprob': -0.159580481679816, 'compression_ratio': 1.816793893129771, 'no_speech_prob': 0.03943580389022827}, {'id': 404, 'seek': 339146, 'start': 3404.78, 'end': 3409.06, 'text': ' example we saw in the class today, which was giving different results on GCC and different', 'tokens': [51030, 1365, 321, 1866, 294, 264, 1508, 965, 11, 597, 390, 2902, 819, 3542, 322, 460, 11717, 293, 819, 51244], 'temperature': 0.0, 'avg_logprob': -0.159580481679816, 'compression_ratio': 1.816793893129771, 'no_speech_prob': 0.03943580389022827}, {'id': 405, 'seek': 339146, 'start': 3409.06, 'end': 3413.9, 'text': ' result on Clang. We would like to analyze that particular program automatically and try to figure', 'tokens': [51244, 1874, 322, 2033, 656, 13, 492, 576, 411, 281, 12477, 300, 1729, 1461, 6772, 293, 853, 281, 2573, 51486], 'temperature': 0.0, 'avg_logprob': -0.159580481679816, 'compression_ratio': 1.816793893129771, 'no_speech_prob': 0.03943580389022827}, {'id': 406, 'seek': 339146, 'start': 3413.9, 'end': 3419.7, 'text': ' out what the problem is. So we will try to write a data flow analysis to do that and then we will', 'tokens': [51486, 484, 437, 264, 1154, 307, 13, 407, 321, 486, 853, 281, 2464, 257, 1412, 3095, 5215, 281, 360, 300, 293, 550, 321, 486, 51776], 'temperature': 0.0, 'avg_logprob': -0.159580481679816, 'compression_ratio': 1.816793893129771, 'no_speech_prob': 0.03943580389022827}, {'id': 407, 'seek': 341970, 'start': 3419.7, 'end': 3428.22, 'text': " try to write a dynamic analysis to do that. So I know it's a little short time and I give my", 'tokens': [50364, 853, 281, 2464, 257, 8546, 5215, 281, 360, 300, 13, 407, 286, 458, 309, 311, 257, 707, 2099, 565, 293, 286, 976, 452, 50790], 'temperature': 0.0, 'avg_logprob': -0.16732509047896774, 'compression_ratio': 1.5217391304347827, 'no_speech_prob': 0.011556614190340042}, {'id': 408, 'seek': 341970, 'start': 3428.22, 'end': 3434.58, 'text': ' students a lot more time but if you have to somehow get used to LLVM. Try out a few things,', 'tokens': [50790, 1731, 257, 688, 544, 565, 457, 498, 291, 362, 281, 6063, 483, 1143, 281, 441, 43, 53, 44, 13, 6526, 484, 257, 1326, 721, 11, 51108], 'temperature': 0.0, 'avg_logprob': -0.16732509047896774, 'compression_ratio': 1.5217391304347827, 'no_speech_prob': 0.011556614190340042}, {'id': 409, 'seek': 341970, 'start': 3434.58, 'end': 3440.58, 'text': " struggle a bit that's okay. But get used to LLVM so that tomorrow we can do something more fun.", 'tokens': [51108, 7799, 257, 857, 300, 311, 1392, 13, 583, 483, 1143, 281, 441, 43, 53, 44, 370, 300, 4153, 321, 393, 360, 746, 544, 1019, 13, 51408], 'temperature': 0.0, 'avg_logprob': -0.16732509047896774, 'compression_ratio': 1.5217391304347827, 'no_speech_prob': 0.011556614190340042}, {'id': 410, 'seek': 344058, 'start': 3440.58, 'end': 3451.54, 'text': ' So any questions still here? So the summary is that we looked at four different analysis and', 'tokens': [50364, 407, 604, 1651, 920, 510, 30, 407, 264, 12691, 307, 300, 321, 2956, 412, 1451, 819, 5215, 293, 50912], 'temperature': 0.0, 'avg_logprob': -0.13652295224806843, 'compression_ratio': 1.5628415300546448, 'no_speech_prob': 0.02339888922870159}, {'id': 411, 'seek': 344058, 'start': 3451.54, 'end': 3459.58, 'text': ' you can find out there is lot of things which are very common. So we want to extract out those', 'tokens': [50912, 291, 393, 915, 484, 456, 307, 688, 295, 721, 597, 366, 588, 2689, 13, 407, 321, 528, 281, 8947, 484, 729, 51314], 'temperature': 0.0, 'avg_logprob': -0.13652295224806843, 'compression_ratio': 1.5628415300546448, 'no_speech_prob': 0.02339888922870159}, {'id': 412, 'seek': 344058, 'start': 3459.58, 'end': 3466.34, 'text': ' things and try to come up with a unified algorithm which works for all of them when we plug in the', 'tokens': [51314, 721, 293, 853, 281, 808, 493, 365, 257, 26787, 9284, 597, 1985, 337, 439, 295, 552, 562, 321, 5452, 294, 264, 51652], 'temperature': 0.0, 'avg_logprob': -0.13652295224806843, 'compression_ratio': 1.5628415300546448, 'no_speech_prob': 0.02339888922870159}, {'id': 413, 'seek': 346634, 'start': 3466.34, 'end': 3470.82, 'text': ' right parameters, the right transfer function, the right meet operator, the right initializations,', 'tokens': [50364, 558, 9834, 11, 264, 558, 5003, 2445, 11, 264, 558, 1677, 12973, 11, 264, 558, 5883, 14455, 11, 50588], 'temperature': 0.0, 'avg_logprob': -0.24013651212056478, 'compression_ratio': 1.3725490196078431, 'no_speech_prob': 0.5569673776626587}, {'id': 414, 'seek': 346634, 'start': 3470.82, 'end': 3474.1800000000003, 'text': ' everything else will just work magically.', 'tokens': [50588, 1203, 1646, 486, 445, 589, 39763, 13, 50756], 'temperature': 0.0, 'avg_logprob': -0.24013651212056478, 'compression_ratio': 1.3725490196078431, 'no_speech_prob': 0.5569673776626587}]